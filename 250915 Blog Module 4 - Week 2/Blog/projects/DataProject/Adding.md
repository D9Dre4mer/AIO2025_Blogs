ğŸ“Œ Ná»™i dung chi tiáº¿t cáº§n thÃªm
1. Giá»›i thiá»‡u / Má»Ÿ Ä‘áº§u

Bá»‘i cáº£nh: Giá»›i thiá»‡u PG&E (Pacific Gas and Electric) â€“ má»™t trong nhá»¯ng cÃ´ng ty Ä‘iá»‡n vÃ  khÃ­ Ä‘á»‘t lá»›n nháº¥t Hoa Ká»³, phá»¥c vá»¥ hÃ ng triá»‡u khÃ¡ch hÃ ng.

Má»¥c tiÃªu cuá»™c thi IISE PG&E Challenge 2025: Dá»± bÃ¡o táº£i Ä‘iá»‡n theo giá» cho cáº£ má»™t nÄƒm, trong bá»‘i cáº£nh khu vá»±c bá»‹ áº£nh hÆ°á»Ÿng máº¡nh bá»Ÿi nÄƒng lÆ°á»£ng máº·t trá»i.

RÃ ng buá»™c:

Chá»‰ dÃ¹ng dá»¯ liá»‡u cung cáº¥p (Load, Temperature, GHI).

KhÃ´ng Ä‘Æ°á»£c dÃ¹ng dá»¯ liá»‡u ngoÃ i.

Dá»± bÃ¡o theo â€œday-aheadâ€.

2. Data Overview

Dá»¯ liá»‡u cung cáº¥p:

2 nÄƒm dá»¯ liá»‡u lá»‹ch sá»­.

Endogenous: Load (má»¥c tiÃªu cáº§n dá»± bÃ¡o).

Exogenous: Nhiá»‡t Ä‘á»™ (Temperature) vÃ  Bá»©c xáº¡ toÃ n pháº§n GHI (Global Horizontal Irradiance) tá»« 5 tráº¡m.

Giáº£i thÃ­ch khÃ¡i niá»‡m:

Endogenous = phá»¥ thuá»™c, Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh trong mÃ´ hÃ¬nh (Load).

Exogenous = Ä‘á»™c láº­p, yáº¿u tá»‘ bÃªn ngoÃ i tÃ¡c Ä‘á»™ng (Temperature, GHI).

Máº«u dá»¯ liá»‡u (Year, Month, Day, Hour, Load, Temp site 1-5, GHI site 1-5).

Thá»‘ng kÃª mÃ´ táº£: Skewness (Î³1), Kurtosis (Î³2).

PhÃ¢n tÃ­ch biáº¿n thiÃªn:

Hourly patterns: chu ká»³ 24h.

Weekly: ngÃ y thÆ°á»ng vs cuá»‘i tuáº§n.

Seasonal: xu hÆ°á»›ng khÃ¡c biá»‡t theo mÃ¹a.

PhÃ¢n tÃ­ch tÆ°Æ¡ng quan:

Load â†” GHI: quan há»‡ nghá»‹ch (nhiá»u náº¯ng â†’ Ã­t táº£i).

Load â†” Temperature: quan há»‡ thuáº­n (nÃ³ng/láº¡nh cá»±c Ä‘oan â†’ tÄƒng táº£i).

GHI â†” Temperature: collinearity cao giá»¯a cÃ¡c site.

Káº¿t luáº­n EDA:

Táº£i Ä‘iá»‡n cÃ³ tÃ­nh phi tuyáº¿n.

CÃ³ hiá»‡u á»©ng chu ká»³ ngÃ y, tuáº§n, mÃ¹a.

CÃ³ hiá»‡n tÆ°á»£ng multicollinearity, cáº§n xá»­ lÃ½ báº±ng PCA, PLS, hoáº·c cÃ¡c mÃ´ hÃ¬nh cÃ¢y nhÆ° RF/XGBoost.

3. Modeling Strategy

Dimension Reduction:

PCA: trÃ­ch xuáº¥t thÃ nh pháº§n chÃ­nh, khÃ´ng xÃ©t Y (unsupervised).

PLS: trÃ­ch xuáº¥t latent variables, tá»‘i Ä‘a covariance(X,Y), supervised â†’ phÃ¹ há»£p hÆ¡n.

Feature Engineering:

Time features: sinusoidal encoding (chu ká»³ ngÃ y/tuáº§n/mÃ¹a), day-of-week, holiday flags.

Weather features: lag1, lag24, delta1, delta24.

Behavioral features: Heating Degree Hours (HDH), Cooling Degree Hours (CDH).

Model Selection (so sÃ¡nh):

Linear models: yáº¿u â†’ loáº¡i.

Random Forest: khÃ¡ tá»‘t nhÆ°ng kÃ©m interpretability.

XGBoost: chá»n lÃ m mÃ´ hÃ¬nh chÃ­nh.

LSTM: cÃ³ thá»ƒ dÃ¹ng cho chuá»—i dÃ i.

Transformers: overkill vá»›i dataset nhá».

Hyperparameter Tuning:

Grid Search (Greedy): dÃ¹ng Masterâ€“Slave (MPI) phÃ¢n chia job.

Bayesian Optimization: surrogate model + acquisition function â†’ cÃ¢n báº±ng exploration vs exploitation.

4. Evaluation

ThÃ­ nghiá»‡m: so sÃ¡nh dáº§n theo feature set:

Raw Data â†’ +DimRed â†’ +Time â†’ +Weather â†’ +Behavioral.

Káº¿t quáº£ (slide 47): Báº£ng thá»ƒ hiá»‡n MSE, RMSE, MAE, MAPE.

Final model: XGBoost vá»›i hyperparams:

Estimators = 200

Max depth = 7

Learning rate = 0.01 â€“ 0.1

Subsample = 0.8 â€“ 1.0

5. Future Works

Probabilistic Forecasting:

LÃ½ do: cáº§n dáº£i dá»± bÃ¡o Ä‘á»ƒ quáº£n lÃ½ lÆ°á»›i, thá»‹ trÆ°á»ng nÄƒng lÆ°á»£ng, rá»§i ro.

Ká»¹ thuáº­t: Quantile Regression (P10/P50/P90), Conformal prediction, Bootstrap, Bayesian/Deep models.