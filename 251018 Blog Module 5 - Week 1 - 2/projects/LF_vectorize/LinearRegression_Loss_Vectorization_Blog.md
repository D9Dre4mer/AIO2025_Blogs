
# ğŸ§® Hiá»ƒu ÄÆ¡n Giáº£n Vá» Linear Regression â€“ Tá»« Loss Function Äáº¿n Vectorization

> Má»™t hÆ°á»›ng dáº«n trá»±c quan giÃºp báº¡n hiá»ƒu cÃ¡ch mÃ¡y tÃ­nh â€œhá»câ€ Ä‘Æ°á»ng tháº³ng tá»‘t nháº¥t qua dá»¯ liá»‡u â€” tá»« cÃ´ng thá»©c cÆ¡ báº£n Ä‘áº¿n tá»‘i Æ°u báº±ng vector hÃ³a.

---

## 1ï¸âƒ£ Linear Regression LÃ  GÃ¬?

Linear Regression (Há»“i quy tuyáº¿n tÃ­nh) lÃ  mÃ´ hÃ¬nh cÆ¡ báº£n trong há»c mÃ¡y, tÃ¬m **Ä‘Æ°á»ng tháº³ng tá»‘t nháº¥t** mÃ´ táº£ má»‘i quan há»‡ giá»¯a **biáº¿n Ä‘áº§u vÃ o (x)** vÃ  **biáº¿n Ä‘áº§u ra (y)**:

\[
\hat{y} = w x + b
\]

- **w**: Ä‘á»™ dá»‘c (weight)  
- **b**: giao Ä‘iá»ƒm trá»¥c tung (bias)  
- **Å· (y-hat)**: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n  

VÃ­ dá»¥:  
> Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  (`price`) theo diá»‡n tÃ­ch (`area`):  
> `price = w * area + b`

![Linear Regression Scatter Plot](img/figure1_scatter_plot.png)
*HÃ¬nh 1: Minh há»a Linear Regression vá»›i scatter plot cá»§a diá»‡n tÃ­ch vÃ  giÃ¡ nhÃ *

---

## 2ï¸âƒ£ Loss Function â€“ CÃ¡ch MÃ¡y TÃ­nh â€œÄoâ€ Sai Sá»‘

Äá»ƒ biáº¿t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»‘t hay khÃ´ng, ta cáº§n **hÃ m máº¥t mÃ¡t (Loss Function)** â€“ Ä‘o Ä‘á»™ lá»‡ch giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿.

---

### ğŸ”¹ Mean Squared Error (MSE)

\[
L = \frac{1}{N} \sum_{i=1}^{N} (\hat{y_i} - y_i)^2
\]

- Dá»… Ä‘áº¡o hÃ m, giÃºp viá»‡c tá»‘i Æ°u trÆ¡n tru.  
- Tuy nhiÃªn **ráº¥t nháº¡y vá»›i outlier** (vÃ¬ lá»—i bá»‹ bÃ¬nh phÆ°Æ¡ng).

**Gradient cá»§a MSE:**
\[
\frac{\partial L}{\partial w} = 2x(\hat{y} - y), \quad 
\frac{\partial L}{\partial b} = 2(\hat{y} - y)
\]

![MSE Curve](img/figure2_mse_curve.png)
*HÃ¬nh 2: Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng cong MSE - outlier lÃ m tÄƒng loss nhanh chÃ³ng*

---

### ğŸ”¹ Mean Absolute Error (MAE)

\[
L = \frac{1}{N} \sum_{i=1}^{N} |\hat{y_i} - y_i|
\]

- Bá»n vá»¯ng hÆ¡n vá»›i outlier (vÃ¬ lá»—i khÃ´ng bá»‹ bÃ¬nh phÆ°Æ¡ng).  
- NhÆ°ng Ä‘áº¡o hÃ m khÃ´ng xÃ¡c Ä‘á»‹nh táº¡i 0, khiáº¿n viá»‡c há»c cháº­m hÆ¡n.

**Gradient cá»§a MAE:**
\[
\frac{\partial L}{\partial w} = x \cdot \text{sign}(\hat{y} - y)
\]

![MAE Curve](img/figure3_mae_curve.png)
*HÃ¬nh 3: Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng cong MAE (hÃ¬nh chá»¯ V) Ä‘á»ƒ so sÃ¡nh vá»›i MSE*

---

### ğŸ”¹ Huber Loss â€“ â€œLáº¥y CÃ¡i Hay Cá»§a Cáº£ Haiâ€

Huber Loss káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a MSE vÃ  MAE:

\[
L_\delta = 
\begin{cases}
\frac{1}{2}(\hat{y} - y)^2, & \text{náº¿u } |\hat{y} - y| \leq \delta \\
\delta \cdot |\hat{y} - y| - \frac{1}{2}\delta^2, & \text{náº¿u } |\hat{y} - y| > \delta
\end{cases}
\]

- Khi sai sá»‘ nhá» â†’ giá»‘ng MSE.  
- Khi sai sá»‘ lá»›n â†’ giá»‘ng MAE.

ğŸ§  **Hiá»ƒu nÃ´m na:**  
Huber Loss giá»‘ng nhÆ° ta â€œmá»m máº¡iâ€ vá»›i lá»—i nhá», nhÆ°ng â€œkhoan dungâ€ vá»›i outlier.

![Huber Loss Curve](img/figure4_huber_curve.png)
*HÃ¬nh 4: Biá»ƒu Ä‘á»“ Huber Loss - Ä‘oáº¡n giá»¯a lÃ  parabol (MSE), hai bÃªn lÃ  tuyáº¿n tÃ­nh (MAE)*

---

## 3ï¸âƒ£ Chuáº©n HÃ³a Dá»¯ Liá»‡u (Normalization)

Khi cÃ¡c Ä‘áº·c trÆ°ng (features) cÃ³ thang giÃ¡ trá»‹ khÃ¡c nhau, viá»‡c há»c sáº½ cháº­m hoáº·c khÃ´ng há»™i tá»¥.  
Giáº£i phÃ¡p: **chuáº©n hÃ³a dá»¯ liá»‡u** vá» cÃ¹ng pháº¡m vi.

\[
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
\]

VÃ­ dá»¥: náº¿u diá»‡n tÃ­ch nhÃ  dao Ä‘á»™ng tá»« 30â€“300 mÂ², thÃ¬ sau chuáº©n hÃ³a, giÃ¡ trá»‹ chá»‰ náº±m trong khoáº£ng [0,1].

![Normalization Histogram](img/figure5_normalization.png)
*HÃ¬nh 5: Histogram trÆ°á»›c vÃ  sau normalization - thang giÃ¡ trá»‹ thay Ä‘á»•i tá»« 30-300 sang 0-1*

---

## 4ï¸âƒ£ Regularization â€“ Chá»‘ng Overfitting

Khi mÃ´ hÃ¬nh há»c â€œquÃ¡ ká»¹â€ dá»¯ liá»‡u huáº¥n luyá»‡n, nÃ³ dá»… **overfit**, tá»©c lÃ  há»c cáº£ nhiá»…u.

Giáº£i phÃ¡p lÃ  thÃªm Ä‘iá»u khoáº£n **pháº¡t trá»ng sá»‘ lá»›n** vÃ o hÃ m loss:

\[
L_{reg} = (\hat{y} - y)^2 + \lambda(w_1^2 + w_2^2 + \dots)
\]

- **Î» (lambda)**: há»‡ sá»‘ Ä‘iá»u chá»‰nh má»©c pháº¡t.  
- GiÃºp mÃ´ hÃ¬nh â€œkhiÃªm tá»‘nâ€ hÆ¡n, khÃ´ng quÃ¡ phá»¥ thuá»™c vÃ o dá»¯ liá»‡u cá»¥ thá»ƒ.

![Overfitting Comparison](img/figure6_overfitting.png)
*HÃ¬nh 6: So sÃ¡nh hai Ä‘Æ°á»ng há»“i quy - má»™t Ä‘Æ°á»ng fit "quÃ¡ sÃ¡t" dá»¯ liá»‡u (overfit) vÃ  má»™t Ä‘Æ°á»ng mÆ°á»£t hÆ¡n (regularized)*

---

## 5ï¸âƒ£ Vectorization â€“ Khi ToÃ¡n Há»c GiÃºp Code Cháº¡y Nhanh HÆ¡n âš¡

Thay vÃ¬ tÃ­nh tá»«ng máº«u riÃªng láº», ta cÃ³ thá»ƒ **gom táº¥t cáº£ dá»¯ liá»‡u thÃ nh ma tráº­n** vÃ  dÃ¹ng phÃ©p nhÃ¢n vector Ä‘á»ƒ cáº­p nháº­t tham sá»‘ cÃ¹ng lÃºc.

### ğŸ§© Biá»ƒu diá»…n ma tráº­n

\[
\mathbf{X} =
\begin{bmatrix}
x_1 & 1 \\
x_2 & 1 \\
\vdots & \vdots \\
x_N & 1
\end{bmatrix},
\quad
\boldsymbol{\theta} =
\begin{bmatrix}
w \\ b
\end{bmatrix},
\quad
\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\theta}
\]

![Vectorization Diagram](img/figure7_vectorization.png)
*HÃ¬nh 7: SÆ¡ Ä‘á»“ vector hÃ³a minh há»a phÃ©p nhÃ¢n ma tráº­n X @ Î¸ â†’ Å·*

---

### ğŸ§® Gradient Descent Dáº¡ng Vector

\[
\boldsymbol{\theta} = \boldsymbol{\theta} - \eta \cdot \frac{1}{N}\mathbf{X}^T ( \hat{\mathbf{y}} - \mathbf{y} )
\]

- **Î· (eta)**: learning rate  
- Táº¥t cáº£ phÃ©p tÃ­nh Ä‘Æ°á»£c gÃ³i gá»n trong vÃ i dÃ²ng NumPy, cá»±c ká»³ nhanh.

![Vectorization Flowchart](img/figure8_flowchart.png)
*HÃ¬nh 8: Flowchart quy trÃ¬nh vector hÃ³a gá»“m 4 bÆ°á»›c: Compute Output â†’ Compute Loss â†’ Compute Gradient â†’ Update Parameters*

---

## 6ï¸âƒ£ Tá»•ng Káº¿t

| HÃ m máº¥t mÃ¡t | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p khi... |
|--------------|-----------|--------------|----------------|
| **MSE** | Dá»… Ä‘áº¡o hÃ m, á»•n Ä‘á»‹nh | Nháº¡y vá»›i outlier | Dá»¯ liá»‡u sáº¡ch |
| **MAE** | Bá»n vá»›i outlier | Äáº¡o hÃ m khÃ´ng mÆ°á»£t | Dá»¯ liá»‡u nhiá»…u |
| **Huber** | Káº¿t há»£p cáº£ hai | Cáº§n chá»n Î´ | CÃ³ outlier nháº¹ |

**CÃ¡c ká»¹ thuáº­t há»— trá»£:**  
- **Normalization:** giÃºp há»™i tá»¥ nhanh hÆ¡n  
- **Regularization:** giáº£m overfitting  
- **Vectorization:** tá»‘i Æ°u tá»‘c Ä‘á»™ tÃ­nh toÃ¡n

---

## ğŸ“˜ Káº¿t Luáº­n

Linear Regression lÃ  ná»n táº£ng cho má»i mÃ´ hÃ¬nh há»c mÃ¡y.  
Hiá»ƒu rÃµ cÃ¡c loáº¡i **loss function**, cÃ¡ch **tÃ­nh gradient**, vÃ  **vector hÃ³a** giÃºp báº¡n náº¯m vá»¯ng má»i mÃ´ hÃ¬nh sÃ¢u hÆ¡n nhÆ° Logistic Regression, Neural Network hay CNN.

---

## ğŸ’» VÃ­ Dá»¥ Python

```python
import numpy as np

# Data
X = np.array([[6.7, 1], [4.6, 1], [3.5, 1], [5.5, 1]])
y = np.array([[9.1], [5.9], [4.6], [6.7]])

# Initialize
theta = np.array([[0.049], [-0.34]])
eta = 0.01
N = len(y)

# Gradient Descent
for epoch in range(1000):
    y_pred = X @ theta
    grad = (2/N) * X.T @ (y_pred - y)
    theta -= eta * grad

print(theta)
```

![Loss Curve](img/figure9_loss_curve.png)
*HÃ¬nh 9: Loss curve giáº£m dáº§n theo sá»‘ epoch*

---

## âœï¸ Gá»£i Ã Khi LÃ m Blog

- ThÃªm mÃ u sáº¯c nháº¹ cho code block vÃ  cÃ´ng thá»©c (Markdown MathJax).  
- Giá»¯ má»—i hÃ¬nh má»™t caption ngáº¯n (â€œMSE curveâ€, â€œHuber Lossâ€, â€œVectorized flowâ€).  
- Sá»­ dá»¥ng cÃ¹ng palette mÃ u cho toÃ n bÃ i (xanh nháº¡t hoáº·c cam Ä‘áº¥t).  
