# HÃ nh TrÃ¬nh ToÃ n Diá»‡n Trong Tháº¿ Giá»›i AI: Tá»« Tá»‘i Æ¯u HÃ³a Äáº¿n Váº­n HÃ nh Xuáº¥t Sáº¯c

*KhÃ¡m phÃ¡ ba trá»¥ cá»™t cá»‘t lÃµi: Gradient Descent, Explainable AI vÃ  MLOps*

## ğŸ“‹ TÃ³m Táº¯t

**ğŸ“š Module 5 - Tuáº§n 01 & 02**  

Chia sáº» ba trá»¥ cá»™t cá»‘t lÃµi trong AI:

ğŸ”¹ **Gradient Descent**: CÃ¡c hÃ m máº¥t mÃ¡t, chuáº©n hÃ³a dá»¯ liá»‡u, vectorization.

ğŸ”¹ **Explainable AI**: PhÆ°Æ¡ng phÃ¡p LIME vÃ  Anchor Ä‘á»ƒ lÃ m AI cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c.

ğŸ”¹ **MLOps**: Thu háº¹p khoáº£ng cÃ¡ch nghiÃªn cá»©u-production, tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh.

**Äá»‘i tÆ°á»£ng:** Ká»¹ sÆ° AI, Data Scientists. **Thá»i gian:** 15-20 phÃºt.

---

## 1. Lá»i Má»Ÿ Äáº§u: HÃ nh TrÃ¬nh Tá»« LÃ½ Thuyáº¿t Äáº¿n Thá»±c Tiá»…n

Trong tháº¿ giá»›i AI hiá»‡n Ä‘áº¡i, viá»‡c xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh Machine Learning thÃ nh cÃ´ng khÃ´ng chá»‰ dá»«ng láº¡i á»Ÿ viá»‡c Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao trÃªn táº­p dá»¯ liá»‡u huáº¥n luyá»‡n. ÄÃ³ lÃ  má»™t hÃ nh trÃ¬nh phá»©c táº¡p bao gá»“m ba trá»¥ cá»™t cá»‘t lÃµi:

1. **Tá»‘i Æ°u hÃ³a ná»n táº£ng** - Hiá»ƒu sÃ¢u vá» Gradient Descent vÃ  cÃ¡c hÃ m máº¥t mÃ¡t
2. **Giáº£i thÃ­ch vÃ  minh báº¡ch** - LÃ m cho AI cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c thÃ´ng qua XAI
3. **Váº­n hÃ nh vÃ  quáº£n lÃ½** - ÄÆ°a AI tá»« phÃ²ng thÃ­ nghiá»‡m ra tháº¿ giá»›i thá»±c qua MLOps

BÃ i viáº¿t nÃ y sáº½ dáº«n dáº¯t báº¡n qua má»™t hÃ nh trÃ¬nh toÃ n diá»‡n, tá»« nhá»¯ng nguyÃªn lÃ½ toÃ¡n há»c cÆ¡ báº£n Ä‘áº¿n viá»‡c váº­n hÃ nh cÃ¡c há»‡ thá»‘ng AI phá»©c táº¡p trong mÃ´i trÆ°á»ng production.

## 2. Pháº§n I: Gradient Descent - Ná»n Táº£ng ToÃ¡n Há»c Cá»§a Há»c MÃ¡y

### 2.1 Linear Regression LÃ  GÃ¬?

Linear Regression (Há»“i quy tuyáº¿n tÃ­nh) lÃ  mÃ´ hÃ¬nh cÆ¡ báº£n trong há»c mÃ¡y, tÃ¬m **Ä‘Æ°á»ng tháº³ng tá»‘t nháº¥t** mÃ´ táº£ má»‘i quan há»‡ giá»¯a **biáº¿n Ä‘áº§u vÃ o (x)** vÃ  **biáº¿n Ä‘áº§u ra (y)**:

$$\hat{y} = w x + b$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **Å· (y-hat)**: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh (predicted value)
- **w**: há»‡ sá»‘ gÃ³c/Ä‘á»™ dá»‘c cá»§a Ä‘Æ°á»ng tháº³ng (slope/weight) - cho biáº¿t má»©c Ä‘á»™ thay Ä‘á»•i cá»§a y khi x tÄƒng 1 Ä‘Æ¡n vá»‹
- **x**: giÃ¡ trá»‹ Ä‘áº§u vÃ o (input feature) - biáº¿n Ä‘á»™c láº­p
- **b**: há»‡ sá»‘ cháº·n/giao Ä‘iá»ƒm trá»¥c tung (intercept/bias) - giÃ¡ trá»‹ cá»§a y khi x = 0  

VÃ­ dá»¥:  
> Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  (`price`) theo diá»‡n tÃ­ch (`area`):  
> `price = w * area + b`

<div align="center">

![Linear Regression Scatter Plot](https://i.ibb.co/qLpttjFt/figure1-scatter-plot.png)

*HÃ¬nh 1: Minh há»a Linear Regression vá»›i scatter plot cá»§a diá»‡n tÃ­ch vÃ  giÃ¡ nhÃ *

</div>

### 2.2 Loss Function â€“ CÃ¡ch MÃ¡y TÃ­nh "Äo" Sai Sá»‘

Äá»ƒ biáº¿t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»‘t hay khÃ´ng, ta cáº§n **hÃ m máº¥t mÃ¡t (Loss Function)** â€“ Ä‘o Ä‘á»™ lá»‡ch giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿.

#### 2.2.1 Mean Squared Error (MSE)

$$L = \frac{1}{N} \sum_{i=1}^{N} (\hat{y_i} - y_i)^2$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **L**: giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t (loss value) - cÃ ng nhá» cÃ ng tá»‘t
- **N**: tá»•ng sá»‘ máº«u dá»¯ liá»‡u trong táº­p huáº¥n luyá»‡n
- **Å·áµ¢**: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cho máº«u thá»© i
- **yáµ¢**: giÃ¡ trá»‹ thá»±c táº¿ (ground truth) cá»§a máº«u thá»© i
- **(Å·áµ¢ - yáµ¢)Â²**: bÃ¬nh phÆ°Æ¡ng sai sá»‘ cho máº«u thá»© i - phÃ©p bÃ¬nh phÆ°Æ¡ng lÃ m tÄƒng trá»ng sá»‘ cá»§a cÃ¡c lá»—i lá»›n

**Gradient cá»§a MSE:**
$$\frac{\partial L}{\partial w} = 2x(\hat{y} - y), \quad 
\frac{\partial L}{\partial b} = 2(\hat{y} - y)$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **âˆ‚L/âˆ‚w**: Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m máº¥t mÃ¡t theo trá»ng sá»‘ w - cho biáº¿t hÆ°á»›ng vÃ  má»©c Ä‘á»™ cáº§n Ä‘iá»u chá»‰nh w
- **âˆ‚L/âˆ‚b**: Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m máº¥t mÃ¡t theo bias b - cho biáº¿t hÆ°á»›ng vÃ  má»©c Ä‘á»™ cáº§n Ä‘iá»u chá»‰nh b
- **2x(Å· - y)**: gradient cho w - tá»· lá»‡ thuáº­n vá»›i giÃ¡ trá»‹ Ä‘áº§u vÃ o x vÃ  sai sá»‘ (Å· - y)
- **2(Å· - y)**: gradient cho b - chá»‰ phá»¥ thuá»™c vÃ o sai sá»‘, khÃ´ng phá»¥ thuá»™c vÃ o giÃ¡ trá»‹ Ä‘áº§u vÃ o

<div align="center">

![MSE Curve](https://i.ibb.co/RT86sny6/figure2-mse-curve.png)

*HÃ¬nh 2: Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng cong MSE - outlier lÃ m tÄƒng loss nhanh chÃ³ng*

</div>

#### 2.2.2 Mean Absolute Error (MAE)

$$L = \frac{1}{N} \sum_{i=1}^{N} |\hat{y_i} - y_i|$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **L**: giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t MAE (Mean Absolute Error)
- **N**: tá»•ng sá»‘ máº«u dá»¯ liá»‡u trong táº­p huáº¥n luyá»‡n
- **Å·áµ¢**: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cho máº«u thá»© i
- **yáµ¢**: giÃ¡ trá»‹ thá»±c táº¿ cá»§a máº«u thá»© i
- **|Å·áµ¢ - yáµ¢|**: giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a sai sá»‘ - khÃ´ng cÃ³ phÃ©p bÃ¬nh phÆ°Æ¡ng nÃªn Ã­t nháº¡y vá»›i outlier hÆ¡n MSE

**Gradient cá»§a MAE:**
$$\frac{\partial L}{\partial w} = x \cdot \text{sign}(\hat{y} - y)$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **âˆ‚L/âˆ‚w**: Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m máº¥t mÃ¡t MAE theo trá»ng sá»‘ w
- **sign(Å· - y)**: hÃ m signum - tráº£ vá» +1 náº¿u Å· > y, -1 náº¿u Å· < y, 0 náº¿u Å· = y
- **x Â· sign(Å· - y)**: gradient cho w - tá»· lá»‡ thuáº­n vá»›i x vÃ  chá»‰ phá»¥ thuá»™c vÃ o dáº¥u cá»§a sai sá»‘
- **LÆ°u Ã½**: gradient khÃ´ng xÃ¡c Ä‘á»‹nh táº¡i Ä‘iá»ƒm Å· = y (sai sá»‘ = 0), cÃ³ thá»ƒ gÃ¢y khÃ³ khÄƒn trong tá»‘i Æ°u hÃ³a

<div align="center">

![MAE Curve](https://i.ibb.co/C546R7hR/figure3-mae-curve.png)

*HÃ¬nh 3: Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng cong MAE (hÃ¬nh chá»¯ V) Ä‘á»ƒ so sÃ¡nh vá»›i MSE*

</div>

#### 2.2.3 Huber Loss â€“ "Láº¥y CÃ¡i Hay Cá»§a Cáº£ Hai"

Huber Loss káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a MSE vÃ  MAE:

$$L_\delta = 
\begin{cases}
\frac{1}{2}(\hat{y} - y)^2, & \text{náº¿u } |\hat{y} - y| \leq \delta \\
\delta \cdot |\hat{y} - y| - \frac{1}{2}\delta^2, & \text{náº¿u } |\hat{y} - y| > \delta
\end{cases}$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **L_Î´**: giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t Huber vá»›i ngÆ°á»¡ng Î´ (delta)
- **Î´**: tham sá»‘ ngÆ°á»¡ng - quyáº¿t Ä‘á»‹nh khi nÃ o chuyá»ƒn tá»« MSE sang MAE
- **TrÆ°á»ng há»£p 1**: khi |Å· - y| â‰¤ Î´ (sai sá»‘ nhá») â†’ sá»­ dá»¥ng MSE: Â½(Å· - y)Â²
- **TrÆ°á»ng há»£p 2**: khi |Å· - y| > Î´ (sai sá»‘ lá»›n) â†’ sá»­ dá»¥ng MAE: Î´|Å· - y| - Â½Î´Â²
- **Â½Î´Â²**: háº±ng sá»‘ Ä‘iá»u chá»‰nh Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh liÃªn tá»¥c cá»§a hÃ m táº¡i Ä‘iá»ƒm chuyá»ƒn Ä‘á»•i

ğŸ§  **Hiá»ƒu nÃ´m na:**  
Huber Loss giá»‘ng nhÆ° ta "má»m máº¡i" vá»›i lá»—i nhá», nhÆ°ng "khoan dung" vá»›i outlier.

<div align="center">

![Huber Loss Curve](https://i.ibb.co/rRXx3cZd/figure4-huber-curve.png)

*HÃ¬nh 4: Biá»ƒu Ä‘á»“ Huber Loss - Ä‘oáº¡n giá»¯a lÃ  parabol (MSE), hai bÃªn lÃ  tuyáº¿n tÃ­nh (MAE)*

</div>

### 2.3 Chuáº©n HÃ³a Dá»¯ Liá»‡u (Normalization)

Khi cÃ¡c Ä‘áº·c trÆ°ng (features) cÃ³ thang giÃ¡ trá»‹ khÃ¡c nhau, viá»‡c há»c sáº½ cháº­m hoáº·c khÃ´ng há»™i tá»¥.  
Giáº£i phÃ¡p: **chuáº©n hÃ³a dá»¯ liá»‡u** vá» cÃ¹ng pháº¡m vi.

$$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **x'**: giÃ¡ trá»‹ sau khi chuáº©n hÃ³a (normalized value) - náº±m trong khoáº£ng [0, 1]
- **x**: giÃ¡ trá»‹ gá»‘c trÆ°á»›c khi chuáº©n hÃ³a (original value)
- **x_min**: giÃ¡ trá»‹ nhá» nháº¥t trong táº­p dá»¯ liá»‡u
- **x_max**: giÃ¡ trá»‹ lá»›n nháº¥t trong táº­p dá»¯ liá»‡u
- **x_max - x_min**: khoáº£ng giÃ¡ trá»‹ (range) cá»§a dá»¯ liá»‡u gá»‘c
- **Má»¥c Ä‘Ã­ch**: Ä‘Æ°a táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng vá» cÃ¹ng má»™t thang Ä‘o Ä‘á»ƒ gradient descent há»™i tá»¥ nhanh hÆ¡n

<div align="center">

![Normalization Histogram](https://i.ibb.co/7NbXzZwK/figure5-normalization.png)

*HÃ¬nh 5: Histogram trÆ°á»›c vÃ  sau normalization - thang giÃ¡ trá»‹ thay Ä‘á»•i tá»« 30-300 sang 0-1*

</div>

### 2.4 Regularization â€“ Chá»‘ng Overfitting

Khi mÃ´ hÃ¬nh há»c "quÃ¡ ká»¹" dá»¯ liá»‡u huáº¥n luyá»‡n, nÃ³ dá»… **overfit**, tá»©c lÃ  há»c cáº£ nhiá»…u.

Giáº£i phÃ¡p lÃ  thÃªm Ä‘iá»u khoáº£n **pháº¡t trá»ng sá»‘ lá»›n** vÃ o hÃ m loss:

$$L_{reg} = (\hat{y} - y)^2 + \lambda(w_1^2 + w_2^2 + \dots)$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **L_reg**: hÃ m máº¥t mÃ¡t cÃ³ regularization (regularized loss function)
- **(Å· - y)Â²**: pháº§n máº¥t mÃ¡t gá»‘c (original loss) - MSE trong vÃ­ dá»¥ nÃ y
- **Î» (lambda)**: há»‡ sá»‘ regularization - Ä‘iá»u chá»‰nh má»©c Ä‘á»™ pháº¡t trá»ng sá»‘ lá»›n
- **wâ‚Â² + wâ‚‚Â² + ...**: tá»•ng bÃ¬nh phÆ°Æ¡ng cÃ¡c trá»ng sá»‘ (L2 regularization)
- **Î»(wâ‚Â² + wâ‚‚Â² + ...)**: pháº§n pháº¡t (penalty term) - ngÄƒn trá»ng sá»‘ trá»Ÿ nÃªn quÃ¡ lá»›n
- **Má»¥c Ä‘Ã­ch**: cÃ¢n báº±ng giá»¯a viá»‡c fit dá»¯ liá»‡u vÃ  giá»¯ mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n Ä‘á»ƒ trÃ¡nh overfitting

<div align="center">

![Overfitting Comparison](https://i.ibb.co/YFpFGb6h/figure6-overfitting.png)

*HÃ¬nh 6: So sÃ¡nh hai Ä‘Æ°á»ng há»“i quy - má»™t Ä‘Æ°á»ng fit "quÃ¡ sÃ¡t" dá»¯ liá»‡u (overfit) vÃ  má»™t Ä‘Æ°á»ng mÆ°á»£t hÆ¡n (regularized)*

</div>

### 2.5 Vectorization â€“ Khi ToÃ¡n Há»c GiÃºp Code Cháº¡y Nhanh HÆ¡n âš¡

Thay vÃ¬ tÃ­nh tá»«ng máº«u riÃªng láº», ta cÃ³ thá»ƒ **gom táº¥t cáº£ dá»¯ liá»‡u thÃ nh ma tráº­n** vÃ  dÃ¹ng phÃ©p nhÃ¢n vector Ä‘á»ƒ cáº­p nháº­t tham sá»‘ cÃ¹ng lÃºc.

#### 2.5.1 Biá»ƒu diá»…n ma tráº­n

$$\mathbf{X} =
\begin{bmatrix}
x_1 & 1 \\
x_2 & 1 \\
\vdots & \vdots \\
x_N & 1
\end{bmatrix},
\quad
\boldsymbol{\theta} =
\begin{bmatrix}
w \\ b
\end{bmatrix},
\quad
\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\theta}$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **X**: ma tráº­n thiáº¿t káº¿ (design matrix) - chá»©a táº¥t cáº£ dá»¯ liá»‡u Ä‘áº§u vÃ o
- **xâ‚, xâ‚‚, ..., x_N**: cÃ¡c giÃ¡ trá»‹ Ä‘áº§u vÃ o cá»§a N máº«u dá»¯ liá»‡u
- **Cá»™t thá»© 2 toÃ n sá»‘ 1**: Ä‘á»ƒ nhÃ¢n vá»›i bias b trong phÃ©p nhÃ¢n ma tráº­n
- **Î¸**: vector tham sá»‘ chá»©a trá»ng sá»‘ w vÃ  bias b
- **Å·**: vector dá»± Ä‘oÃ¡n - káº¿t quáº£ cá»§a phÃ©p nhÃ¢n ma tráº­n XÎ¸
- **Lá»£i Ã­ch**: tÃ­nh toÃ¡n song song cho táº¥t cáº£ máº«u cÃ¹ng lÃºc thay vÃ¬ tá»«ng máº«u má»™t

#### 2.5.2 Gradient Descent Dáº¡ng Vector

$$\boldsymbol{\theta} = \boldsymbol{\theta} - \eta \cdot \frac{1}{N}\mathbf{X}^T ( \hat{\mathbf{y}} - \mathbf{y} )$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **Î¸**: vector tham sá»‘ cáº§n cáº­p nháº­t [w, b]
- **Î· (eta)**: learning rate - tá»‘c Ä‘á»™ há»c, Ä‘iá»u chá»‰nh má»©c Ä‘á»™ thay Ä‘á»•i trong má»—i bÆ°á»›c
- **1/N**: chuáº©n hÃ³a theo sá»‘ máº«u Ä‘á»ƒ gradient khÃ´ng phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c táº­p dá»¯ liá»‡u
- **X^T**: ma tráº­n chuyá»ƒn vá»‹ cá»§a X - Ä‘á»ƒ tÃ­nh gradient cho táº¥t cáº£ tham sá»‘ cÃ¹ng lÃºc
- **(Å· - y)**: vector sai sá»‘ giá»¯a dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c
- **X^T(Å· - y)**: gradient vector cho táº¥t cáº£ tham sá»‘
- **Má»¥c Ä‘Ã­ch**: cáº­p nháº­t táº¥t cáº£ tham sá»‘ cÃ¹ng lÃºc má»™t cÃ¡ch hiá»‡u quáº£


### 2.6 Tá»•ng Káº¿t CÃ¡c HÃ m Máº¥t MÃ¡t

| HÃ m máº¥t mÃ¡t | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p khi... |
|--------------|-----------|--------------|----------------|
| **MSE** | Dá»… Ä‘áº¡o hÃ m, á»•n Ä‘á»‹nh | Nháº¡y vá»›i outlier | Dá»¯ liá»‡u sáº¡ch |
| **MAE** | Bá»n vá»›i outlier | Äáº¡o hÃ m khÃ´ng mÆ°á»£t | Dá»¯ liá»‡u nhiá»…u |
| **Huber** | Káº¿t há»£p cáº£ hai | Cáº§n chá»n Î´ | CÃ³ outlier nháº¹ |

**CÃ¡c ká»¹ thuáº­t há»— trá»£:**  
- **Normalization:** giÃºp há»™i tá»¥ nhanh hÆ¡n  
- **Regularization:** giáº£m overfitting  
- **Vectorization:** tá»‘i Æ°u tá»‘c Ä‘á»™ tÃ­nh toÃ¡n

### 2.7 VÃ­ Dá»¥ Python Thá»±c Táº¿

```python
import numpy as np

# Data
X = np.array([[6.7, 1], [4.6, 1], [3.5, 1], [5.5, 1]])
y = np.array([[9.1], [5.9], [4.6], [6.7]])

# Initialize
theta = np.array([[0.049], [-0.34]])
eta = 0.01
N = len(y)

# Gradient Descent
for epoch in range(1000):
    y_pred = X @ theta
    grad = (2/N) * X.T @ (y_pred - y)
    theta -= eta * grad

print(theta)
```

<div align="center">

![Loss Curve](https://i.ibb.co/CKbMsdCC/figure9-loss-curve.png)

*HÃ¬nh 7: Loss curve giáº£m dáº§n theo sá»‘ epoch*

</div>

## 3. Pháº§n II: Explainable AI - LÃ m Cho AI CÃ³ Thá»ƒ Hiá»ƒu ÄÆ°á»£c

### 3.1 Lá»i Má»Ÿ: Táº¡i Sao Cáº§n Giáº£i ThÃ­ch MÃ´ HÃ¬nh?

Báº¡n cÃ³ má»™t mÃ´ hÃ¬nh cho Ä‘iá»ƒm tÃ­n dá»¥ng cháº¡y ráº¥t tá»‘t trÃªn thÆ°á»›c Ä‘o AUC vÃ  F1. Má»™t ngÃ y, mÃ´ hÃ¬nh tá»« chá»‘i má»™t há»“ sÆ¡ khÃ¡ch hÃ ng mÃ  nhÃ¢n viÃªn tháº©m Ä‘á»‹nh tin ráº±ng Ä‘Ã¡ng Ä‘Æ°á»£c duyá»‡t. Ban lÃ£nh Ä‘áº¡o há»i: VÃ¬ sao há»‡ thá»‘ng láº¡i ra quyáº¿t Ä‘á»‹nh nhÆ° tháº¿. Báº¡n má»Ÿ dashboard, tháº¥y vÃ i cá»™t Ä‘áº·c trÆ°ng Ä‘Æ°á»£c gÃ¡n táº§m quan trá»ng cao. CÃ¢u tráº£ lá»i Ä‘Ã³ chÆ°a Ä‘á»§. NhÃ¢n viÃªn muá»‘n biáº¿t **ngay trong trÆ°á»ng há»£p nÃ y** mÃ´ hÃ¬nh Ä‘Ã£ nhÃ¬n vÃ o Ä‘iá»u gÃ¬. Manager muá»‘n biáº¿t quyáº¿t Ä‘á»‹nh **á»•n Ä‘á»‹nh** ra sao náº¿u dá»¯ liá»‡u Ä‘áº§u vÃ o biáº¿n Ä‘á»™ng nhá». Ká»¹ sÆ° muá»‘n biáº¿t cÃ¡ch **kiá»ƒm chá»©ng** lá»i giáº£i thÃ­ch.

**Giáº£i thÃ­ch khÃ´ng pháº£i Ä‘á»ƒ trang trÃ­ bÃ¡o cÃ¡o. Má»¥c tiÃªu lÃ  há»— trá»£ ra quyáº¿t Ä‘á»‹nh cÃ³ trÃ¡ch nhiá»‡m, kiá»ƒm thá»­ vÃ  cáº£i thiá»‡n mÃ´ hÃ¬nh trong bá»‘i cáº£nh cá»¥ thá»ƒ.**

### 3.2 Interpretability VÃ  Explainability: Hai KhÃ¡i Niá»‡m Cáº§n PhÃ¢n Biá»‡t

**Diá»…n giáº£i Ä‘Æ°á»£c (interpretability):** Má»©c Ä‘á»™ mÃ  con ngÆ°á»i cÃ³ thá»ƒ hiá»ƒu trá»±c tiáº¿p cÃ¡ch mÃ´ hÃ¬nh Ã¡nh xáº¡ Ä‘áº§u vÃ o sang Ä‘áº§u ra. VÃ­ dá»¥ há»“i quy tuyáº¿n tÃ­nh vá»›i vÃ i Ä‘áº·c trÆ°ng Ä‘Ã£ chuáº©n hoÃ¡ cÃ³ thá»ƒ Ä‘Æ°á»£c xem lÃ  diá»…n giáº£i Ä‘Æ°á»£c.

**Giáº£i thÃ­ch Ä‘Æ°á»£c (explainability):** Kháº£ nÄƒng Ä‘Æ°a ra lá»i giáº£i thÃ­ch vá» hÃ nh vi cá»§a mÃ´ hÃ¬nh, cÃ³ thá»ƒ báº±ng phÆ°Æ¡ng phÃ¡p háº­u kiá»ƒm vÃ  mÃ´ hÃ¬nh thay tháº¿. VÃ­ dá»¥ LIME vÃ  Anchor giáº£i thÃ­ch **cá»¥c bá»™** dá»± Ä‘oÃ¡n cá»§a má»™t mÃ´ hÃ¬nh báº¥t ká»³.

Trong bÃ i viáº¿t nÃ y, chÃºng ta chÃº trá»ng cÃ¡ch giáº£i thÃ­ch háº­u kiá»ƒm cho mÃ´ hÃ¬nh há»™p Ä‘en, Æ°u tiÃªn phÆ°Æ¡ng phÃ¡p báº¥t phá»¥ thuá»™c mÃ´ hÃ¬nh vÃ  Ã¡p dá»¥ng Ä‘Æ°á»£c trong thá»±c táº¿.

### 3.3 Báº£n Äá»“ XAI Trong 10 PhÃºt

#### 3.3.1 PhÃ¢n Loáº¡i Nhanh CÃ¡c HÆ°á»›ng Tiáº¿p Cáº­n

CÃ³ ba trá»¥c phÃ¢n loáº¡i há»¯u Ã­ch Ä‘á»ƒ Ä‘á»‹nh vá»‹ má»™t ká»¹ thuáº­t XAI:

1. **Thá»i Ä‘iá»ƒm can thiá»‡p:** Ante hoc so vá»›i Post hoc
   - **Ante hoc:** thiáº¿t káº¿ mÃ´ hÃ¬nh vá»‘n Ä‘Ã£ dá»… diá»…n giáº£i, vÃ­ dá»¥ há»“i quy tuyáº¿n tÃ­nh thÆ°a hoáº·c cÃ¢y quyáº¿t Ä‘á»‹nh nÃ´ng
   - **Post hoc:** giáº£i thÃ­ch mÃ´ hÃ¬nh cÃ³ sáºµn, thÆ°á»ng lÃ  há»™p Ä‘en. LIME vÃ  Anchor thuá»™c nhÃ³m nÃ y

2. **Má»©c phá»¥ thuá»™c mÃ´ hÃ¬nh:** Phá»¥ thuá»™c mÃ´ hÃ¬nh so vá»›i Báº¥t phá»¥ thuá»™c mÃ´ hÃ¬nh
   - **Phá»¥ thuá»™c mÃ´ hÃ¬nh** dá»±a vÃ o cáº¥u trÃºc vÃ  gradient ná»™i táº¡i, vÃ­ dá»¥ Integrated Gradients
   - **Báº¥t phá»¥ thuá»™c mÃ´ hÃ¬nh** chá»‰ cáº§n truy cáº­p hÃ m dá»± Ä‘oÃ¡n, vÃ­ dá»¥ LIME, Anchor

3. **Pháº¡m vi hiá»‡u lá»±c:** Cá»¥c bá»™ so vá»›i ToÃ n cá»¥c
   - **Cá»¥c bá»™** giáº£i thÃ­ch má»™t dá»± Ä‘oÃ¡n cá»¥ thá»ƒ hoáº·c má»™t vÃ¹ng lÃ¢n cáº­n quanh má»™t Ä‘iá»ƒm dá»¯ liá»‡u
   - **ToÃ n cá»¥c** mÃ´ táº£ xu hÆ°á»›ng vÃ  cáº¥u trÃºc chung cá»§a mÃ´ hÃ¬nh trÃªn toÃ n táº­p dá»¯ liá»‡u

#### 3.3.2 Ba Há» PhÆ°Æ¡ng PhÃ¡p Phá»• Biáº¿n

- **GÃ¡n Ä‘Ã³ng gÃ³p theo Ä‘áº·c trÆ°ng:** vÃ­ dá»¥ LIME, SHAP, saliency dá»±a nhiá»…u. PhÃ¹ há»£p khi ta cáº§n biáº¿t yáº¿u tá»‘ nÃ o kÃ©o dá»± Ä‘oÃ¡n lÃªn hoáº·c xuá»‘ng cho má»™t Ä‘iá»ƒm cá»¥ thá»ƒ
- **Dá»±a trÃªn vÃ­ dá»¥:** vÃ­ dá»¥ prototype, criticism, case based reasoning. Há»¯u Ã­ch khi ngÆ°á»i dÃ¹ng tin tÆ°á»Ÿng báº±ng so sÃ¡nh gáº§n nháº¥t
- **Dá»±a trÃªn quy táº¯c:** vÃ­ dá»¥ Anchor hoáº·c rule list. PhÃ¹ há»£p khi ngÆ°á»i dÃ¹ng Æ°a cÃ¡c má»‡nh Ä‘á» Ä‘iá»u kiá»‡n dá»… kiá»ƒm chá»©ng

| **Ká»¹ thuáº­t** | **Thá»i Ä‘iá»ƒm** | **Phá»¥ thuá»™c mÃ´ hÃ¬nh** | **Pháº¡m vi** |
|---|---|---|---|
| LIME | Post hoc | Báº¥t phá»¥ thuá»™c | Cá»¥c bá»™ |
| Anchor | Post hoc | Báº¥t phá»¥ thuá»™c | Cá»¥c bá»™ |
| SHAP máº«u hoÃ¡ | Post hoc | Báº¥t phá»¥ thuá»™c | Cá»¥c bá»™ Ä‘áº¿n bÃ¡n toÃ n cá»¥c |
| CÃ¢y quyáº¿t Ä‘á»‹nh nÃ´ng | Ante hoc | Ná»n táº£ng mÃ´ hÃ¬nh | ToÃ n cá»¥c |
| Integrated Gradients | Post hoc | Phá»¥ thuá»™c mÃ´ hÃ¬nh | Cá»¥c bá»™ |

### 3.4 Trá»±c GiÃ¡c LIME

**LIME (Local Interpretable Model-agnostic Explanations)** cung cáº¥p má»™t **mÃ´ hÃ¬nh thay tháº¿ cá»¥c bá»™** $g$ Ä‘á»ƒ mÃ´ táº£ hÃ nh vi cá»§a mÃ´ hÃ¬nh gá»‘c $f$ quanh má»™t Ä‘iá»ƒm quan tÃ¢m $x$. Thay vÃ¬ cá»‘ hiá»ƒu toÃ n bá»™ $f$, ta "phÃ³ng to" vÃ o vÃ¹ng lÃ¢n cáº­n cá»§a $x$ báº±ng má»™t thÆ°á»›c Ä‘o gáº§n--xa, sinh cÃ¡c Ä‘iá»ƒm nhiá»…u cÃ³ trá»ng sá»‘ theo Ä‘á»™ gáº§n, rá»“i khá»›p má»™t mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n (thÆ°á»ng lÃ  tuyáº¿n tÃ­nh thÆ°a) Ä‘á»ƒ suy luáº­n Ä‘Ã³ng gÃ³p cá»§a Ä‘áº·c trÆ°ng.

#### 3.4.1 Äá»‹nh NghÄ©a VÃ  HÃ m Má»¥c TiÃªu

Gá»i $\pi_x(z)$ lÃ  trá»ng sá»‘ lÃ¢n cáº­n (kernel) Ä‘o má»©c "gáº§n" giá»¯a $z$ vÃ  $x$, $L$ lÃ  máº¥t mÃ¡t Ä‘o chÃªnh lá»‡ch dá»± Ä‘oÃ¡n giá»¯a $f$ vÃ  mÃ´ hÃ¬nh thay tháº¿ $g$, vÃ  $\Omega(g)$ lÃ  pháº¡t Ä‘á»™ phá»©c táº¡p. LIME tá»‘i Æ°u:
$$g^\star = \arg\min_{g \in \mathcal{G}} L(f,g,\pi_x) + \Omega(g)$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **g***: mÃ´ hÃ¬nh thay tháº¿ tá»‘i Æ°u (optimal surrogate model)
- **arg min**: tÃ¬m Ä‘á»‘i sá»‘ (argument) lÃ m cho hÃ m Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t
- **g âˆˆ G**: mÃ´ hÃ¬nh thay tháº¿ g thuá»™c táº­p cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ G (thÆ°á»ng lÃ  mÃ´ hÃ¬nh tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n)
- **L(f,g,Ï€_x)**: hÃ m máº¥t mÃ¡t Ä‘o Ä‘á»™ khÃ¡c biá»‡t giá»¯a mÃ´ hÃ¬nh gá»‘c f vÃ  mÃ´ hÃ¬nh thay tháº¿ g
- **Ï€_x**: phÃ¢n phá»‘i trá»ng sá»‘ lÃ¢n cáº­n quanh Ä‘iá»ƒm x cáº§n giáº£i thÃ­ch
- **Î©(g)**: hÃ m pháº¡t Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh g (regularization term)
- **Má»¥c Ä‘Ã­ch**: tÃ¬m mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n nháº¥t cÃ³ thá»ƒ mÃ´ táº£ hÃ nh vi cá»§a mÃ´ hÃ¬nh phá»©c táº¡p quanh Ä‘iá»ƒm x

Vá»›i phÃ¢n loáº¡i nhá»‹ phÃ¢n, $L$ thÆ°á»ng lÃ  máº¥t mÃ¡t logistic hoáº·c bÃ¬nh phÆ°Æ¡ng cÃ³ trá»ng sá»‘; vá»›i há»“i quy, thÆ°á»ng lÃ  MSE cÃ³ trá»ng sá»‘:
$$L(f,g,\pi_x) = \mathbb{E}_{z \sim \pi_x}\!\Big[w(z)\,\ell\!\big(f(z),g(z)\big)\Big], \quad w(z)=\pi_x(z)$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **L(f,g,Ï€_x)**: hÃ m máº¥t mÃ¡t cÃ³ trá»ng sá»‘ cá»§a LIME
- **E_{z~Ï€_x}**: ká»³ vá»ng (expectation) theo phÃ¢n phá»‘i Ï€_x quanh Ä‘iá»ƒm x
- **w(z)**: trá»ng sá»‘ cá»§a Ä‘iá»ƒm z, báº±ng Ï€_x(z)
- **â„“(f(z),g(z))**: hÃ m máº¥t mÃ¡t cÆ¡ báº£n giá»¯a dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh gá»‘c f(z) vÃ  mÃ´ hÃ¬nh thay tháº¿ g(z)
- **f(z)**: dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh gá»‘c táº¡i Ä‘iá»ƒm z
- **g(z)**: dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh thay tháº¿ táº¡i Ä‘iá»ƒm z
- **Má»¥c Ä‘Ã­ch**: Ä‘o Ä‘á»™ khÃ¡c biá»‡t giá»¯a hai mÃ´ hÃ¬nh, cÃ³ trá»ng sá»‘ cao cho cÃ¡c Ä‘iá»ƒm gáº§n x

#### 3.4.2 Quy TrÃ¬nh LIME Theo Tá»«ng BÆ°á»›c

1. **Chá»n Ä‘iá»ƒm cáº§n giáº£i thÃ­ch** $x$ vÃ  lá»›p Ä‘Ã­ch (náº¿u phÃ¢n loáº¡i Ä‘a lá»›p)
2. **Sinh máº«u lÃ¢n cáº­n:** táº¡o $N$ biáº¿n thá»ƒ $z_i' \sim q(\cdot\,|\,x')$ báº±ng cÃ¡ch báº­t/táº¯t cÃ¡c thÃ nh pháº§n cá»§a $x'$ rá»“i Ã¡nh xáº¡ $z_i=\phi(z_i')$
3. **GÃ¡n trá»ng sá»‘ lÃ¢n cáº­n:** $w_i=\pi_x(z_i)$ vá»›i khoáº£ng cÃ¡ch $D$ phÃ¹ há»£p vÃ  kernel width $\sigma$
4. **Gá»i mÃ´ hÃ¬nh gá»‘c:** láº¥y $y_i=f(z_i)$
5. **Khá»›p mÃ´ hÃ¬nh thay tháº¿** $g$: thÆ°á»ng dÃ¹ng há»“i quy tuyáº¿n tÃ­nh thÆ°a Ä‘á»ƒ tá»‘i thiá»ƒu $L+\Omega$
6. **TrÃ¬nh bÃ y lá»i giáº£i thÃ­ch:** há»‡ sá»‘ $\beta_j$ cá»§a $g$ cho biáº¿t má»©c Ä‘Ã³ng gÃ³p cá»¥c bá»™ cá»§a Ä‘áº·c trÆ°ng $j$ vÃ o dá»± Ä‘oÃ¡n táº¡i $x$

#### 3.4.3 CÃ¡c TiÃªu ChÃ­ ÄÃ¡nh GiÃ¡ Lá»i Giáº£i ThÃ­ch

**Fidelity cá»¥c bá»™:** Má»©c Ä‘á»™ mÃ´ hÃ¬nh thay tháº¿ hoáº·c quy táº¯c tÃ¡i hiá»‡n hÃ nh vi cá»§a mÃ´ hÃ¬nh gá»‘c quanh Ä‘iá»ƒm Ä‘ang xÃ©t:
$$\mathrm{Fid}(x) = \mathbb{E}_{z \sim \pi_x}\big[\ell\big(f(z),g(z)\big)\big]$$

**ChÃº thÃ­ch chi tiáº¿t:**
- **Fid(x)**: Ä‘á»™ trung thá»±c cá»¥c bá»™ (local fidelity) táº¡i Ä‘iá»ƒm x
- **E_{z~Ï€_x}**: ká»³ vá»ng theo phÃ¢n phá»‘i lÃ¢n cáº­n Ï€_x quanh Ä‘iá»ƒm x
- **â„“(f(z),g(z))**: hÃ m máº¥t mÃ¡t giá»¯a dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh gá»‘c f(z) vÃ  mÃ´ hÃ¬nh thay tháº¿ g(z)
- **f(z)**: dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh gá»‘c táº¡i Ä‘iá»ƒm z
- **g(z)**: dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh thay tháº¿ táº¡i Ä‘iá»ƒm z
- **Má»¥c Ä‘Ã­ch**: Ä‘o má»©c Ä‘á»™ mÃ´ hÃ¬nh thay tháº¿ g cÃ³ thá»ƒ tÃ¡i hiá»‡n hÃ nh vi cá»§a mÃ´ hÃ¬nh gá»‘c f quanh Ä‘iá»ƒm x
- **GiÃ¡ trá»‹**: cÃ ng nhá» cÃ ng tá»‘t - cho tháº¥y mÃ´ hÃ¬nh thay tháº¿ cÃ ng giá»‘ng mÃ´ hÃ¬nh gá»‘c

**Stability:** Má»©c Ä‘á»™ lá»i giáº£i thÃ­ch Ã­t thay Ä‘á»•i khi láº·p láº¡i vá»›i háº¡t giá»‘ng ngáº«u nhiÃªn khÃ¡c, vá»›i nhiá»…u nhá» á»Ÿ Ä‘áº§u vÃ o, hoáº·c vá»›i cáº¥u hÃ¬nh lÃ¢n cáº­n.

**Sparsity:** Má»©c Ä‘á»™ gá»n cá»§a lá»i giáº£i thÃ­ch, vÃ­ dá»¥ sá»‘ Ä‘áº·c trÆ°ng Ä‘Æ°á»£c chá»n hoáº·c Ä‘á»™ dÃ i quy táº¯c.

**Coverage:** Vá»›i quy táº¯c, pháº§n trÄƒm cÃ¡c Ä‘iá»ƒm trÃªn phÃ¢n phá»‘i dá»¯ liá»‡u mÃ  quy táº¯c cÃ³ thá»ƒ Ã¡p dá»¥ng.

### 3.5 Anchor: Quy Táº¯c CÃ³ Äá»™ Tin Cáº­y Cao

**Anchor** tÃ¬m cÃ¡c quy táº¯c dáº¡ng if-then cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao trong má»™t miá»n Ã¡p dá»¥ng nháº¥t Ä‘á»‹nh, kÃ¨m Ä‘á»™ bao phá»§ Ä‘á»ƒ nÃ³i rÃµ pháº¡m vi mÃ  quy táº¯c cÃ³ hiá»‡u lá»±c.

#### 3.5.1 Ã TÆ°á»Ÿng Cá»‘t LÃµi

Anchor tiáº¿p cáº­n theo dáº¡ng quy táº¯c thoáº£ ngÆ°á»¡ng Ä‘á»™ chÃ­nh xÃ¡c, Ä‘á»“ng thá»i tÃ¬m Ä‘á»™ bao phá»§ lá»›n nháº¥t trong pháº¡m vi cÃ²n giá»¯ Ä‘Æ°á»£c Ä‘á»™ tin cáº­y. VÃ­ dá»¥: "Náº¿u tuá»•i > 30 VÃ€ thu nháº­p > 50,000,000 VND thÃ¬ mÃ´ hÃ¬nh sáº½ tá»« chá»‘i há»“ sÆ¡ vá»›i Ä‘á»™ chÃ­nh xÃ¡c 95% vÃ  Ä‘á»™ bao phá»§ 15%".

#### 3.5.2 Quy TrÃ¬nh Anchor

1. **Khá»Ÿi táº¡o:** Báº¯t Ä‘áº§u vá»›i má»™t quy táº¯c rá»—ng
2. **Má»Ÿ rá»™ng:** ThÃªm cÃ¡c Ä‘iá»u kiá»‡n Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c
3. **Kiá»ƒm tra:** ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ bao phá»§
4. **Tá»‘i Æ°u:** CÃ¢n báº±ng giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ bao phá»§

### 3.6 Khi NÃ o DÃ¹ng PhÆ°Æ¡ng PhÃ¡p Cá»¥c Bá»™

PhÆ°Æ¡ng phÃ¡p cá»¥c bá»™ há»¯u Ã­ch khi cÃ¢u há»i nghiá»‡p vá»¥ mang tÃ­nh tá»«ng ca cá»¥ thá»ƒ nhÆ° tá»« chá»‘i má»™t há»“ sÆ¡ tÃ­n dá»¥ng, gá»£i Ã½ má»™t Ä‘Æ¡n thuá»‘c, hay duyá»‡t má»™t giao dá»‹ch. Khi Ä‘Ã³ ta quan tÃ¢m vÃ¹ng lÃ¢n cáº­n cá»§a Ä‘iá»ƒm $x$, Ä‘áº·c trÆ°ng bá»Ÿi má»™t **phÃ¢n phá»‘i lÃ¢n cáº­n** $\pi_x$.

**SÆ¡ Ä‘á»“ quyáº¿t Ä‘á»‹nh nhanh Ä‘á»ƒ chá»n cÃ´ng cá»¥:**

1. **Báº¡n cáº§n lá»i giáº£i thÃ­ch cho má»™t ca cá»¥ thá»ƒ hay cho bá»©c tranh chung**
   - TrÆ°á»ng há»£p cá»¥ thá»ƒ: Æ°u tiÃªn phÆ°Æ¡ng phÃ¡p cá»¥c bá»™ nhÆ° LIME hoáº·c Anchor
   - Bá»©c tranh chung: cÃ¢n nháº¯c mÃ´ hÃ¬nh diá»…n giáº£i Ä‘Æ¡n giáº£n, phÃ¢n tÃ­ch toÃ n cá»¥c

2. **NgÆ°á»i dÃ¹ng má»¥c tiÃªu muá»‘n Ä‘á»c gÃ¬**
   - Äiá»ƒm cá»™ng trá»« theo Ä‘áº·c trÆ°ng: LIME
   - Má»‡nh Ä‘á» Ä‘iá»u kiá»‡n dá»… kiá»ƒm tra: Anchor

3. **RÃ ng buá»™c tÃ­nh toÃ¡n**
   - Chi phÃ­ láº¥y máº«u háº¡n cháº¿: giá»›i háº¡n sá»‘ máº«u vÃ  kÃ­ch thÆ°á»›c lÃ¢n cáº­n
   - Dá»¯ liá»‡u hÃ¬nh áº£nh: chÃº Ã½ phÃ¢n Ä‘oáº¡n superpixel vÃ  tÃ¡c Ä‘á»™ng Ä‘áº¿n á»•n Ä‘á»‹nh lá»i giáº£i thÃ­ch

## 4. Pháº§n III: MLOps - Tá»« Há»—n Loáº¡n Thá»­ Nghiá»‡m Äáº¿n Váº­n HÃ nh Xuáº¥t Sáº¯c

### 4.1 Lá»i Má»Ÿ Äáº§u: CÆ¡n Ãc Má»™ng LÃºc 3 Giá» SÃ¡ng

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n lÃ  má»™t ká»¹ sÆ° trong má»™t cÃ´ng ty thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ hÃ ng Ä‘áº§u táº¡i Viá»‡t Nam. Äá»™i ngÅ© Khoa há»c Dá»¯ liá»‡u (Data Science) vá»«a cho ra máº¯t má»™t há»‡ thá»‘ng gá»£i Ã½ sáº£n pháº©m (recommendation engine) vÃ´ cÃ¹ng thÃ´ng minh. Má»i chá»‰ sá»‘ trong mÃ´i trÆ°á»ng thá»­ nghiá»‡m Ä‘á»u hoÃ n háº£o. Ban lÃ£nh Ä‘áº¡o ká»³ vá»ng doanh thu sáº½ tÄƒng vá»t.

Tháº¿ rá»“i, vÃ o má»™t Ä‘Ãªm thá»© Báº£y, lÃºc 3 giá» sÃ¡ng, Ä‘iá»‡n thoáº¡i cá»§a báº¡n reo lÃªn liÃªn há»“i. Há»‡ thá»‘ng cáº£nh bÃ¡o kháº©n cáº¥p: Doanh sá»‘ táº¡i SÃ i GÃ²n sá»¥t giáº£m tháº£m háº¡i. Khi kiá»ƒm tra, báº¡n bÃ ng hoÃ ng phÃ¡t hiá»‡n ra há»‡ thá»‘ng Ä‘ang gá»£i Ã½... **Ã¡o phao vÃ  Ã¡o giá»¯ nhiá»‡t cho ngÆ°á»i dÃ¹ng á»Ÿ SÃ i GÃ²n, giá»¯a lÃºc thÃ nh phá»‘ Ä‘ang tráº£i qua Ä‘á»£t náº¯ng nÃ³ng Ä‘á»‰nh Ä‘iá»ƒm.**

<div align="center">

![Sá»± cá»‘ váº­n hÃ nh cÃ³ thá»ƒ xáº£y ra báº¥t cá»© lÃºc nÃ o](https://i.ibb.co/bMMJXscX/3am-crisis.png)

*HÃ¬nh 8: Sá»± cá»‘ váº­n hÃ nh cÃ³ thá»ƒ xáº£y ra báº¥t cá»© lÃºc nÃ o trong há»‡ thá»‘ng ML*

</div>

Má»™t loáº¡t cÃ¢u há»i hiá»‡n ra trong Ä‘áº§u báº¡n:
- PhiÃªn báº£n mÃ´ hÃ¬nh nÃ o Ä‘ang cháº¡y trÃªn production?
- NÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn bá»™ dá»¯ liá»‡u nÃ o?
- LÃ m tháº¿ nÃ o mÃ  nÃ³ cÃ³ thá»ƒ vÆ°á»£t qua Ä‘Æ°á»£c cÃ¡c khÃ¢u kiá»ƒm thá»­?
- Ai Ä‘Ã£ triá»ƒn khai nÃ³ vÃ  vÃ o lÃºc nÃ o?

Náº¿u báº¡n khÃ´ng thá»ƒ tráº£ lá»i nhá»¯ng cÃ¢u há»i nÃ y má»™t cÃ¡ch nhanh chÃ³ng, báº¡n khÃ´ng Ä‘Æ¡n Ä‘á»™c. ÄÃ¢y chÃ­nh lÃ  "cÆ¡n Ã¡c má»™ng" mÃ  ráº¥t nhiá»u tá»• chá»©c Ä‘Ã£ vÃ  Ä‘ang pháº£i Ä‘á»‘i máº·t. NÃ³ phÆ¡i bÃ y má»™t sá»± tháº­t tráº§n trá»¥i: viá»‡c xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh Machine Learning (ML) hoáº¡t Ä‘á»™ng tá»‘t trÃªn Jupyter Notebook chá»‰ lÃ  **10% cá»§a táº£ng bÄƒng chÃ¬m**. 90% cÃ²n láº¡i, pháº§n phá»©c táº¡p vÃ  quyáº¿t Ä‘á»‹nh sá»± thÃ nh báº¡i cá»§a má»™t dá»± Ã¡n AI, náº±m á»Ÿ viá»‡c váº­n hÃ nh, duy trÃ¬ vÃ  quáº£n lÃ½ nÃ³ má»™t cÃ¡ch bá»n vá»¯ng trong mÃ´i trÆ°á»ng thá»±c táº¿.

<div align="center">

![Pháº§n chÃ¬m cá»§a táº£ng bÄƒng trong cÃ¡c há»‡ thá»‘ng ML](https://i.ibb.co/GvxDV7t8/hidden-complexity.png)

*HÃ¬nh 9: Pháº§n chÃ¬m cá»§a táº£ng bÄƒng trong cÃ¡c há»‡ thá»‘ng ML*

</div>

VÃ  Ä‘Ã³ chÃ­nh lÃ  lÃ½ do **MLOps** ra Ä‘á»i. NÃ³ khÃ´ng pháº£i lÃ  má»™t cÃ´ng cá»¥, cÅ©ng khÃ´ng pháº£i lÃ  má»™t cÃ´ng nghá»‡ Ä‘Æ¡n láº». MLOps lÃ  má»™t triáº¿t lÃ½, má»™t vÄƒn hÃ³a, má»™t táº­p há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p thá»±c hÃ nh tá»‘t nháº¥t nháº±m thu háº¹p khoáº£ng cÃ¡ch giá»¯a tháº¿ giá»›i thá»­ nghiá»‡m cá»§a cÃ¡c nhÃ  khoa há»c dá»¯ liá»‡u vÃ  tháº¿ giá»›i váº­n hÃ nh cá»§a cÃ¡c ká»¹ sÆ°.

### 4.2 HÃ nh TrÃ¬nh Lá»‹ch Sá»­: Táº¡i Sao MLOps LÃ  Má»™t Äiá»u Táº¥t Yáº¿u?

#### 4.2.1 Nhá»¯ng NÄƒm Ná»n Táº£ng (1960s - 1990s): Giáº¥c MÆ¡ Ban Äáº§u

Giai Ä‘oáº¡n nÃ y chá»©ng kiáº¿n sá»± ra Ä‘á»i cá»§a cÃ¡c khÃ¡i niá»‡m sÆ¡ khai nhÆ° máº¡ng nÆ¡-ron Perceptron. AI lÃºc nÃ y chá»§ yáº¿u náº±m trong cÃ¡c phÃ²ng thÃ­ nghiá»‡m, vá»›i nhiá»u ká»³ vá»ng nhÆ°ng cÅ©ng nhanh chÃ³ng rÆ¡i vÃ o "mÃ¹a Ä‘Ã´ng AI" do nhá»¯ng háº¡n cháº¿ vá» nÄƒng lá»±c tÃ­nh toÃ¡n vÃ  dá»¯ liá»‡u.

#### 4.2.2 Thá»i Ká»³ Phá»¥c HÆ°ng (2000s - 2010): Sá»± Trá»—i Dáº­y Cá»§a Deep Learning

Má»i thá»© thay Ä‘á»•i vÃ o nhá»¯ng nÄƒm 2000 vÃ  Ä‘áº·c biá»‡t lÃ  sau 2010. Ba yáº¿u tá»‘ cÃ¹ng há»™i tá»¥:

1. **Äá»™t phÃ¡ vá» thuáº­t toÃ¡n:** NghiÃªn cá»©u cá»§a Geoffrey Hinton Ä‘Ã£ khÆ¡i láº¡i cuá»™c cÃ¡ch máº¡ng vá» Deep Learning
2. **Sá»©c máº¡nh tÃ­nh toÃ¡n:** Sá»± chuyá»ƒn dá»‹ch tá»« CPU sang GPU Ä‘Ã£ cho phÃ©p huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n ráº¥t nhiá»u
3. **Dá»¯ liá»‡u lá»›n (Big Data):** Internet bÃ¹ng ná»•, táº¡o ra nguá»“n "nhiÃªn liá»‡u" khá»•ng lá»“ cho cÃ¡c mÃ´ hÃ¬nh ML

#### 4.2.3 Ká»· NguyÃªn CÃ´ng Nghiá»‡p HÃ³a (2010 - 2015): "Váº¥n Äá» Chiáº¿c Laptop"

CÃ¡c cÃ´ng ty báº¯t Ä‘áº§u á»“ áº¡t triá»ƒn khai ML. Tuy nhiÃªn, há» nhanh chÃ³ng Ä‘á»‘i máº·t vá»›i má»™t thá»±c táº¿ phÅ© phÃ ng: má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng hoÃ n háº£o trÃªn laptop cá»§a nhÃ  khoa há»c dá»¯ liá»‡u láº¡i tháº¥t báº¡i tháº£m háº¡i khi Ä‘Æ°a lÃªn mÃ´i trÆ°á»ng production.

- **MÃ´i trÆ°á»ng khÃ´ng nháº¥t quÃ¡n:** ThÆ° viá»‡n, phiÃªn báº£n Python, vÃ  cáº¥u hÃ¬nh trÃªn mÃ¡y cÃ¡ nhÃ¢n khÃ¡c xa so vá»›i mÃ¡y chá»§
- **Dá»¯ liá»‡u Ä‘á»™ng:** Dá»¯ liá»‡u thá»±c táº¿ luÃ´n thay Ä‘á»•i, khÃ´ng "sáº¡ch" vÃ  tÄ©nh nhÆ° dá»¯ liá»‡u huáº¥n luyá»‡n
- **YÃªu cáº§u vá» quy mÃ´ vÃ  Ä‘á»™ tin cáº­y:** Production Ä‘Ã²i há»i kháº£ nÄƒng phá»¥c vá»¥ hÃ ng triá»‡u ngÆ°á»i dÃ¹ng vÃ  pháº£i hoáº¡t Ä‘á»™ng 24/7

Khoáº£ng cÃ¡ch giá»¯a nghiÃªn cá»©u vÃ  sáº£n xuáº¥t ngÃ y cÃ ng lá»›n, táº¡o ra má»™t "nÃºt tháº¯t cá»• chai" khá»•ng lá»“. 87% cÃ¡c dá»± Ã¡n ML khÃ´ng bao giá» Ä‘áº¿n Ä‘Æ°á»£c tay ngÆ°á»i dÃ¹ng cuá»‘i.

<div align="center">

![Sá»± khÃ¡c biá»‡t giá»¯a mÃ´i trÆ°á»ng NghiÃªn cá»©u vÃ  Production](https://i.ibb.co/1JfCmLK1/research-vs-production.png)

*HÃ¬nh 10: Sá»± khÃ¡c biá»‡t giá»¯a mÃ´i trÆ°á»ng NghiÃªn cá»©u vÃ  Production*

</div>

#### 4.2.4 Sá»± Ra Äá»i Cá»§a MLOps (2015 - 2018): Lá»i Giáº£i Cho BÃ i ToÃ¡n Váº­n HÃ nh

Cá»™ng Ä‘á»“ng nháº­n ra ráº±ng Ä‘á»ƒ "cÃ´ng nghiá»‡p hÃ³a" AI thÃ nh cÃ´ng, chÃºng ta cáº§n má»™t phÆ°Æ¡ng phÃ¡p luáº­n má»›i. Thuáº­t ngá»¯ "MLOps" ra Ä‘á»i, lÃ  sá»± káº¿t há»£p cÃ¡c nguyÃªn táº¯c cá»§a **DevOps** vá»›i cÃ¡c quy trÃ¬nh Ä‘áº·c thÃ¹ cá»§a **Machine Learning** vÃ  **Data Engineering**. MLOps táº­p trung giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c cá»‘t lÃµi:

- **Kháº£ nÄƒng tÃ¡i láº­p (Reproducibility):** Äáº£m báº£o cÃ³ thá»ƒ táº¡o láº¡i mÃ´ hÃ¬nh má»™t cÃ¡ch nháº¥t quÃ¡n
- **Quáº£n lÃ½ phiÃªn báº£n (Versioning):** Theo dÃµi sá»± thay Ä‘á»•i cá»§a cáº£ code, dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh
- **Triá»ƒn khai (Deployment):** ÄÆ°a mÃ´ hÃ¬nh lÃªn production má»™t cÃ¡ch Ä‘Ã¡ng tin cáº­y
- **GiÃ¡m sÃ¡t (Monitoring):** Theo dÃµi hiá»‡u suáº¥t vÃ  phÃ¡t hiá»‡n cÃ¡c váº¥n Ä‘á» nhÆ° "data drift"
- **Quáº£n trá»‹ (Governance):** Äáº£m báº£o tuÃ¢n thá»§ vÃ  sá»­ dá»¥ng AI má»™t cÃ¡ch cÃ³ trÃ¡ch nhiá»‡m

### 4.3 MLOps vs. DevOps: NgÆ°á»i Thá»«a Káº¿ Hay Má»™t Thá»±c Thá»ƒ HoÃ n ToÃ n Má»›i?

Má»™t cÃ¢u há»i tÃ´i thÆ°á»ng gáº·p lÃ : "MLOps cÃ³ pháº£i chá»‰ lÃ  DevOps dÃ nh cho Machine Learning khÃ´ng?" CÃ¢u tráº£ lá»i lÃ  **vá»«a Ä‘Ãºng, vá»«a khÃ´ng**. MLOps thá»«a hÆ°á»Ÿng triáº¿t lÃ½ cá»‘t lÃµi cá»§a DevOps, tuy nhiÃªn, há»‡ thá»‘ng ML cÃ³ nhá»¯ng Ä‘áº·c thÃ¹ ráº¥t riÃªng.

#### 4.3.1 Analogy: NhÃ  HÃ ng

HÃ£y tÆ°á»Ÿng tÆ°á»£ng DevOps giá»‘ng nhÆ° viá»‡c váº­n hÃ nh má»™t chuá»—i nhÃ  hÃ ng thá»©c Äƒn nhanh. Má»i thá»© Ä‘á»u cÃ³ cÃ´ng thá»©c chuáº©n, quy trÃ¬nh láº¯p rÃ¡p (build) vÃ  phá»¥c vá»¥ (deploy) Ä‘Æ°á»£c tá»± Ä‘á»™ng hÃ³a tá»‘i Ä‘a.

MLOps thÃ¬ giá»‘ng nhÆ° viá»‡c váº­n hÃ nh má»™t nhÃ  hÃ ng sao Michelin:

- **NguyÃªn liá»‡u (Dá»¯ liá»‡u) lÃ  Vua:** Cháº¥t lÆ°á»£ng mÃ³n Äƒn phá»¥ thuá»™c tuyá»‡t Ä‘á»‘i vÃ o Ä‘á»™ tÆ°Æ¡i ngon cá»§a nguyÃªn liá»‡u vÃ  cÃ³ thá»ƒ thay Ä‘á»•i theo mÃ¹a (data drift)
- **CÃ´ng thá»©c (MÃ´ hÃ¬nh) mang tÃ­nh thá»­ nghiá»‡m:** Báº¿p trÆ°á»Ÿng (nhÃ  khoa há»c dá»¯ liá»‡u) liÃªn tá»¥c thá»­ nghiá»‡m cÃ¡c cÃ´ng thá»©c má»›i. Cáº§n pháº£i cÃ³ má»™t há»‡ thá»‘ng Ä‘á»ƒ ghi láº¡i táº¥t cáº£ cÃ¡c thá»­ nghiá»‡m nÃ y
- **Cháº¥t lÆ°á»£ng mÃ³n Äƒn (Hiá»‡u suáº¥t mÃ´ hÃ¬nh) cÃ³ thá»ƒ suy giáº£m:** Má»™t mÃ³n Äƒn Ä‘Æ°á»£c yÃªu thÃ­ch hÃ´m nay cÃ³ thá»ƒ trá»Ÿ nÃªn nhÃ m chÃ¡n vÃ o ngÃ y mai (concept drift). Cáº§n liÃªn tá»¥c theo dÃµi pháº£n há»“i Ä‘á»ƒ Ä‘iá»u chá»‰nh

<div align="center">

![Váº­n hÃ nh má»™t há»‡ thá»‘ng ML giá»‘ng nhÆ° Ä‘iá»u hÃ nh má»™t nhÃ  báº¿p chuyÃªn nghiá»‡p](https://i.ibb.co/QjcwJhR0/restaurant-analogy.png)

*HÃ¬nh 11: Váº­n hÃ nh má»™t há»‡ thá»‘ng ML giá»‘ng nhÆ° Ä‘iá»u hÃ nh má»™t nhÃ  báº¿p chuyÃªn nghiá»‡p*

</div>

#### 4.3.2 Nhá»¯ng Äiá»ƒm KhÃ¡c Biá»‡t Cá»‘t LÃµi

| **KhÃ­a cáº¡nh** | **DevOps** | **MLOps** |
|---|---|---|
| **ThÃ nh pháº§n chÃ­nh** | MÃ£ nguá»“n (Code), Háº¡ táº§ng (Infrastructure) | Code, **Dá»¯ liá»‡u (Data)**, **MÃ´ hÃ¬nh (Models)** |
| **Táº­p trung kiá»ƒm thá»­** | Chá»©c nÄƒng, TÃ­ch há»£p, Hiá»‡u nÄƒng há»‡ thá»‘ng | Cháº¥t lÆ°á»£ng dá»¯ liá»‡u, Hiá»‡u suáº¥t mÃ´ hÃ¬nh, **Sá»± suy giáº£m hiá»‡u suáº¥t (Drift)** |
| **Quáº£n lÃ½ phiÃªn báº£n** | Code, Cáº¥u hÃ¬nh | Code, Cáº¥u hÃ¬nh, **Dá»¯ liá»‡u**, **MÃ´ hÃ¬nh**, **CÃ¡c thá»­ nghiá»‡m** |
| **GiÃ¡m sÃ¡t** | Sá»©c khá»e há»‡ thá»‘ng (CPU, RAM), Logs | Sá»©c khá»e há»‡ thá»‘ng + **Äá»™ trÃ´i dá»¯ liá»‡u (Data Drift)**, **Äá»™ trÃ´i khÃ¡i niá»‡m (Concept Drift)**, **Cháº¥t lÆ°á»£ng dá»± Ä‘oÃ¡n** |
| **VÃ²ng Ä‘á»i phÃ¡t triá»ƒn** | Tuyáº¿n tÃ­nh hÆ¡n (Plan -> Code -> Build -> Test -> Deploy) | Mang tÃ­nh thá»­ nghiá»‡m vÃ  láº·p láº¡i cao (**Data -> Model -> Deploy -> Monitor -> Retrain**) |

Sá»± xuáº¥t hiá»‡n cá»§a **Dá»¯ liá»‡u** vÃ  **MÃ´ hÃ¬nh** nhÆ° nhá»¯ng "cÃ´ng dÃ¢n háº¡ng nháº¥t" (first-class citizens) Ä‘Ã£ lÃ m thay Ä‘á»•i hoÃ n toÃ n cuá»™c chÆ¡i.

<div align="center">

![VÃ²ng Ä‘á»i MLOps má»Ÿ rá»™ng vá»›i cÃ¡c giai Ä‘oáº¡n Ä‘áº·c thÃ¹](https://i.ibb.co/Q7WRpV39/mlops-lifecycle.png)

*HÃ¬nh 12: VÃ²ng Ä‘á»i MLOps má»Ÿ rá»™ng vá»›i cÃ¡c giai Ä‘oáº¡n Ä‘áº·c thÃ¹*

</div>

### 4.4 CÃ¡c Trá»¥ Cá»™t Cá»‘t LÃµi Cá»§a MLOps

Má»™t há»‡ thá»‘ng MLOps trÆ°á»Ÿng thÃ nh Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn nhiá»u trá»¥ cá»™t. DÆ°á»›i Ä‘Ã¢y lÃ  nhá»¯ng trá»¥ cá»™t quan trá»ng nháº¥t mÃ  báº¥t ká»³ ká»¹ sÆ° nÃ o cÅ©ng cáº§n náº¯m vá»¯ng.

#### 4.4.1 Quáº£n LÃ½ PhiÃªn Báº£n ToÃ n Diá»‡n (Version Everything)

ÄÃ¢y lÃ  ná»n táº£ng cá»§a má»i thá»©. Trong MLOps, chÃºng ta khÃ´ng chá»‰ `git commit` mÃ£ nguá»“n.

- **Version Code:** Sá»­ dá»¥ng Git nhÆ° thÃ´ng thÆ°á»ng Ä‘á»ƒ quáº£n lÃ½ code tiá»n xá»­ lÃ½, huáº¥n luyá»‡n, vÃ  triá»ƒn khai
- **Version Data:** Dá»¯ liá»‡u lÃ  "mÃ£ nguá»“n" cá»§a mÃ´ hÃ¬nh. CÃ¡c cÃ´ng cá»¥ nhÆ° **DVC (Data Version Control)** hay Pachyderm cho phÃ©p chÃºng ta "version" dá»¯ liá»‡u
- **Version Model:** Má»—i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n lÃ  má»™t "artifact" cáº§n Ä‘Æ°á»£c lÆ°u trá»¯ vÃ  quáº£n lÃ½ phiÃªn báº£n thÃ´ng qua cÃ¡c Model Registry (nhÆ° trong MLflow, SageMaker)

#### 4.4.2 Tá»± Äá»™ng HÃ³a Quy TrÃ¬nh (Automated Pipelines - CI/CD for ML)

Tá»± Ä‘á»™ng hÃ³a lÃ  trÃ¡i tim cá»§a MLOps. Má»™t quy trÃ¬nh ML (ML Pipeline) tá»± Ä‘á»™ng hÃ³a táº¥t cáº£ cÃ¡c bÆ°á»›c tá»« dá»¯ liá»‡u thÃ´ Ä‘áº¿n mÃ´ hÃ¬nh trÃªn production.

- **Continuous Integration (CI):** Bao gá»“m kiá»ƒm thá»­ vÃ  xÃ¡c thá»±c Code, Dá»¯ liá»‡u, vÃ  MÃ´ hÃ¬nh
- **Continuous Deployment (CD):** Bao gá»“m Ä‘Ã³ng gÃ³i vÃ  triá»ƒn khai MÃ´ hÃ¬nh má»™t cÃ¡ch tá»± Ä‘á»™ng, thÆ°á»ng sá»­ dá»¥ng cÃ¡c chiáº¿n lÆ°á»£c nhÆ° Canary Release hoáº·c A/B Testing

#### 4.4.3 GiÃ¡m SÃ¡t LiÃªn Tá»¥c (Continuous Monitoring)

CÃ´ng viá»‡c cá»§a má»™t ká»¹ sÆ° MLOps khÃ´ng káº¿t thÃºc khi mÃ´ hÃ¬nh Ä‘Æ°á»£c triá»ƒn khai.

- **GiÃ¡m sÃ¡t Há»‡ thá»‘ng:** Theo dÃµi cÃ¡c chá»‰ sá»‘ váº­n hÃ nh nhÆ° Ä‘á»™ trá»… (latency), lÆ°u lÆ°á»£ng (traffic), tá»· lá»‡ lá»—i (error rate)
- **GiÃ¡m sÃ¡t Hiá»‡u suáº¥t MÃ´ hÃ¬nh:** Theo dÃµi cÃ¡c chá»‰ sá»‘ nghiá»‡p vá»¥ (business metrics) nhÆ° tá»· lá»‡ click, tá»· lá»‡ chuyá»ƒn Ä‘á»•i
- **GiÃ¡m sÃ¡t Äá»™ trÃ´i (Drift Detection):** PhÃ¡t hiá»‡n **Data Drift** vÃ  **Concept Drift** Ä‘á»ƒ kÃ­ch hoáº¡t cáº£nh bÃ¡o hoáº·c quy trÃ¬nh huáº¥n luyá»‡n láº¡i (retraining)

#### 4.4.4 Quáº£n Trá»‹ VÃ  Kháº£ NÄƒng Giáº£i ThÃ­ch (Governance & Explainability)

- **Model Lineage:** Kháº£ nÄƒng truy váº¿t nguá»“n gá»‘c cá»§a má»™t mÃ´ hÃ¬nh: nÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« code nÃ o, dá»¯ liá»‡u nÃ o, bá»Ÿi ai, vÃ  khi nÃ o
- **Explainability:** Sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t nhÆ° SHAP hoáº·c LIME Ä‘á»ƒ giáº£i thÃ­ch cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh, giÃºp xÃ¢y dá»±ng lÃ²ng tin vÃ  tuÃ¢n thá»§ cÃ¡c quy Ä‘á»‹nh

### 4.5 MLOps Thá»±c Chiáº¿n: Há»c Há»i Tá»« Nhá»¯ng NgÆ°á»i Khá»•ng Lá»“

#### 4.5.1 Netflix: Metaflow - Äáº·t Con NgÆ°á»i VÃ o Trung TÃ¢m

Há» xÃ¢y dá»±ng **Metaflow**, má»™t framework cho phÃ©p cÃ¡c nhÃ  khoa há»c dá»¯ liá»‡u dá»… dÃ ng má»Ÿ rá»™ng quy mÃ´ tá»« local lÃªn cloud mÃ  khÃ´ng cáº§n thay Ä‘á»•i code. Triáº¿t lÃ½ cá»§a há» lÃ : **HÃ£y Ä‘á»ƒ cÃ´ng cá»¥ thÃ­ch á»©ng vá»›i con ngÆ°á»i, chá»© khÃ´ng pháº£i báº¯t con ngÆ°á»i cháº¡y theo cÃ´ng cá»¥.**

<div align="center">

![Kiáº¿n trÃºc Ä‘Æ¡n giáº£n nhÆ°ng máº¡nh máº½ cá»§a Metaflow](https://i.ibb.co/PGYb5c0n/metaflow-solution.png)

*HÃ¬nh 13: Kiáº¿n trÃºc Ä‘Æ¡n giáº£n nhÆ°ng máº¡nh máº½ cá»§a Metaflow*

</div>

#### 4.5.2 Uber: Michelangelo & Feature Store - Ná»n Táº£ng Cho Quy MÃ´ Lá»›n

Há» xÃ¢y dá»±ng ná»n táº£ng **Michelangelo**, vá»›i "trÃ¡i tim" lÃ  **Feature Store** - má»™t kho lÆ°u trá»¯ táº­p trung cÃ¡c Ä‘áº·c trÆ°ng cÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng, giÃºp loáº¡i bá» sá»± trÃ¹ng láº·p vÃ  tÄƒng tá»‘c Ä‘á»™ phÃ¡t triá»ƒn.

<div align="center">

![Káº¿t quáº£ áº¥n tÆ°á»£ng cá»§a Uber sau khi Ã¡p dá»¥ng MLOps](https://i.ibb.co/pjHFP83Z/uber-results.png)

*HÃ¬nh 14: Káº¿t quáº£ áº¥n tÆ°á»£ng cá»§a Uber sau khi Ã¡p dá»¥ng MLOps*

</div>

#### 4.5.3 OpenAI: RLHF - Khi Pháº£n Há»“i Cá»§a Con NgÆ°á»i LÃ  Má»™t Pháº§n Cá»§a "Ops"

OpenAI tiÃªn phong trong viá»‡c sá»­ dá»¥ng **Reinforcement Learning from Human Feedback (RLHF)**, tÃ­ch há»£p sá»± Ä‘Ã¡nh giÃ¡ tinh vi cá»§a con ngÆ°á»i vÃ o vÃ²ng láº·p cáº£i tiáº¿n mÃ´ hÃ¬nh.

<div align="center">

![Quy trÃ¬nh RLHF tÃ­ch há»£p pháº£n há»“i con ngÆ°á»i vÃ o vÃ²ng láº·p váº­n hÃ nh](https://i.ibb.co/s9FPDPqT/rlhf-process.png)

*HÃ¬nh 15: Quy trÃ¬nh RLHF tÃ­ch há»£p pháº£n há»“i con ngÆ°á»i vÃ o vÃ²ng láº·p váº­n hÃ nh*

</div>

### 4.6 Con NgÆ°á»i Váº­n HÃ nh Há»‡ Thá»‘ng: Sá»± Trá»—i Dáº­y Cá»§a Ká»¹ SÆ° MLOps

#### 4.6.1 CÃ¡c MÃ´ HÃ¬nh Tá»• Chá»©c

CÃ³ hai mÃ´ hÃ¬nh phá»• biáº¿n:

1. **NhÃ  Khoa Há»c Dá»¯ Liá»‡u ToÃ n NÄƒng (End-to-End Data Scientist):** Má»™t ngÆ°á»i Ä‘áº£m nháº­n toÃ n bá»™ vÃ²ng Ä‘á»i. MÃ´ hÃ¬nh nÃ y linh hoáº¡t, phÃ¹ há»£p vá»›i cÃ¡c startup hoáº·c dá»± Ã¡n nhá»
2. **Äá»™i NgÅ© Äa Chá»©c NÄƒng (Cross-Functional Team):** Má»™t Ä‘á»™i ngÅ© bao gá»“m cÃ¡c chuyÃªn gia vá»›i vai trÃ² rÃµ rÃ ng. ÄÃ¢y lÃ  mÃ´ hÃ¬nh phá»• biáº¿n vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng tá»‘t hÆ¡n

<div align="center">

![Sá»± cá»™ng hÆ°á»Ÿng cá»§a cÃ¡c chuyÃªn gia trong Ä‘á»™i ngÅ© MLOps](https://i.ibb.co/rKyY7Fbx/cross-functional-team.png)

*HÃ¬nh 16: Sá»± cá»™ng hÆ°á»Ÿng cá»§a cÃ¡c chuyÃªn gia trong Ä‘á»™i ngÅ© MLOps*

</div>

#### 4.6.2 CÃ¡c Vai TrÃ² ChÃ­nh Trong Äá»™i NgÅ© MLOps

- **Data Scientist:** Táº­p trung vÃ o viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u, thá»­ nghiá»‡m vÃ  xÃ¢y dá»±ng mÃ´ hÃ¬nh Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n kinh doanh
- **Data Engineer:** XÃ¢y dá»±ng vÃ  duy trÃ¬ cÃ¡c Ä‘Æ°á»ng á»‘ng dá»¯ liá»‡u (data pipelines) vá»¯ng cháº¯c, Ä‘áº£m báº£o dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao luÃ´n sáºµn sÃ ng
- **ML Engineer:** LÃ  cáº§u ná»‘i giá»¯a Data Scientist vÃ  MLOps Engineer. Há» tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh, xÃ¢y dá»±ng cÃ¡c pipeline huáº¥n luyá»‡n vÃ  tÃ­ch há»£p mÃ´ hÃ¬nh vÃ o cÃ¡c á»©ng dá»¥ng
- **MLOps Engineer / AI Platform Engineer:** ChuyÃªn gia vá» háº¡ táº§ng vÃ  tá»± Ä‘á»™ng hÃ³a. Há» lÃ  nhá»¯ng ngÆ°á»i Ä‘áº£m báº£o toÃ n bá»™ cá»— mÃ¡y AI váº­n hÃ nh trÆ¡n tru, Ä‘Ã¡ng tin cáº­y vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng

#### 4.6.3 Lá»™ TrÃ¬nh Sá»± Nghiá»‡p

MLOps lÃ  má»™t miá»n Ä‘áº¥t há»©a cho cÃ¡c ká»¹ sÆ°. Lá»™ trÃ¬nh phÃ¡t triá»ƒn thÆ°á»ng Ä‘i theo hai hÆ°á»›ng chÃ­nh:

1. **Tá»« DevOps -> MLOps Engineer:** Náº¿u báº¡n Ä‘Ã£ cÃ³ ná»n táº£ng vá»¯ng cháº¯c vá» DevOps, Kubernetes, CI/CD, báº¡n cÃ³ thá»ƒ há»c thÃªm kiáº¿n thá»©c vá» ML
2. **Tá»« Data Scientist/Software Engineer -> ML Engineer:** Náº¿u báº¡n máº¡nh vá» xÃ¢y dá»±ng mÃ´ hÃ¬nh hoáº·c phÃ¡t triá»ƒn pháº§n má»m, báº¡n cÃ³ thá»ƒ trau dá»“i thÃªm ká»¹ nÄƒng vá» váº­n hÃ nh

<div align="center">

![Sá»± tiáº¿n hÃ³a cá»§a cáº¥u trÃºc Ä‘á»™i ngÅ© MLOps theo quy mÃ´ tá»• chá»©c](https://i.ibb.co/kVyWDJQ1/team-evolution.png)

*HÃ¬nh 17: Sá»± tiáº¿n hÃ³a cá»§a cáº¥u trÃºc Ä‘á»™i ngÅ© MLOps theo quy mÃ´ tá»• chá»©c*

</div>

### 4.7 Con ÄÆ°á»ng PhÃ­a TrÆ°á»›c: Tá»« MLOps Äáº¿n LLMOps VÃ  AgenticAI Ops

Tháº¿ giá»›i AI khÃ´ng ngá»«ng váº­n Ä‘á»™ng. MLOps lÃ  ná»n táº£ng, nhÆ°ng trÃªn ná»n táº£ng Ä‘Ã³, nhá»¯ng phÆ°Æ¡ng phÃ¡p váº­n hÃ nh má»›i Ä‘ang hÃ¬nh thÃ nh Ä‘á»ƒ Ä‘Ã¡p á»©ng sá»± phá»©c táº¡p ngÃ y cÃ ng tÄƒng cá»§a cÃ¡c há»‡ thá»‘ng AI.

<div align="center">

![Sá»± tiáº¿n hÃ³a cá»§a cÃ¡c framework váº­n hÃ nh AI](https://i.ibb.co/60g5CB4B/ops-spectrum.png)

*HÃ¬nh 18: Sá»± tiáº¿n hÃ³a cá»§a cÃ¡c framework váº­n hÃ nh AI*

</div>

- **LLMOps:** Khi cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) trá»Ÿ nÃªn phá»• biáº¿n, cÃ¡c thÃ¡ch thá»©c váº­n hÃ nh má»›i cÅ©ng xuáº¥t hiá»‡n, táº­p trung vÃ o Quáº£n lÃ½ Prompt, CÆ¡ sá»Ÿ dá»¯ liá»‡u Vector vÃ  giÃ¡m sÃ¡t cÃ¡c váº¥n Ä‘á» Ä‘áº·c thÃ¹ cá»§a LLM
- **AgenticAI Ops:** ÄÃ¢y lÃ  tÆ°Æ¡ng lai xa hÆ¡n, khi cÃ¡c há»‡ thá»‘ng AI (agents) cÃ³ kháº£ nÄƒng tá»± chá»§ láº­p káº¿ hoáº¡ch, sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ vÃ  thá»±c thi cÃ¡c tÃ¡c vá»¥ phá»©c táº¡p, Ä‘Ã²i há»i viá»‡c váº­n hÃ nh táº­p trung vÃ o Äiá»u phá»‘i cÃ´ng cá»¥, Quáº£n lÃ½ bá»™ nhá»› vÃ  cÃ¡c lan can an toÃ n

<div align="center">

![CÃ¡c táº§ng váº­n hÃ nh AI, má»—i táº§ng má»›i xÃ¢y dá»±ng dá»±a trÃªn ná»n táº£ng cá»§a táº§ng trÆ°á»›c Ä‘Ã³](https://i.ibb.co/Qvyy6bHN/ops-layers.png)

*HÃ¬nh 19: CÃ¡c táº§ng váº­n hÃ nh AI, má»—i táº§ng má»›i xÃ¢y dá»±ng dá»±a trÃªn ná»n táº£ng cá»§a táº§ng trÆ°á»›c Ä‘Ã³*

</div>

Viá»‡c náº¯m vá»¯ng MLOps hÃ´m nay chÃ­nh lÃ  báº¡n Ä‘ang xÃ¢y dá»±ng ná»n mÃ³ng vá»¯ng cháº¯c Ä‘á»ƒ sáºµn sÃ ng chinh phá»¥c nhá»¯ng Ä‘á»‰nh cao má»›i cá»§a LLMOps vÃ  AgenticAI Ops trong tÆ°Æ¡ng lai.

## 5. Káº¿t Luáº­n: HÃ nh TrÃ¬nh ToÃ n Diá»‡n Trong Tháº¿ Giá»›i AI

ChÃºng ta Ä‘Ã£ Ä‘i qua má»™t cháº·ng Ä‘Æ°á»ng dÃ i, tá»« nhá»¯ng nguyÃªn lÃ½ toÃ¡n há»c cÆ¡ báº£n cá»§a Gradient Descent, Ä‘áº¿n viá»‡c lÃ m cho AI cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c thÃ´ng qua Explainable AI, vÃ  cuá»‘i cÃ¹ng lÃ  váº­n hÃ nh cÃ¡c há»‡ thá»‘ng AI phá»©c táº¡p trong mÃ´i trÆ°á»ng production thÃ´ng qua MLOps.

### 5.1 Ba Trá»¥ Cá»™t Cá»‘t LÃµi

**1. Gradient Descent - Ná»n Táº£ng ToÃ¡n Há»c:**
- Hiá»ƒu sÃ¢u vá» Ä‘áº¡o hÃ m, gradient vÃ  cÃ¡c hÃ m máº¥t mÃ¡t
- Náº¯m vá»¯ng vai trÃ² cá»§a chuáº©n hÃ³a dá»¯ liá»‡u trong tá»‘i Æ°u hÃ³a
- Lá»±a chá»n hÃ m máº¥t mÃ¡t phÃ¹ há»£p vá»›i Ä‘áº·c tÃ­nh dá»¯ liá»‡u vÃ  má»¥c tiÃªu bÃ i toÃ¡n

**2. Explainable AI - Minh Báº¡ch VÃ  Tin Cáº­y:**
- PhÃ¢n biá»‡t giá»¯a interpretability vÃ  explainability
- Ãp dá»¥ng LIME vÃ  Anchor Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh há»™p Ä‘en
- ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng lá»i giáº£i thÃ­ch thÃ´ng qua fidelity, stability, sparsity vÃ  coverage

**3. MLOps - Váº­n HÃ nh Xuáº¥t Sáº¯c:**
- Thu háº¹p khoáº£ng cÃ¡ch giá»¯a nghiÃªn cá»©u vÃ  production
- XÃ¢y dá»±ng há»‡ thá»‘ng quáº£n lÃ½ phiÃªn báº£n toÃ n diá»‡n
- Tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh vÃ  giÃ¡m sÃ¡t liÃªn tá»¥c

### 5.2 Nhá»¯ng GÃ¬ Cáº§n Ghi Nhá»›

1. **AI thÃ nh cÃ´ng lÃ  sá»± káº¿t há»£p cá»§a ba yáº¿u tá»‘:** LÃ½ thuyáº¿t vá»¯ng cháº¯c, kháº£ nÄƒng giáº£i thÃ­ch vÃ  váº­n hÃ nh hiá»‡u quáº£
2. **Má»—i trá»¥ cá»™t Ä‘á»u quan trá»ng:** KhÃ´ng thá»ƒ bá» qua báº¥t ká»³ yáº¿u tá»‘ nÃ o trong hÃ nh trÃ¬nh xÃ¢y dá»±ng há»‡ thá»‘ng AI
3. **HÃ nh trÃ¬nh lÃ  má»™t cuá»™c marathon:** HÃ£y báº¯t Ä‘áº§u tá»« nhá»¯ng bÆ°á»›c nhá» nháº¥t vÃ  kiÃªn trÃ¬ há»c há»i
4. **TÆ°Æ¡ng lai thuá»™c vá» nhá»¯ng ngÆ°á»i hiá»ƒu cáº£ ba:** Ká»¹ sÆ° AI trong tÆ°Æ¡ng lai cáº§n náº¯m vá»¯ng cáº£ lÃ½ thuyáº¿t, kháº£ nÄƒng giáº£i thÃ­ch vÃ  váº­n hÃ nh

### 5.3 Nhá»¯ng BÆ°á»›c Äáº§u TiÃªn Báº¡n CÃ³ Thá»ƒ LÃ m Ngay HÃ´m Nay

**Vá» Gradient Descent:**
- Thá»±c hÃ nh tÃ­nh gradient báº±ng tay cho cÃ¡c hÃ m Ä‘Æ¡n giáº£n
- So sÃ¡nh hiá»‡u quáº£ cá»§a MSE vÃ  MAE trÃªn cÃ¡c bá»™ dá»¯ liá»‡u khÃ¡c nhau
- Thá»±c hiá»‡n chuáº©n hÃ³a dá»¯ liá»‡u Ä‘Ãºng cÃ¡ch Ä‘á»ƒ trÃ¡nh data leakage

**Vá» Explainable AI:**
- Sá»­ dá»¥ng LIME Ä‘á»ƒ giáº£i thÃ­ch má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘Æ¡n giáº£n
- Thá»­ nghiá»‡m vá»›i Anchor Ä‘á»ƒ tÃ¬m quy táº¯c cÃ³ Ä‘á»™ tin cáº­y cao
- Äo lÆ°á»ng fidelity vÃ  stability cá»§a cÃ¡c lá»i giáº£i thÃ­ch

**Vá» MLOps:**
- Báº¯t Ä‘áº§u Ä‘Æ°a code ML vÃ o Git vÃ  sá»­ dá»¥ng DVC Ä‘á»ƒ quáº£n lÃ½ dá»¯ liá»‡u
- Sá»­ dá»¥ng MLflow Ä‘á»ƒ theo dÃµi cÃ¡c thá»­ nghiá»‡m
- XÃ¢y dá»±ng má»™t pipeline Ä‘Æ¡n giáº£n Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh huáº¥n luyá»‡n

Tháº¿ giá»›i AI Ä‘ang phÃ¡t triá»ƒn vá»›i tá»‘c Ä‘á»™ vÅ© bÃ£o, vÃ  vai trÃ² cá»§a nhá»¯ng ká»¹ sÆ° cÃ³ kháº£ nÄƒng "thuáº§n hÃ³a" sá»± phá»©c táº¡p cá»§a nÃ³ sáº½ ngÃ y cÃ ng trá»Ÿ nÃªn quan trá»ng. NhÆ° Peter Drucker Ä‘Ã£ nÃ³i: **"CÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai lÃ  táº¡o ra nÃ³."** ChÃºc báº¡n thÃ nh cÃ´ng trÃªn hÃ nh trÃ¬nh táº¡o ra tÆ°Æ¡ng lai cá»§a AI.

## 6. TÃ i Liá»‡u Tham Kháº£o

> **ChÃº thÃ­ch:** Má»™t sá»‘ hÃ¬nh minh há»a trong bÃ i viáº¿t Ä‘Æ°á»£c láº¥y tá»« cÃ¡c nguá»“n: AIO, vÃ  Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng báº±ng AI.

### Gradient Descent & Optimization
- Ruder, S. (2016). An overview of gradient descent optimization algorithms. *arXiv preprint arXiv:1609.04747*
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press
- Bottou, L., Curtis, F. R., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. *SIAM Review*, 60(2), 223-311

### Explainable AI
- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why should I trust you?" Explaining the predictions of any classifier. *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining*
- Ribeiro, M. T., Singh, S., & Guestrin, C. (2018). Anchors: High-precision model-agnostic explanations. *Proceedings of the AAAI conference on artificial intelligence*
- Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. *arXiv preprint arXiv:1702.08608*

### MLOps & Production ML
- Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems. *NIPS 2015*
- Netflix Metaflow Documentation (2020). *Netflix Technology Blog*
- Uber Michelangelo Platform (2017). *Uber Engineering Blog*
- Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. *arXiv preprint arXiv:2203.02155*

### Tools & Frameworks
- MLflow Documentation (2023). *Apache Software Foundation*
- DVC Documentation (2023). *Iterative.ai*
- LIME Documentation (2023). *GitHub*