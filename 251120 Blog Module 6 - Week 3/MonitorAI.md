# üëÅÔ∏è MonitorAI ‚Äî üî• Thi√™n La ƒê·ªãa V√µng: Prometheus, Loki v√† Grafana Gi√°m S√°t LLM

## üìù T√≥m t·∫Øt

Khi ch·∫°y nhi·ªÅu LLM models c√πng l√∫c, ta th∆∞·ªùng g·∫∑p kh√≥ khƒÉn: kh√¥ng bi·∫øt process n√†o ƒëang ch·∫°y, CPU/GPU ƒëang d√πng bao nhi√™u, hay logs n·∫±m ·ªü ƒë√¢u. **MonitorAI** gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y b·∫±ng c√°ch t·ª± ƒë·ªông ph√°t hi·ªán c√°c LLM processes, thu th·∫≠p metrics CPU/Memory/GPU, v√† t·∫≠p trung logs v√†o m·ªôt dashboard ƒë·∫πp m·∫Øt. B√†i vi·∫øt n√†y s·∫Ω d·∫´n d·∫Øt ta t·ª´ng b∆∞·ªõc ƒë·ªÉ hi·ªÉu c√°ch h·ªá th·ªëng ho·∫°t ƒë·ªông v√† c√°ch s·ª≠ d·ª•ng n√≥ trong th·ª±c t·∫ø.

---

> *"T·ª´ b√†i to√°n gi√°m s√°t LLM processes v·ªõi h√†ng trƒÉm metrics v√† logs ‚Äî c√πng t√¨m hi·ªÉu c√°ch MonitorAI x√¢y d·ª±ng h·ªá th·ªëng monitoring to√†n di·ªán v·ªõi Grafana, Prometheus, Loki v√† Tempo."*

<div align="center">
![Logo](https://i.ibb.co/d41fHgXL/Logo-999.png)
</div>

## 1. Gi·ªõi thi·ªáu: B√†i to√°n th·ª±c t·∫ø

### 1.1 Th√°ch th·ª©c khi gi√°m s√°t LLM Processes

H√£y t∆∞·ªüng t∆∞·ª£ng ta ƒëang ch·∫°y 3-4 LLM models c√πng l√∫c tr√™n m√°y t√≠nh. M·ªói model c√≥ th·ªÉ l√† GPT-2, LLaMA, hay Mistral. L√†m sao ta bi·∫øt ƒë∆∞·ª£c:

- Process n√†o ƒëang ch·∫°y model n√†o?
- CPU v√† Memory ƒëang d√πng bao nhi√™u?
- GPU c√≥ ƒëang b·ªã qu√° t·∫£i kh√¥ng?
- Logs n·∫±m ·ªü ƒë√¢u khi c√≥ l·ªói?

ƒê√¢y ch√≠nh l√† nh·ªØng c√¢u h·ªèi m√† **MonitorAI** gi√∫p ta tr·∫£ l·ªùi. H·ªá th·ªëng t·ª± ƒë·ªông ph√°t hi·ªán c√°c LLM processes, thu th·∫≠p metrics, v√† hi·ªÉn th·ªã m·ªçi th·ª© tr√™n m·ªôt dashboard d·ªÖ ƒë·ªçc.

> **üí° D·ª± √°n m√£ ngu·ªìn m·ªü:** To√†n b·ªô code v√† c·∫•u h√¨nh c·ªßa h·ªá th·ªëng n√†y ƒë√£ ƒë∆∞·ª£c c√¥ng khai tr√™n GitHub t·∫°i [https://github.com/D9Dre4mer/MonitorAI](https://github.com/D9Dre4mer/MonitorAI).

### 1.2 C√°c LLM Frameworks ƒë∆∞·ª£c h·ªó tr·ª£

MonitorAI c√≥ th·ªÉ t·ª± ƒë·ªông ph√°t hi·ªán nhi·ªÅu frameworks ph·ªï bi·∫øn:

- **Hugging Face Transformers** - Framework ph·ªï bi·∫øn nh·∫•t cho LLM
- **llama.cpp** - Inference engine t·ªëi ∆∞u cho llama models
- **vLLM** - High-throughput LLM serving engine
- **TensorRT** - NVIDIA's inference optimization framework
- **ONNX Runtime** - Cross-platform inference engine
- **PyTorch** - Deep learning framework
- **TensorFlow** - Google's machine learning platform

H·ªá th·ªëng c≈©ng t·ª± ƒë·ªông tr√≠ch xu·∫•t t√™n model t·ª´ command line. V√≠ d·ª•, n·∫øu ta ch·∫°y `python run-model.py --model-name gpt2`, h·ªá th·ªëng s·∫Ω bi·∫øt model ƒëang ch·∫°y l√† `gpt2`.

### 1.3 Th√°ch th·ª©c k·ªπ thu·∫≠t

Tr√™n th·ª±c t·∫ø, vi·ªác gi√°m s√°t LLM processes kh√¥ng ƒë∆°n gi·∫£n. Ta g·∫∑p nhi·ªÅu th√°ch th·ª©c:

**Th·ª© nh·∫•t**, m·ªói framework c√≥ c√°ch detect kh√°c nhau. Transformers d√πng `from_pretrained()`, c√≤n llama.cpp l·∫°i c√≥ pattern ri√™ng. Ta c·∫ßn pattern matching th√¥ng minh ƒë·ªÉ ph√°t hi·ªán t·∫•t c·∫£.

**Th·ª© hai**, tr√™n Windows, `nvidia-smi` kh√¥ng th·ªÉ query GPU memory per process m·ªôt c√°ch ch√≠nh x√°c. Ta ph·∫£i d√πng c√°ch kh√°c: ƒë·ªÉ process t·ª± ghi GPU memory v√†o file JSON, r·ªìi ƒë·ªçc file ƒë√≥. (Tr√™n Linux, `nvidia-smi` c√≥ th·ªÉ query ch√≠nh x√°c h∆°n, nh∆∞ng file-based approach v·∫´n l√† l·ª±a ch·ªçn t·ªët ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n.)

**Th·ª© ba**, logs t·ª´ nhi·ªÅu processes n·∫±m r·∫£i r√°c. Ta c·∫ßn t·∫≠p trung ch√∫ng l·∫°i ƒë·ªÉ d·ªÖ t√¨m ki·∫øm.

**Cu·ªëi c√πng**, metrics c·∫ßn c·∫≠p nh·∫≠t nhanh nh∆∞ng kh√¥ng ƒë∆∞·ª£c qu√° t·∫£i h·ªá th·ªëng. Ta ch·ªçn 10-15 gi√¢y l√† kho·∫£ng th·ªùi gian h·ª£p l√Ω.

### 1.4 Gi·∫£i ph√°p MonitorAI

MonitorAI gi·∫£i quy·∫øt c√°c th√°ch th·ª©c tr√™n b·∫±ng c√°ch:

- **T·ª± ƒë·ªông ph√°t hi·ªán**: D√πng regex patterns ƒë·ªÉ t√¨m LLM processes trong command line
- **File-based GPU exposure**: Processes ghi GPU memory v√†o JSON files, LLM Monitor ƒë·ªçc v√† expose metrics
- **Prometheus metrics**: D√πng format chu·∫©n, d·ªÖ t√≠ch h·ª£p v·ªõi nhi·ªÅu tools
- **Loki logs**: T·∫≠p trung logs v·ªõi LogQL ƒë·ªÉ query d·ªÖ d√†ng
- **Forest Green theme**: Dashboard ƒë·∫πp, d·ªÖ ƒë·ªçc, th·ªëng nh·∫•t m√†u s·∫Øc
- **Auto cleanup**: T·ª± ƒë·ªông x√≥a metrics sau 10 gi√¢y n·∫øu process ƒë√£ d·ª´ng

> üí° **Fun fact v·ªÅ Prometheus**: Trong th·∫ßn tho·∫°i Hy L·∫°p, Prometheus l√† v·ªã th·∫ßn Titan ƒë√£ ƒë√°nh c·∫Øp l·ª≠a t·ª´ Olympus v√† mang ƒë·∫øn cho lo√†i ng∆∞·ªùi, t∆∞·ª£ng tr∆∞ng cho s·ª± khai s√°ng v√† tri th·ª©c. C√¥ng c·ª• Prometheus c≈©ng v·∫≠y - n√≥ "mang l·ª≠a" gi√°m s√°t h·ªá th·ªëng ƒë·∫øn cho ta, gi√∫p ta "th·∫•y √°nh s√°ng" trong vi·ªác theo d√µi metrics!

> üí° **Fun fact v·ªÅ Loki**: Loki l√† v·ªã th·∫ßn l·ª´a ƒë·∫£o trong th·∫ßn tho·∫°i B·∫Øc √Çu, n·ªïi ti·∫øng v·ªõi kh·∫£ nƒÉng bi·∫øn h√¨nh v√† tinh qu√°i. C√¥ng c·ª• Loki c≈©ng "bi·∫øn h√≥a" logs c·ªßa ta m·ªôt c√°ch linh ho·∫°t, gi√∫p ta "b·∫Øt qu·∫£ tang" nh·ªØng v·∫•n ƒë·ªÅ ·∫©n gi·∫•u trong h·ªá th·ªëng!

### 1.5 T·∫°i sao ch·ªçn Observability Stack?

Ta ch·ªçn stack Grafana + Prometheus + Loki + Tempo v√¨ ƒë√¢y l√† nh·ªØng c√¥ng c·ª• m·∫°nh m·∫Ω v√† ph·ªï bi·∫øn:

> üí° **Fun fact v·ªÅ Grafana**: T√™n "Grafana" xu·∫•t ph√°t t·ª´ ch·ªØ "graph" (ƒë·ªì th·ªã) v√† h·∫≠u t·ªë "-ana", g·ª£i nh·ªõ ƒë·∫øn "Nirvana" (ni·∫øt b√†n). C√≥ l·∫Ω nh√≥m ph√°t tri·ªÉn mu·ªën ta ƒë·∫°t ƒë·∫øn "c√µi ni·∫øt b√†n" c·ªßa d·ªØ li·ªáu khi xem dashboard ƒë·∫πp m·∫Øt n√†y! üòÑ

- **Standard protocols**: Prometheus metrics, LogQL, OTLP traces - t·∫•t c·∫£ ƒë·ªÅu l√† chu·∫©n c√¥ng nghi·ªáp
- **Scalable**: C√≥ th·ªÉ m·ªü r·ªông cho nhi·ªÅu hosts v√† services
- **Rich ecosystem**: Nhi·ªÅu exporters v√† integrations s·∫µn c√≥
- **Beautiful dashboards**: Grafana c√≥ nhi·ªÅu visualization options
- **Open source**: Mi·ªÖn ph√≠, c·ªông ƒë·ªìng l·ªõn
- **Production-ready**: ƒê∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong production

---

## 2. Pipeline H·ªá Th·ªëng MonitorAI

### 2.1 S∆° ƒë·ªì t·ªïng quan Pipeline

H·ªá th·ªëng MonitorAI ho·∫°t ƒë·ªông theo **3 pipeline ch√≠nh**:

1. **Metrics Pipeline** - Thu th·∫≠p CPU, Memory, GPU metrics
2. **Logs Pipeline** - T·∫≠p trung logs t·ª´ nhi·ªÅu processes
3. **Tracing Pipeline** - Distributed tracing (t√πy ch·ªçn, t∆∞∆°ng lai)

H√£y xem t·ª´ng pipeline ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o.

**Pipeline Metrics (CPU, Memory, GPU):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Processes ‚îÇ
‚îÇ  (Python apps) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚ñ∫ CPU/Memory metrics ‚îÄ‚îÄ‚îê
         ‚îÇ                        ‚îÇ
         ‚îî‚îÄ‚ñ∫ GPU info (JSON) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
            logs/gpu-info-*.json  ‚îÇ
                                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ      LLM Monitor (9101)         ‚îÇ
         ‚îÇ  - Detect LLM processes         ‚îÇ
         ‚îÇ  - Read GPU info files          ‚îÇ
         ‚îÇ  - Collect CPU/Memory/GPU       ‚îÇ
         ‚îÇ  - Expose Prometheus metrics    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   GPU Exporter (9100)          ‚îÇ
         ‚îÇ  - Query nvidia-smi             ‚îÇ
         ‚îÇ  - Collect overall GPU metrics  ‚îÇ
         ‚îÇ  - Expose Prometheus metrics    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ      Prometheus (9090)          ‚îÇ
         ‚îÇ  - Scrape metrics every 15s     ‚îÇ
         ‚îÇ  - Store time-series data       ‚îÇ
         ‚îÇ  - Retention: 200 hours         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ      Grafana (3000)             ‚îÇ
         ‚îÇ  - Query Prometheus via PromQL  ‚îÇ
         ‚îÇ  - Visualize in dashboards      ‚îÇ
         ‚îÇ  - Forest Green theme           ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pipeline Logs:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Processes ‚îÇ
‚îÇ  (Python apps)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Write logs
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ logs/llm-model. ‚îÇ
‚îÇ      log        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Read logs
         ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Promtail      ‚îÇ
         ‚îÇ  (Log Shipper)  ‚îÇ
         ‚îÇ  - Tail log file‚îÇ
         ‚îÇ  - Parse & label‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Push logs
         ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Loki (3100)   ‚îÇ
         ‚îÇ  - Store logs   ‚îÇ
         ‚îÇ  - Index by     ‚îÇ
         ‚îÇ    labels       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Query logs
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Grafana       ‚îÇ
‚îÇ  - Logs panel   ‚îÇ
‚îÇ  - LogQL queries‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pipeline Tracing (Optional - Future):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Applications   ‚îÇ
‚îÇ  (OpenTelemetry)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Send traces
         ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Tempo (3200)    ‚îÇ
         ‚îÇ  - Store traces  ‚îÇ
         ‚îÇ  - OTLP protocol ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Query traces
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Grafana       ‚îÇ
‚îÇ  - Trace view   ‚îÇ
‚îÇ  - Flame graphs ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> üí° **Fun fact v·ªÅ Tempo**: "Tempo" trong ti·∫øng √ù c√≥ nghƒ©a l√† "nh·ªãp ƒë·ªô" trong √¢m nh·∫°c. C√¥ng c·ª• Tempo c≈©ng v·∫≠y - n√≥ theo d√µi "nh·ªãp ƒë·ªô" c·ªßa traces trong h·ªá th·ªëng, gi√∫p ta hi·ªÉu ƒë∆∞·ª£c timing v√† flow c·ªßa requests qua c√°c services!

### 2.2 6 B∆∞·ªõc chi ti·∫øt theo Pipeline

#### 2.2.1 B∆∞·ªõc 1: Ph√°t hi·ªán LLM Processes

H·ªá th·ªëng b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác qu√©t t·∫•t c·∫£ Python processes ƒëang ch·∫°y. Gi·ªëng nh∆∞ m·ªôt "radar" qu√©t to√†n b·ªô h·ªá th·ªëng ƒë·ªÉ t√¨m c√°c LLM processes.

Ta s·ª≠ d·ª•ng th∆∞ vi·ªán `psutil` ƒë·ªÉ l·∫•y danh s√°ch t·∫•t c·∫£ processes. Sau ƒë√≥, ta ki·ªÉm tra command line c·ªßa m·ªói process xem c√≥ ch·ª©a t·ª´ kh√≥a c·ªßa LLM frameworks kh√¥ng.

**C√°ch ho·∫°t ƒë·ªông:**

Ta ƒë·ªãnh nghƒ©a c√°c patterns cho t·ª´ng framework. V√≠ d·ª•, v·ªõi Transformers, ta t√¨m c√°c t·ª´ kh√≥a nh∆∞ `transformers`, `huggingface`, hay `from_pretrained`. Khi t√¨m th·∫•y, ta bi·∫øt process ƒë√≥ ƒëang ch·∫°y LLM.

```python
LLM_PATTERNS = {
    'transformers': [
        r'transformers',
        r'huggingface',
        r'\.from_pretrained',
        r'pipeline\(.*model',
    ],
    'llama.cpp': [
        r'llama',
        r'llama\.cpp',
        r'gguf',
    ],
    'vllm': [
        r'vllm',
        r'vllm\.engine',
    ],
    # ... c√°c patterns kh√°c
}

def detect_llm_processes():
    processes = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        cmdline = ' '.join(proc.info['cmdline'] or [])
        for framework, patterns in LLM_PATTERNS.items():
            if any(re.search(pattern, cmdline, re.IGNORECASE) 
                   for pattern in patterns):
                processes.append({
                    'pid': proc.info['pid'],
                    'framework': framework,
                    'model_name': extract_model_name(cmdline)
                })
    return processes
```

**K·∫øt qu·∫£ th·ª±c t·∫ø:**

Sau khi qu√©t, ta c√≥ th·ªÉ ph√°t hi·ªán ƒë∆∞·ª£c:
- **Process 15176**: `transformers` framework, model `gpt2`
- **Process 22716**: `transformers` framework, model `DialoGPT-small`

#### 2.2.2 B∆∞·ªõc 2: Thu th·∫≠p Metrics

Sau khi ph√°t hi·ªán processes, ta c·∫ßn thu th·∫≠p metrics. Ta quan t√¢m ƒë·∫øn 4 lo·∫°i metrics ch√≠nh: CPU usage, Memory usage, GPU memory, v√† GPU utilization.

**CPU v√† Memory:**

Ta d√πng `psutil` ƒë·ªÉ l·∫•y CPU percentage v√† memory usage (RSS) cho t·ª´ng process. ƒê√¢y l√† c√°ch ƒë∆°n gi·∫£n v√† ch√≠nh x√°c.

**GPU metrics:**

ƒê√¢y l√† ph·∫ßn ph·ª©c t·∫°p h∆°n. Tr√™n Windows, `nvidia-smi` kh√¥ng th·ªÉ query GPU memory per process ch√≠nh x√°c. V√¨ v·∫≠y, ta d√πng c√°ch kh√°c: process t·ª± ghi GPU memory v√†o file JSON, r·ªìi ta ƒë·ªçc file ƒë√≥.

```python
def collect_process_metrics(pid):
    proc = psutil.Process(pid)
    
    # CPU v√† Memory
    cpu_percent = proc.cpu_percent(interval=1.0)
    memory_bytes = proc.memory_info().rss
    
    # GPU metrics t·ª´ file JSON
    gpu_info_file = Path(f'logs/gpu-info-{pid}.json')
    if gpu_info_file.exists():
        with open(gpu_info_file) as f:
            gpu_info = json.load(f)
            gpu_memory = gpu_info.get('gpu_memory_allocated_bytes', 0)
            gpu_util = gpu_info.get('gpu_utilization', 0)
    
    return {
        'cpu_percent': cpu_percent,
        'memory_bytes': memory_bytes,
        'gpu_memory_bytes': gpu_memory,
        'gpu_utilization': gpu_util
    }
```

**K·∫øt qu·∫£ th·ª±c t·∫ø:**

Sau khi thu th·∫≠p, ta c√≥ th·ªÉ th·∫•y:
- **Process 15176** (gpt2): Memory ~254 MB, GPU Memory ~21-26 GiB
- **Process 22716** (DialoGPT-small): Memory ~307 MB, GPU Memory 0 GiB (kh√¥ng s·ª≠ d·ª•ng GPU)

#### 2.2.3 B∆∞·ªõc 3: Expose Prometheus Metrics

Sau khi thu th·∫≠p metrics, ta c·∫ßn expose ch√∫ng theo format Prometheus. Prometheus s·∫Ω scrape metrics t·ª´ endpoint `/metrics` c·ªßa ta.

**C√°ch ho·∫°t ƒë·ªông:**

Ta d√πng th∆∞ vi·ªán `prometheus_client` ƒë·ªÉ t·∫°o c√°c Gauge metrics. M·ªói metric c√≥ labels ƒë·ªÉ ta c√≥ th·ªÉ filter v√† group d·ªÖ d√†ng. V√≠ d·ª•, `llm_process_cpu_percent` c√≥ labels `pid`, `name`, `model_name`, `framework`.

```python
from prometheus_client import Gauge, start_http_server

# Define metrics
llm_process_cpu_percent = Gauge(
    'llm_process_cpu_percent',
    'CPU usage percentage per LLM process',
    ['pid', 'name', 'llm_type', 'model_name', 'framework']
)

llm_process_memory_bytes = Gauge(
    'llm_process_memory_bytes',
    'Memory usage in bytes per LLM process',
    ['pid', 'name', 'llm_type', 'model_name', 'framework']
)

# Start HTTP server
start_http_server(9101)

# Update metrics
for process in detected_processes:
    metrics = collect_process_metrics(process['pid'])
    llm_process_cpu_percent.labels(
        pid=process['pid'],
        name=process['name'],
        llm_type='llm',
        model_name=process['model_name'],
        framework=process['framework']
    ).set(metrics['cpu_percent'])
```

**K·∫øt qu·∫£:**

Metrics ƒë∆∞·ª£c expose t·∫°i `http://localhost:9101/metrics` v·ªõi format Prometheus standard. Prometheus c√≥ th·ªÉ scrape v√† l∆∞u tr·ªØ ch√∫ng.

#### 2.2.4 B∆∞·ªõc 4: Scrape v√† L∆∞u tr·ªØ

Prometheus t·ª± ƒë·ªông scrape metrics t·ª´ LLM Monitor v√† GPU Exporter m·ªói 15 gi√¢y. Ta ch·ªçn 15 gi√¢y v√¨ ƒë·ªß nhanh ƒë·ªÉ capture changes nh∆∞ng kh√¥ng qu√° t·∫£i h·ªá th·ªëng.

> üí° **Fun fact**: Nh∆∞ v·ªã th·∫ßn Prometheus trong th·∫ßn tho·∫°i ƒë√£ "ƒë√°nh c·∫Øp l·ª≠a" t·ª´ Olympus, c√¥ng c·ª• Prometheus c·ªßa ta c≈©ng "ƒë√°nh c·∫Øp" metrics t·ª´ c√°c services m·ªói 15 gi√¢y, mang l·∫°i "√°nh s√°ng" cho vi·ªác gi√°m s√°t h·ªá th·ªëng!

**C·∫•u h√¨nh:**

Ta c·∫•u h√¨nh Prometheus ƒë·ªÉ scrape t·ª´ 2 targets: LLM Monitor (port 9101) v√† GPU Exporter (port 9100). Prometheus s·∫Ω l∆∞u tr·ªØ metrics v·ªõi retention 200 gi·ªù - ƒë·ªß ƒë·ªÉ ph√¢n t√≠ch trends.

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'llm-monitor'
    scrape_interval: 15s
    static_configs:
      - targets: ['host.docker.internal:9101']
  
  - job_name: 'gpu-exporter'
    scrape_interval: 15s
    static_configs:
      - targets: ['host.docker.internal:9100']
```

**K·∫øt qu·∫£:**

Prometheus l∆∞u tr·ªØ metrics v·ªõi labels ƒë·∫ßy ƒë·ªß. Ta c√≥ th·ªÉ query b·∫±ng PromQL:
- `llm_process_cpu_percent{framework="transformers"}` - CPU usage c·ªßa t·∫•t c·∫£ Transformers processes
- `llm_process_gpu_memory_bytes{model_name="gpt2"}` - GPU memory c·ªßa model gpt2

#### 2.2.5 B∆∞·ªõc 5: T·∫≠p trung Logs

Logs t·ª´ LLM processes ƒë∆∞·ª£c ghi v√†o file `logs/llm-model.log`. Promtail ƒë·ªçc file n√†y v√† ship ƒë·∫øn Loki ƒë·ªÉ t·∫≠p trung.

> üí° **Fun fact v·ªÅ Promtail**: T√™n n√†y l√† s·ª± k·∫øt h·ª£p c·ªßa "Prometheus" + "tail" (l·ªánh Unix ƒë·ªÉ theo d√µi log files). N√≥ ho·∫°t ƒë·ªông nh∆∞ m·ªôt "ƒëu√¥i" theo d√µi log files v√† ship ƒë·∫øn Loki - gi·ªëng nh∆∞ m·ªôt con m√®o ƒëu√¥i d√†i lu√¥n theo d√µi m·ªçi th·ª©! üê±

**C√°ch ho·∫°t ƒë·ªông:**

Promtail ho·∫°t ƒë·ªông nh∆∞ m·ªôt "log shipper". N√≥ ƒë·ªçc log file, parse v√† th√™m labels, r·ªìi push ƒë·∫øn Loki. Loki l∆∞u tr·ªØ logs v·ªõi indexing theo labels ƒë·ªÉ query nhanh.

```yaml
# promtail-config.yml
scrape_configs:
  - job_name: llm-model
    static_configs:
      - targets:
          - localhost
        labels:
          job: llm-model
          __path__: /logs/llm-model.log
```

**K·∫øt qu·∫£:**

Logs ƒë∆∞·ª£c t·∫≠p trung trong Loki. Ta c√≥ th·ªÉ query b·∫±ng LogQL:
- `{job="llm-model"} |= "error"` - T√¨m t·∫•t c·∫£ error logs
- `{job="llm-model"} | json | model_name="gpt2"` - Filter logs theo model name

#### 2.2.6 B∆∞·ªõc 6: Visualization

Cu·ªëi c√πng, Grafana query metrics t·ª´ Prometheus v√† logs t·ª´ Loki, r·ªìi hi·ªÉn th·ªã tr√™n dashboard v·ªõi Forest Green theme.

> üí° **Fun fact**: Nh∆∞ t√™n g·ªçi g·ª£i nh·ªõ ƒë·∫øn "Nirvana" (ni·∫øt b√†n), Grafana ƒë∆∞a ta ƒë·∫øn "c√µi ni·∫øt b√†n" c·ªßa d·ªØ li·ªáu - n∆°i m·ªçi metrics v√† logs ƒë∆∞·ª£c tr·ª±c quan h√≥a m·ªôt c√°ch ƒë·∫πp m·∫Øt, gi√∫p ta ƒë·∫°t ƒë∆∞·ª£c s·ª± "gi√°c ng·ªô" v·ªÅ tr·∫°ng th√°i h·ªá th·ªëng! üòä

**C√°ch ho·∫°t ƒë·ªông:**

Grafana k·∫øt n·ªëi ƒë·∫øn Prometheus v√† Loki qua datasources. Ta d√πng PromQL ƒë·ªÉ query metrics v√† LogQL ƒë·ªÉ query logs. Dashboard t·ª± ƒë·ªông refresh m·ªói 15 gi√¢y ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi nh·∫•t.

```json
{
  "panels": [
    {
      "title": "KPI‚ÄìCPU Usage",
      "targets": [{
        "expr": "llm_process_cpu_percent{framework=\"transformers\"}"
      }],
      "type": "stat",
      "fieldConfig": {
        "defaults": {
          "color": {"fixedColor": "#27AE60"},
          "thresholds": {
            "steps": [
              {"color": "green", "value": 0},
              {"color": "yellow", "value": 70},
              {"color": "red", "value": 90}
            ]
          }
        }
      }
    }
  ]
}
```

**K·∫øt qu·∫£:**

Dashboard hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß metrics v·ªõi Forest Green theme, d·ªÖ ƒë·ªçc v√† ph√¢n t√≠ch. Ta c√≥ th·ªÉ th·∫•y CPU, Memory, GPU usage c·ªßa t·ª´ng process m·ªôt c√°ch tr·ª±c quan.

---

## 3. √Åp d·ª•ng MonitorAI: Chi ti·∫øt k·ªπ thu·∫≠t

### 3.1 Ki·∫øn tr√∫c h·ªá th·ªëng

H·ªá th·ªëng MonitorAI g·ªìm 7 components ch√≠nh, m·ªói component c√≥ vai tr√≤ ri√™ng:

| Component | Port | Ch·ª©c nƒÉng |
|-----------|------|-----------|
| **LLM Monitor** | 9101 | Ph√°t hi·ªán v√† thu th·∫≠p metrics t·ª´ LLM processes |
| **GPU Exporter** | 9100 | Thu th·∫≠p overall GPU metrics t·ª´ nvidia-smi |
| **Prometheus** | 9090 | Scrape v√† l∆∞u tr·ªØ metrics |
| **Loki** | 3100 | L∆∞u tr·ªØ logs |
| **Tempo** | 3200 | L∆∞u tr·ªØ traces (future) |
| **Grafana** | 3000 | Visualization dashboard |
| **Promtail** | - | Ship logs ƒë·∫øn Loki |

**Data Flow:**

D·ªØ li·ªáu ch·∫£y qua h·ªá th·ªëng theo 3 lu·ªìng:
- **Metrics**: LLM Processes ‚Üí GPU JSON files ‚Üí LLM Monitor ‚Üí Prometheus ‚Üí Grafana
- **Logs**: LLM Processes ‚Üí Log files ‚Üí Promtail ‚Üí Loki ‚Üí Grafana
- **GPU Overall**: nvidia-smi ‚Üí GPU Exporter ‚Üí Prometheus ‚Üí Grafana

### 3.2 GPU Monitoring tr√™n Windows v√† Linux

#### 3.2.1 Tr√™n Windows

**V·∫•n ƒë·ªÅ:**

Tr√™n Windows, `nvidia-smi` kh√¥ng th·ªÉ query GPU memory per process m·ªôt c√°ch ch√≠nh x√°c. ƒê√¢y l√† h·∫°n ch·∫ø c·ªßa Windows, kh√¥ng ph·∫£i c·ªßa `nvidia-smi`.

**Gi·∫£i ph√°p:**

Ta d√πng c∆° ch·∫ø **file-based exposure**. Process t·ª± ghi GPU memory v√†o file JSON, r·ªìi LLM Monitor ƒë·ªçc file ƒë√≥.

**B∆∞·ªõc 1: Process t·ª± expose GPU memory**

Khi ch·∫°y LLM model, ta ghi GPU memory v√†o file `logs/gpu-info-{PID}.json`. File n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t m·ªói l·∫ßn inference (m·ªói 10 gi√¢y).

```python
# Trong run-llm-model-gpu.py
def save_gpu_info(pid):
    gpu_info = {
        'pid': pid,
        'gpu_memory_allocated_bytes': torch.cuda.memory_allocated(0),
        'gpu_memory_reserved_bytes': torch.cuda.memory_reserved(0),
        'gpu_utilization': get_gpu_utilization(),
        'gpu_index': 0,
        'timestamp': datetime.now().isoformat()
    }
    
    gpu_info_file = Path(f'logs/gpu-info-{pid}.json')
    with open(gpu_info_file, 'w') as f:
        json.dump(gpu_info, f)
```

**B∆∞·ªõc 2: LLM Monitor ƒë·ªçc file JSON**

LLM Monitor ƒë·ªçc t·∫•t c·∫£ file `logs/gpu-info-*.json` m·ªói 10 gi√¢y. N·∫øu process kh√¥ng c√≤n t·ªìn t·∫°i, ta t·ª± ƒë·ªông x√≥a file.

```python
# Trong llm_monitor.py
def read_gpu_info_files():
    gpu_info_files = Path('logs').glob('gpu-info-*.json')
    for file in gpu_info_files:
        pid = int(file.stem.split('-')[-1])
        if not psutil.pid_exists(pid):
            file.unlink()  # X√≥a file n·∫øu process kh√¥ng c√≤n t·ªìn t·∫°i
            continue
        
        with open(file) as f:
            gpu_info = json.load(f)
            # Expose metrics t·ª´ gpu_info
```

**L·ª£i √≠ch:**

C√°ch n√†y cho ta GPU memory ch√≠nh x√°c t·ª´ PyTorch, kh√¥ng ph·ª• thu·ªôc v√†o `nvidia-smi`. ƒê·ªô ch√≠nh x√°c cao h∆°n nhi·ªÅu.

#### 3.2.2 Tr√™n Linux

**Kh√°c bi·ªát:**

Tr√™n Linux, `nvidia-smi` c√≥ th·ªÉ query GPU memory per process m·ªôt c√°ch ch√≠nh x√°c h∆°n nhi·ªÅu so v·ªõi Windows. Ta c√≥ th·ªÉ d√πng l·ªánh:

```bash
nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
```

L·ªánh n√†y s·∫Ω tr·∫£ v·ªÅ danh s√°ch c√°c processes ƒëang s·ª≠ d·ª•ng GPU c√πng v·ªõi memory usage c·ªßa t·ª´ng process.

**Gi·∫£i ph√°p:**

Tr√™n Linux, ta c√≥ **2 l·ª±a ch·ªçn**:

**Option 1: D√πng nvidia-smi tr·ª±c ti·∫øp (ƒê∆°n gi·∫£n h∆°n)**

LLM Monitor c√≥ th·ªÉ query `nvidia-smi` tr·ª±c ti·∫øp ƒë·ªÉ l·∫•y GPU memory per process. Kh√¥ng c·∫ßn file-based exposure.

```python
# Tr√™n Linux, c√≥ th·ªÉ d√πng nvidia-smi tr·ª±c ti·∫øp
import subprocess

def get_gpu_memory_per_process():
    result = subprocess.run(
        ['nvidia-smi', '--query-compute-apps=pid,used_memory', '--format=csv,noheader,nounits'],
        capture_output=True,
        text=True
    )
    # Parse k·∫øt qu·∫£ v√† tr·∫£ v·ªÅ dict {pid: memory_bytes}
    return parse_nvidia_smi_output(result.stdout)
```

**Option 2: V·∫´n d√πng file-based exposure (Nh·∫•t qu√°n v·ªõi Windows)**

N·∫øu ta mu·ªën code ch·∫°y ƒë∆∞·ª£c tr√™n c·∫£ Windows v√† Linux m√† kh√¥ng c·∫ßn thay ƒë·ªïi, ta v·∫´n c√≥ th·ªÉ d√πng file-based exposure. C√°ch n√†y ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n gi·ªØa c√°c platform.

> üí° **L∆∞u √Ω**: Tr√™n Linux, ta **kh√¥ng b·∫Øt bu·ªôc** ph·∫£i d√πng file-based exposure nh∆∞ tr√™n Windows. Tuy nhi√™n, n·∫øu mu·ªën code ch·∫°y ƒë∆∞·ª£c tr√™n c·∫£ hai platform m√† kh√¥ng c·∫ßn thay ƒë·ªïi, file-based approach v·∫´n l√† l·ª±a ch·ªçn t·ªët. Ngo√†i ra, file-based approach cho ta GPU memory ch√≠nh x√°c t·ª´ PyTorch (thay v√¨ t·ª´ nvidia-smi), n√™n c√≥ th·ªÉ ch√≠nh x√°c h∆°n trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p.

### 3.3 LLM Detection Patterns

Ta s·ª≠ d·ª•ng regex patterns ƒë·ªÉ ph√°t hi·ªán c√°c LLM frameworks. M·ªói framework c√≥ patterns ri√™ng:

```python
LLM_PATTERNS = {
    'transformers': [
        r'transformers',
        r'huggingface',
        r'\.from_pretrained',
        r'pipeline\(.*model',
    ],
    'llama.cpp': [
        r'llama',
        r'llama\.cpp',
        r'gguf',
    ],
    'vllm': [
        r'vllm',
        r'vllm\.engine',
    ],
    'tensorrt': [
        r'tensorrt',
        r'trt',
    ],
    'onnx': [
        r'onnxruntime',
        r'onnx',
    ],
    'pytorch': [
        r'torch',
        r'pytorch',
    ],
    'tensorflow': [
        r'tensorflow',
        r'tf\.',
    ],
}
```

**Model name extraction:**

Ta c≈©ng t·ª± ƒë·ªông tr√≠ch xu·∫•t t√™n model t·ª´ command line:

```python
MODEL_NAME_PATTERNS = [
    r'model[_-]?name["\']?\s*[:=]\s*["\']?([^"\']+)',
    r'--model[_-]?name["\']?\s+([^\s]+)',
    r'--model["\']?\s+([^\s]+)',
    r'model["\']?\s*[:=]\s*["\']?([^"\']+)',
    r'from_pretrained\(["\']([^"\']+)',
    r'load[_-]?model\(["\']([^"\']+)',
]
```

### 3.4 Metrics ƒë∆∞·ª£c expose

**LLM Process Metrics:**

Ta expose 5 metrics ch√≠nh cho m·ªói LLM process:

| Metric | Type | Labels | M√¥ t·∫£ |
|--------|------|--------|-------|
| `llm_process_count` | Gauge | llm_type, model_name, framework | S·ªë l∆∞·ª£ng LLM processes |
| `llm_process_cpu_percent` | Gauge | pid, name, llm_type, model_name, framework | CPU usage per process |
| `llm_process_memory_bytes` | Gauge | pid, name, llm_type, model_name, framework | Memory usage per process |
| `llm_process_gpu_memory_bytes` | Gauge | pid, name, gpu, llm_type, model_name, framework | GPU memory usage per process |
| `llm_process_gpu_utilization` | Gauge | pid, name, gpu, llm_type, model_name, framework | GPU utilization per process |

**Overall GPU Metrics (t·ª´ GPU Exporter):**

Ta c≈©ng expose overall GPU metrics:

| Metric | Type | Labels | M√¥ t·∫£ |
|--------|------|--------|-------|
| `nvidia_gpu_utilization` | Gauge | gpu, gpu_type, service, namespace | Overall GPU utilization |
| `nvidia_gpu_memory_used_bytes` | Gauge | gpu, gpu_type, service, namespace | Overall GPU memory used |
| `nvidia_gpu_memory_total_bytes` | Gauge | gpu, gpu_type, service, namespace | Total GPU memory |
| `nvidia_gpu_temperature` | Gauge | gpu, gpu_type, service, namespace | GPU temperature |
| `nvidia_gpu_power_usage` | Gauge | gpu, gpu_type, service, namespace | GPU power usage |

---

## 4. K·∫øt qu·∫£ th·ª±c t·∫ø: Dashboard v√† Metrics

### 4.1 Dashboard Forest Green Theme

<div align="center">
![Dashboard CPU v√† GPU Monitoring](https://i.ibb.co/Qv9zWhWG/Dashboard-CPU-GPU.png)
</div>

<div align="center">
*H√¨nh 1: Dashboard MonitorAI v·ªõi Forest Green theme - Hi·ªÉn th·ªã CPU, Memory, GPU metrics cho LLM processes*
</div>

Quan s√°t H√¨nh 1, ta c√≥ th·ªÉ th·∫•y dashboard ƒë∆∞·ª£c thi·∫øt k·∫ø theo **Forest Green theme** v·ªõi dark background v√† c√°c elements m√†u xanh l√°, v√†ng n·ªïi b·∫≠t. Dashboard ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh c√°c ph·∫ßn ch√≠nh:

**Filters v√† Time Range:**

·ªû tr√™n c√πng, ta th·∫•y:
- **Filters**: "Framework All" v√† "Model Name All" - cho ph√©p filter theo framework v√† model name
- **Time Range**: "Last 1 minute" v·ªõi refresh interval "30s" - c·∫≠p nh·∫≠t t·ª± ƒë·ªông m·ªói 30 gi√¢y

**T·∫ßng 1: KPI Overview (4 panels m√†u xanh l√°)**

·ªû h√†ng ƒë·∫ßu ti√™n, ta th·∫•y 4 KPI panels l·ªõn hi·ªÉn th·ªã c√°c ch·ªâ s·ªë quan tr·ªçng:
- **KPI-Active LLM Processes**: **2** - S·ªë l∆∞·ª£ng LLM processes ƒëang ch·∫°y
- **KPI-Total GPU VRAM Used**: **3.0 GiB** - T·ªïng GPU memory ƒëang s·ª≠ d·ª•ng
- **GPU_Temperature**: **41.0¬∞** - Nhi·ªát ƒë·ªô GPU hi·ªán t·∫°i
- **KPI-Avg GPU Utilization**: **13.0%** - GPU utilization trung b√¨nh

ƒê√¢y l√† nh·ªØng s·ªë li·ªáu ta c·∫ßn theo d√µi th∆∞·ªùng xuy√™n nh·∫•t ƒë·ªÉ n·∫Øm ƒë∆∞·ª£c t·ªïng quan h·ªá th·ªëng.

**T·∫ßng 2: Detail Metrics (2 graphs v√† 1 log panel)**

·ªû h√†ng th·ª© hai, ta th·∫•y 3 panels chi ti·∫øt:

**DETAIL-CPU Metrics by Process (Graph b√™n tr√°i):**
- Time-series line graph hi·ªÉn th·ªã CPU usage (memory bytes) theo th·ªùi gian
- 2 processes ƒëang ƒë∆∞·ª£c monitor:
  - **DialoGPT-small** (PID 22716) - ƒë∆∞·ªùng m√†u xanh l√°, gi√° tr·ªã ~307 MB
  - **gpt2** (PID 15176) - ƒë∆∞·ªùng m√†u v√†ng, gi√° tr·ªã ~254 MB
- B·∫£ng b√™n d∆∞·ªõi hi·ªÉn th·ªã Last, Max, Mean values cho t·ª´ng process

**DETAIL-GPU Metrics by Process (Graph gi·ªØa):**
- Time-series area graph hi·ªÉn th·ªã GPU memory usage theo th·ªùi gian
- 2 processes ƒëang ƒë∆∞·ª£c monitor:
  - **DialoGPT-small** (PID 22716) - ƒë∆∞·ªùng m√†u xanh l√°, gi√° tr·ªã 0 (kh√¥ng d√πng GPU)
  - **gpt2** (PID 15176) - v√πng m√†u v√†ng, gi√° tr·ªã dao ƒë·ªông t·ª´ 0 ƒë·∫øn ~26 GiB, peak ·ªü ~21-26 GiB
- B·∫£ng b√™n d∆∞·ªõi hi·ªÉn th·ªã Last v√† Max values

**LOGS-User & Model Interactions (Panel b√™n ph·∫£i):**
- Log viewer hi·ªÉn th·ªã logs t∆∞∆°ng t√°c gi·ªØa user v√† model
- M·ªói entry c√≥:
  - Timestamp (v√≠ d·ª•: "2025-11-14 16:43:03.099")
  - Level: "INFO"
  - Tag: `[USER]` cho user requests ho·∫∑c `[MODE]` cho model responses
  - Message v·ªõi ID (v√≠ d·ª•: #278, #284, #277)
  - N·ªôi dung nh∆∞ "Hello, this is test..." ho·∫∑c "Hello, this is GPU test..."

**M√†u s·∫Øc Forest Green:**

Dashboard s·ª≠ d·ª•ng dark theme v·ªõi b·∫£ng m√†u th·ªëng nh·∫•t:
- **Dark background**: N·ªÅn t·ªëi t·∫°o ƒë·ªô t∆∞∆°ng ph·∫£n t·ªët
- **Green elements**: M√†u xanh l√° (#27AE60) cho c√°c KPI panels v√† m·ªôt s·ªë graphs
- **Yellow elements**: M√†u v√†ng cho c√°c processes v√† metrics kh√°c
- **Soft text**: M√†u xanh nh·∫°t cho text ph·ª•

M√†u s·∫Øc n√†y t·∫°o c·∫£m gi√°c d·ªÖ ch·ªãu, kh√¥ng g√¢y m·ªèi m·∫Øt khi xem l√¢u, ph√π h·ª£p v·ªõi theme "Forest Green".

### 4.2 Metrics th·ª±c t·∫ø t·ª´ Dashboard

T·ª´ dashboard th·ª±c t·∫ø trong H√¨nh 1, ta c√≥ th·ªÉ th·∫•y c√°c metrics nh∆∞ sau:

**Process DialoGPT-small** (PID 22716):
- CPU Memory: ~307 MB (Last/Max: 307,068,928 bytes)
- GPU Memory: 0 GiB (kh√¥ng s·ª≠ d·ª•ng GPU)
- Model: DialoGPT-small

**Process gpt2** (PID 15176):
- CPU Memory: ~254 MB (Last/Max: 254,058,496 bytes)
- GPU Memory: ~21-26 GiB (Last: 21 GiB, Max: 26 GiB)
- Model: gpt2

**Overall GPU Metrics:**
- Active LLM Processes: 2
- Total GPU VRAM Used: 3.0 GiB
- GPU Temperature: 41.0¬∞C
- Avg GPU Utilization: 13.0%

T·ª´ nh·ªØng s·ªë li·ªáu n√†y, ta c√≥ th·ªÉ bi·∫øt:
- C√≥ 2 LLM processes ƒëang ch·∫°y: DialoGPT-small (CPU only) v√† gpt2 (GPU)
- GPU ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng ·ªü m·ª©c th·∫•p (13% utilization, 3.0 GiB VRAM)
- Nhi·ªát ƒë·ªô GPU ·ªü m·ª©c r·∫•t an to√†n (41¬∞C)
- Process gpt2 ƒëang s·ª≠ d·ª•ng GPU memory ƒë√°ng k·ªÉ (21-26 GiB)

### 4.3 Logs aggregation

Logs ƒë∆∞·ª£c t·∫≠p trung trong Loki. Ta c√≥ th·ªÉ xem logs nh∆∞ sau:

> üí° **Fun fact**: Nh∆∞ v·ªã th·∫ßn Loki trong th·∫ßn tho·∫°i B·∫Øc √Çu c√≥ kh·∫£ nƒÉng "bi·∫øn h√¨nh", c√¥ng c·ª• Loki c·ªßa ta c≈©ng "bi·∫øn h√≥a" logs m·ªôt c√°ch linh ho·∫°t, gi√∫p ta t√¨m th·∫•y nh·ªØng th√¥ng tin ·∫©n gi·∫•u trong h·ªá th·ªëng!

```
2025-01-15 10:30:15 - INFO - Model gpt2 loaded successfully
2025-01-15 10:30:16 - INFO - GPU available: NVIDIA RTX 4090
2025-01-15 10:30:17 - INFO - Starting inference...
2025-01-15 10:30:25 - INFO - Inference completed in 8.2s
2025-01-15 10:30:26 - ERROR - Failed to load model: Out of memory
```

**LogQL queries:**

Ta c√≥ th·ªÉ query logs b·∫±ng LogQL:
- `{job="llm-model"} |= "error"` - T√¨m t·∫•t c·∫£ error logs
- `{job="llm-model"} | json | model_name="gpt2"` - Filter logs theo model name
- `{job="llm-model"} | line_format "{{.timestamp}} {{.message}}"` - Format logs

ƒêi·ªÅu n√†y gi√∫p ta t√¨m ki·∫øm logs nhanh ch√≥ng, kh√¥ng c·∫ßn m·ªü nhi·ªÅu files.

---

## 5. So s√°nh: Tr∆∞·ªõc v√† Sau khi c√≥ MonitorAI

### 5.1 Tr∆∞·ªõc khi c√≥ MonitorAI

**V·∫•n ƒë·ªÅ ta g·∫∑p ph·∫£i:**

Khi ch∆∞a c√≥ MonitorAI, ta g·∫∑p nhi·ªÅu kh√≥ khƒÉn:
- ‚ùå Kh√¥ng bi·∫øt process n√†o ƒëang ch·∫°y LLM
- ‚ùå Kh√¥ng c√≥ metrics CPU, Memory, GPU per process
- ‚ùå Logs r·∫£i r√°c ·ªü nhi·ªÅu files kh√°c nhau
- ‚ùå Kh√¥ng c√≥ dashboard ƒë·ªÉ visualize
- ‚ùå Kh√≥ debug khi c√≥ v·∫•n ƒë·ªÅ
- ‚ùå Kh√¥ng bi·∫øt GPU utilization v√† memory usage

**C√°ch l√†m th·ªß c√¥ng:**

Ta ph·∫£i l√†m th·ªß c√¥ng:
- Ch·∫°y `nvidia-smi` th·ªß c√¥ng ƒë·ªÉ xem GPU
- Ch·∫°y `top` ho·∫∑c `htop` ƒë·ªÉ xem CPU/Memory
- ƒê·ªçc logs t·ª´ nhi·ªÅu files kh√°c nhau
- Kh√¥ng c√≥ l·ªãch s·ª≠ metrics

C√°ch n√†y t·ªën th·ªùi gian v√† d·ªÖ b·ªè s√≥t th√¥ng tin quan tr·ªçng.

### 5.2 Sau khi c√≥ MonitorAI

**Gi·∫£i ph√°p MonitorAI mang l·∫°i:**

V·ªõi MonitorAI, ta c√≥:
- ‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán LLM processes
- ‚úÖ Metrics ƒë·∫ßy ƒë·ªß: CPU, Memory, GPU per process
- ‚úÖ Logs t·∫≠p trung trong Loki
- ‚úÖ Dashboard ƒë·∫πp v·ªõi Forest Green theme
- ‚úÖ D·ªÖ debug v·ªõi logs v√† metrics
- ‚úÖ Real-time monitoring m·ªói 10-15 gi√¢y
- ‚úÖ L·ªãch s·ª≠ metrics 200 gi·ªù

**C·∫£i thi·ªán:**

So v·ªõi c√°ch l√†m th·ªß c√¥ng:
- üìä **Visibility**: TƒÉng 100% - th·∫•y ƒë∆∞·ª£c t·∫•t c·∫£ metrics v√† logs
- ‚ö° **Debug time**: Gi·∫£m 80% - t√¨m v·∫•n ƒë·ªÅ nhanh h∆°n
- üéØ **Proactive**: Ph√°t hi·ªán v·∫•n ƒë·ªÅ tr∆∞·ªõc khi ·∫£nh h∆∞·ªüng
- üìà **Optimization**: D·ª±a v√†o metrics ƒë·ªÉ t·ªëi ∆∞u performance

---

## 6. H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

### 6.1 Y√™u c·∫ßu

Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu, ta c·∫ßn chu·∫©n b·ªã:
- Docker Desktop ƒëang ch·∫°y
- Conda environment `MonitorAI` v·ªõi Python 3.11+
- NVIDIA GPU v·ªõi nvidia-smi (optional, cho GPU metrics)
- PyTorch v·ªõi CUDA support (cho GPU monitoring ch√≠nh x√°c)

### 6.2 B∆∞·ªõc 1: Start T·∫•t C·∫£ Services

Ta ch·∫°y script PowerShell ƒë·ªÉ kh·ªüi ƒë·ªông t·∫•t c·∫£ services:

```powershell
.\start-all.ps1
```

Script n√†y s·∫Ω t·ª± ƒë·ªông kh·ªüi ƒë·ªông:
- Docker services: Grafana (3000), Prometheus (9090), Loki (3100), Tempo (3200)
- LLM Monitor (port 9101) - ch·∫°y background
- GPU Exporter (port 9100) - ch·∫°y background (n·∫øu c√≥ NVIDIA GPU)

Sau khi ch·∫°y, ta ƒë·ª£i v√†i gi√¢y ƒë·ªÉ c√°c services kh·ªüi ƒë·ªông ho√†n to√†n.

### 6.3 B∆∞·ªõc 2: Ch·∫°y LLM Model (Ch·ªâ ƒë·ªÉ test)

> üí° **L∆∞u √Ω**: B∆∞·ªõc n√†y ch·ªâ c·∫ßn thi·∫øt khi ta mu·ªën **test h·ªá th·ªëng monitoring**. N·∫øu ta ƒëang ch·∫°y gi√°m s√°t th·ª±c t·∫ø c√°c LLM processes ƒë√£ c√≥ s·∫µn, ta c√≥ th·ªÉ b·ªè qua b∆∞·ªõc n√†y v√† chuy·ªÉn th·∫≥ng sang B∆∞·ªõc 3 ƒë·ªÉ xem dashboard.

**Option 1: Ch·∫°y model v·ªõi GPU (khuy·∫øn ngh·ªã)**

Ta m·ªü terminal m·ªõi, k√≠ch ho·∫°t conda environment v√† ch·∫°y:

```powershell
python run-llm-model-gpu.py
```

Script n√†y s·∫Ω:
- T·ª± ƒë·ªông detect GPU v√† load model l√™n GPU
- Expose GPU memory usage qua file JSON (`logs/gpu-info-{PID}.json`)
- LLM Monitor s·∫Ω ƒë·ªçc file n√†y ƒë·ªÉ l·∫•y GPU metrics ch√≠nh x√°c

**Option 2: Ch·∫°y model CPU ho·∫∑c GPU (t·ª± ƒë·ªông detect)**

N·∫øu ta mu·ªën h·ªá th·ªëng t·ª± ƒë·ªông detect CPU/GPU:

```powershell
python run-llm-model.py --model-name microsoft/DialoGPT-small
```

LLM Monitor s·∫Ω t·ª± ƒë·ªông detect model v√† collect metrics (CPU, Memory, GPU n·∫øu c√≥).

### 6.4 B∆∞·ªõc 3: Xem Dashboard

Sau khi ch·∫°y model, ta truy c·∫≠p dashboard:
- URL: http://localhost:3000
- Login: `admin` / `admin`
- Dashboard: **Dashboards** ‚Üí **LLM Monitoring ‚Äì Forest Green Dashboard**

Ta s·∫Ω th·∫•y metrics v√† logs hi·ªÉn th·ªã real-time tr√™n dashboard.

### 6.5 D·ª´ng services

Khi xong, ta d·ª´ng t·∫•t c·∫£ services:

```powershell
.\stop-all.ps1
```

Script n√†y s·∫Ω d·ª´ng Docker services, LLM Monitor, v√† GPU Exporter.

---

## 7. Tham s·ªë c·∫•u h√¨nh

### 7.1 C·∫•u h√¨nh LLM Monitor

Ta c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh c√°c tham s·ªë sau:

| Tham s·ªë | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | M√¥ t·∫£ |
|---------|------------------|-------|
| **Scrape Interval** | 10 gi√¢y | T·∫ßn su·∫•t qu√©t processes |
| **Port** | 9101 | HTTP port ƒë·ªÉ expose metrics |
| **GPU Info Path** | `logs/gpu-info-*.json` | ƒê∆∞·ªùng d·∫´n ƒë·∫øn GPU info files |
| **Cleanup Timeout** | 10 gi√¢y | Th·ªùi gian ch·ªù tr∆∞·ªõc khi x√≥a metrics c·ªßa process ƒë√£ d·ª´ng |

### 7.2 C·∫•u h√¨nh Prometheus

| Tham s·ªë | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | M√¥ t·∫£ |
|---------|------------------|-------|
| **Scrape Interval** | 15 gi√¢y | T·∫ßn su·∫•t scrape metrics |
| **Retention** | 200 gi·ªù | Th·ªùi gian l∆∞u tr·ªØ metrics |
| **Storage Path** | `/prometheus` | ƒê∆∞·ªùng d·∫´n l∆∞u tr·ªØ data |

### 7.3 C·∫•u h√¨nh Grafana

| Tham s·ªë | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | M√¥ t·∫£ |
|---------|------------------|-------|
| **Port** | 3000 | HTTP port |
| **Admin User** | `admin` | Username m·∫∑c ƒë·ªãnh |
| **Admin Password** | `admin` | Password m·∫∑c ƒë·ªãnh |
| **Auto-refresh** | 15 gi√¢y | T·∫ßn su·∫•t refresh dashboard |

---

## 8. K·∫øt lu·∫≠n: Nh·ªØng g√¨ ch√∫ng ta ƒë√£ h·ªçc ƒë∆∞·ª£c

### 8.1 Th√†nh t·ª±u ƒë·∫°t ƒë∆∞·ª£c

D·ª± √°n MonitorAI ƒë√£ ch·ª©ng minh r·∫±ng **observability stack c√≥ th·ªÉ gi·∫£i quy·∫øt hi·ªáu qu·∫£ b√†i to√°n monitoring LLM processes** trong th·ª±c t·∫ø. V·ªõi stack Grafana + Prometheus + Loki + Tempo, h·ªá th·ªëng ƒë√£:

- **T·ª± ƒë·ªông ph√°t hi·ªán** c√°c LLM processes v·ªõi nhi·ªÅu frameworks kh√°c nhau
- **Thu th·∫≠p metrics ƒë·∫ßy ƒë·ªß**: CPU, Memory, GPU per process
- **T·∫≠p trung logs** t·ª´ nhi·ªÅu processes
- **Dashboard ƒë·∫πp** v·ªõi Forest Green theme
- **Real-time monitoring** m·ªói 10-15 gi√¢y
- **H·ªó tr·ª£ Windows v√† Linux** v·ªõi file-based GPU monitoring (Windows b·∫Øt bu·ªôc, Linux t√πy ch·ªçn)

### 8.2 B√†i h·ªçc quan tr·ªçng

**File-based GPU exposure l√† ch√¨a kh√≥a tr√™n Windows**: Vi·ªác s·ª≠ d·ª•ng JSON files ƒë·ªÉ expose GPU memory t·ª´ processes thay v√¨ d·ª±a v√†o `nvidia-smi` ƒë√£ mang l·∫°i ƒë·ªô ch√≠nh x√°c cao h∆°n. Tr√™n Linux, ta c√≥ th·ªÉ d√πng `nvidia-smi` tr·ª±c ti·∫øp, nh∆∞ng file-based approach v·∫´n l√† l·ª±a ch·ªçn t·ªët ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n gi·ªØa c√°c platform. **Prometheus metrics format** l√† standard v√† d·ªÖ t√≠ch h·ª£p v·ªõi nhi·ªÅu tools kh√°c.

**Observability stack l√† foundation t·ªët**: Grafana + Prometheus + Loki + Tempo t·∫°o th√†nh m·ªôt stack m·∫°nh m·∫Ω, c√≥ th·ªÉ m·ªü r·ªông v√† production-ready. **Forest Green theme** t·∫°o ra dashboard ƒë·∫πp, d·ªÖ ƒë·ªçc, th·ªëng nh·∫•t.

### 8.3 H·∫°n ch·∫ø v√† th√°ch th·ª©c

**Windows GPU monitoring**: C·∫ßn file-based approach v√¨ `nvidia-smi` kh√¥ng ch√≠nh x√°c. Tr√™n Linux, c√≥ th·ªÉ d√πng `nvidia-smi` tr·ª±c ti·∫øp, nh∆∞ng file-based approach v·∫´n ƒë∆∞·ª£c khuy·∫øn ngh·ªã ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n. **Process detection** ph·ª• thu·ªôc v√†o pattern matching, c√≥ th·ªÉ miss m·ªôt s·ªë processes n·∫øu pattern kh√¥ng match.

**Scalability**: V·ªõi nhi·ªÅu hosts, c·∫ßn th√™m Prometheus federation ho·∫∑c Thanos. **Tracing** ch∆∞a ƒë∆∞·ª£c implement, c·∫ßn t√≠ch h·ª£p OpenTelemetry SDK.

### 8.4 ·ª®ng d·ª•ng th·ª±c t·∫ø

K·∫øt qu·∫£ n√†y c√≥ th·ªÉ **ti·∫øt ki·ªám h√†ng gi·ªù debug time** v√† gi√∫p **t·ªëi ∆∞u performance** c·ªßa LLM applications. ƒê·∫∑c bi·ªát h·ªØu √≠ch trong **production environments** v·ªõi nhi·ªÅu LLM models ch·∫°y ƒë·ªìng th·ªùi.

**H∆∞·ªõng ph√°t tri·ªÉn**: T√≠ch h·ª£p OpenTelemetry SDK ƒë·ªÉ c√≥ distributed tracing, th√™m alerting rules, v√† m·ªü r·ªông cho multi-host monitoring.

---

*"Observability kh√¥ng ch·ªâ l√† monitoring ‚Äî m√† l√† c√°ch ch√∫ng ta hi·ªÉu v√† t·ªëi ∆∞u h·ªá th·ªëng AI c·ªßa m√¨nh."*

---

## 9. T√†i li·ªáu tham kh·∫£o

1. Prometheus Documentation. (2025). *Prometheus - Monitoring system & time series database*. https://prometheus.io/docs/
2. Grafana Labs. (2025). *Grafana - The open observability platform*. https://grafana.com/docs/
3. Loki Documentation. (2025). *Loki - Log aggregation system*. https://grafana.com/docs/loki/latest/
4. Tempo Documentation. (2025). *Tempo - Distributed tracing backend*. https://grafana.com/docs/tempo/latest/
5. psutil Documentation. (2025). *psutil - Cross-platform lib for process and system monitoring*. https://psutil.readthedocs.io/
6. PyTorch Documentation. (2025). *PyTorch - GPU memory management*. https://pytorch.org/docs/stable/notes/cuda.html
