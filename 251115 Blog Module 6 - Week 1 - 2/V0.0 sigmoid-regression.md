---
title: "ğŸ§  HÃ m Sigmoid trong Logistic Regression â€” Tá»« log-odds Ä‘áº¿n xÃ¡c suáº¥t"
description: "Giáº£i thÃ­ch vÃ¬ sao hÃ m Sigmoid xuáº¥t hiá»‡n tá»± nhiÃªn trong Logistic Regression: tá»« odds â†’ logit â†’ giáº£ Ä‘á»‹nh tuyáº¿n tÃ­nh â†’ sigmoid. Bao gá»“m lá»‹ch sá»­, tá»‘i Æ°u hÃ³a, vÃ  vÃ­ dá»¥ minh há»a."
date: 2025-11-13
author: Nguyá»…n Quang Linh
tags: [machine learning, logistic regression, sigmoid, xÃ¡c suáº¥t, thá»‘ng kÃª, cross-entropy, maximum likelihood]
---

# ğŸ§  HÃ m Sigmoid trong Logistic Regression â€” Tá»« log-odds Ä‘áº¿n xÃ¡c suáº¥t

Khi há»c vá» **Logistic Regression**, ta biáº¿t mÃ´ hÃ¬nh dÃ¹ng **hÃ m Sigmoid** Ä‘á»ƒ Ã¡nh xáº¡ giÃ¡ trá»‹ Ä‘áº§u ra vá» khoáº£ng \([0,1]\).  
NhÆ°ng Ã­t ai há»i sÃ¢u hÆ¡n:

> ğŸ’­ LÃ m sao ngÆ°á»i ta â€œnghÄ© raâ€ Ä‘Æ°á»£c hÃ m Sigmoid? NÃ³ xuáº¥t phÃ¡t tá»« Ä‘Ã¢u vÃ  táº¡i sao láº¡i há»£p lÃ½ Ä‘áº¿n nhÆ° váº­y?

CÃ¢u tráº£ lá»i thá»±c ra ráº¥t Ä‘áº¹p:  
HÃ m Sigmoid Ä‘Æ°á»£c **suy luáº­n toÃ¡n há»c tá»± nhiÃªn** tá»« nhu cáº§u **biá»ƒu diá»…n xÃ¡c suáº¥t báº±ng mÃ´ hÃ¬nh tuyáº¿n tÃ­nh**, chá»© khÃ´ng pháº£i Ä‘Æ°á»£c â€œchá»n bá»«aâ€.

---

## âš™ï¸ 1. Báº¯t Ä‘áº§u tá»« mong muá»‘n mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t

Giáº£ sá»­ ta Ä‘ang giáº£i quyáº¿t bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n:
- \(y = 1\): Ä‘á»‘i tÆ°á»£ng thuá»™c lá»›p 1  
- \(y = 0\): Ä‘á»‘i tÆ°á»£ng khÃ´ng thuá»™c lá»›p 1  

Ta muá»‘n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n:

\[
P(y=1|x)
\]

vÃ  giÃ¡ trá»‹ nÃ y **pháº£i náº±m trong [0,1]** Ä‘á»ƒ cÃ³ thá»ƒ hiá»ƒu lÃ  xÃ¡c suáº¥t.  
Tuy nhiÃªn, há»“i quy tuyáº¿n tÃ­nh:

\[
\hat{y} = w^T x + b
\]

cho káº¿t quáº£ cÃ³ thá»ƒ Ã¢m, hoáº·c lá»›n hÆ¡n 1 â€” âŒ khÃ´ng há»£p lÃ½ vá» máº·t xÃ¡c suáº¥t.

---

## ğŸ” 2. Ã tÆ°á»Ÿng: Dá»± Ä‘oÃ¡n **tá»· lá»‡ xáº£y ra vÃ  khÃ´ng xáº£y ra**

Thay vÃ¬ dá»± Ä‘oÃ¡n trá»±c tiáº¿p \(P\), cÃ¡c nhÃ  thá»‘ng kÃª tháº¿ ká»· 19 (Ä‘áº·c biá»‡t **Pierre FranÃ§ois Verhulst**, 1838) Ä‘Ã£ nghÄ© khÃ¡c Ä‘i:  
HÃ£y dá»± Ä‘oÃ¡n **tá»· lá»‡ odds**:

\[
\text{odds} = \frac{P(y=1|x)}{1 - P(y=1|x)}
\]

- Náº¿u \(P = 0.5\) â†’ odds = 1 (cÆ¡ há»™i ngang nhau)  
- Náº¿u \(P = 0.8\) â†’ odds = 4 (xáº£y ra gáº¥p 4 láº§n khÃ´ng xáº£y ra)  
- Náº¿u \(P = 0.2\) â†’ odds = 0.25 (xáº£y ra Ã­t hÆ¡n 4 láº§n)

Tuy nhiÃªn, odds váº«n chá»‰ dÆ°Æ¡ng, chÆ°a thá»ƒ mÃ´ hÃ¬nh hÃ³a báº±ng hÃ m tuyáº¿n tÃ­nh (vÃ¬ tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ã¢m).  
VÃ¬ váº­y, ta **láº¥y log cá»§a odds** Ä‘á»ƒ â€œtráº£iâ€ nÃ³ ra toÃ n trá»¥c thá»±c:

\[
\text{logit}(P) = \log\left(\frac{P}{1 - P}\right)
\]

â†’ ÄÃ¢y chÃ­nh lÃ  **hÃ m logit**.  
VÃ  nhá» log, ta cÃ³ thá»ƒ cho giÃ¡ trá»‹ nÃ y báº±ng **má»™t biá»ƒu thá»©c tuyáº¿n tÃ­nh**.

---

## ğŸ§® 3. Giáº£ Ä‘á»‹nh quan há»‡ tuyáº¿n tÃ­nh trong miá»n logit

Ta Ä‘áº·t giáº£ Ä‘á»‹nh:

\[
\text{logit}(P) = w^T x + b
\]

hay tÆ°Æ¡ng Ä‘Æ°Æ¡ng:

\[
\log\left(\frac{P}{1 - P}\right) = w^T x + b
\]

ÄÃ¢y lÃ  bÆ°á»›c â€œthiáº¿t káº¿â€ cá»‘t lÃµi â€” **má»™t giáº£ Ä‘á»‹nh thá»‘ng kÃª há»£p lÃ½**:
- VÃ¬ Ä‘áº§u vÃ o \(x\) áº£nh hÆ°á»Ÿng tuyáº¿n tÃ­nh lÃªn â€œÄ‘á»™ tin cáº­yâ€ (log-odds)
- MÃ  logit cho phÃ©p ta lÃ m viá»‡c trÃªn toÃ n miá»n sá»‘ thá»±c

Giá» ta chá»‰ cáº§n biáº¿n Ä‘á»•i ngÆ°á»£c Ä‘á»ƒ tÃ¬m \(P\).

---

## ğŸ”„ 4. Biáº¿n Ä‘á»•i ngÆ°á»£c â†’ HÃ m Sigmoid ra Ä‘á»i ğŸ¯

Báº¯t Ä‘áº§u tá»«:

\[
\frac{P}{1 - P} = e^{w^T x + b}
\]

Giáº£i theo \(P\):

\[
P = \frac{e^{w^T x + b}}{1 + e^{w^T x + b}} = \frac{1}{1 + e^{-(w^T x + b)}}
\]

ğŸ‰ ÄÃ¢y chÃ­nh lÃ  **hÃ m Sigmoid** mÃ  ta dÃ¹ng ngÃ y nay:

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

<div style="text-align:center;">
  <img src="https://i.ibb.co/v4Yr2Rb2/sigmoid.png" alt="ÄÆ°á»ng cong Sigmoid" width="720"/>
</div>

---

## ğŸ“ˆ 5. HÃ m Sigmoid vÃ  má»‘i quan há»‡ vá»›i Logit

- Sigmoid lÃ  **nghá»‹ch Ä‘áº£o cá»§a hÃ m logit**:  
  \[
  \sigma(z) = \frac{1}{1 + e^{-z}} \iff \text{logit}(P) = z
  \]
- Khi \(z = 0\) â†’ \(P = 0.5\)  
- Khi \(z \to +\infty\) â†’ \(P \to 1\)  
- Khi \(z \to -\infty\) â†’ \(P \to 0\)

HÃ m nÃ y **Ä‘Æ¡n Ä‘iá»‡u tÄƒng**, cÃ³ Ä‘áº¡o hÃ m gá»n Ä‘áº¹p:
\[
\sigma'(z) = \sigma(z)(1 - \sigma(z))
\]
â†’ thuáº­n lá»£i cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng **Gradient Descent**.

---

## ğŸ“Š 6. Tá»‘i Æ°u hÃ³a báº±ng Maximum Likelihood vÃ  Cross-Entropy

Sau khi cÃ³ mÃ´ hÃ¬nh \(P(y|x; w, b)\), ta cáº§n tÃ¬m tham sá»‘ \(w, b\).  
NguyÃªn táº¯c lÃ  **tá»‘i Ä‘a hÃ³a kháº£ nÄƒng dá»¯ liá»‡u xuáº¥t hiá»‡n** â€” Maximum Likelihood Estimation (MLE):

\[
L(w,b) = \prod_i P(y_i|x_i; w,b)
\]

Láº¥y log cho dá»… tá»‘i Æ°u:

\[
\ell(w,b) = \sum_i \left[y_i \log \sigma(z_i) + (1 - y_i)\log(1 - \sigma(z_i))\right]
\]

VÃ  ta **tá»‘i Ä‘a hÃ³a log-likelihood**, tÆ°Æ¡ng Ä‘Æ°Æ¡ng **tá»‘i thiá»ƒu hÃ³a hÃ m máº¥t mÃ¡t cross-entropy**:

\[
\mathcal{L} = -\ell(w,b)
\]

Gradient ráº¥t gá»n:

\[
\nabla_w \mathcal{L} = (\sigma(z) - y)x
\]

â†’ ÄÃ¢y lÃ  lÃ½ do hÃ m Sigmoid Ä‘Æ°á»£c Æ°a chuá»™ng: Ä‘áº¡o hÃ m cá»§a nÃ³ khiáº¿n cáº­p nháº­t gradient cá»±c ká»³ Ä‘Æ¡n giáº£n vÃ  á»•n Ä‘á»‹nh.

---

## âš–ï¸ 7. á»”n Ä‘á»‹nh sá»‘ khi tÃ­nh Sigmoid

Trong thá»±c táº¿, vá»›i \(z\) ráº¥t lá»›n hoáº·c ráº¥t nhá», \(e^{-z}\) cÃ³ thá»ƒ **trÃ n sá»‘ (overflow/underflow)**.  
Má»™t sá»‘ cÃ¡ch kháº¯c phá»¥c:
- Giá»›i háº¡n \(z\) trong khoáº£ng \([-20, 20]\) trÆ°á»›c khi tÃ­nh  
- DÃ¹ng biá»ƒu thá»©c á»•n Ä‘á»‹nh hÆ¡n trong log-loss:  
  \[
  \log(1 + e^{-z}) \approx \text{softplus}(z)
  \]
- Trong framework há»c mÃ¡y, ngÆ°á»i ta thÆ°á»ng dÃ¹ng hÃ m `torch.nn.BCEWithLogitsLoss` (PyTorch) hoáº·c `tf.nn.sigmoid_cross_entropy_with_logits` (TensorFlow), vá»‘n Ä‘Ã£ xá»­ lÃ½ á»•n Ä‘á»‹nh sá»‘ bÃªn trong.

---

## ğŸ§© 8. So sÃ¡nh Logistic vÃ  Probit

Má»™t mÃ´ hÃ¬nh gáº§n giá»‘ng Logistic Regression lÃ  **Probit Regression**, trong Ä‘Ã³:
\[
P(y=1|x) = \Phi(w^T x + b)
\]
vá»›i \(\Phi\) lÃ  hÃ m tÃ­ch lÅ©y cá»§a phÃ¢n phá»‘i chuáº©n.  
Hai mÃ´ hÃ¬nh nÃ y cho káº¿t quáº£ tÆ°Æ¡ng tá»±, chá»‰ khÃ¡c vá» **Ä‘uÃ´i phÃ¢n phá»‘i** vÃ  **diá»…n giáº£i thá»‘ng kÃª** â€” Logistic cÃ³ Ä‘uÃ´i dÃ i hÆ¡n, nÃªn thÆ°á»ng dÃ¹ng phá»• biáº¿n hÆ¡n trong há»c mÃ¡y.

---

## ğŸ§  9. Lá»‹ch sá»­ ra Ä‘á»i cá»§a hÃ m Sigmoid

- NÄƒm **1838**, **Pierre FranÃ§ois Verhulst** (nhÃ  toÃ¡n há»c Bá»‰) giá»›i thiá»‡u hÃ m logistic Ä‘á»ƒ mÃ´ táº£ **tÄƒng trÆ°á»Ÿng dÃ¢n sá»‘**:
  \[
  N(t) = \frac{K}{1 + e^{-r(t - t_0)}}
  \]
  Trong Ä‘Ã³:
  - \(K\): giá»›i háº¡n dÃ¢n sá»‘ tá»‘i Ä‘a  
  - \(r\): tá»‘c Ä‘á»™ tÄƒng trÆ°á»Ÿng  
  - \(t_0\): thá»i Ä‘iá»ƒm tÄƒng máº¡nh nháº¥t  

  ÄÃ¢y lÃ  má»™t **Ä‘Æ°á»ng cong S (sigmoid)** thá»ƒ hiá»‡n sá»± tÄƒng dáº§n cháº­m dáº§n khi tiáº¿n Ä‘áº¿n giá»›i háº¡n \(K\).

- Tháº¿ ká»· 20, cÃ¡c nhÃ  thá»‘ng kÃª nháº­n ra cÃ¹ng hÃ m nÃ y cÃ³ thá»ƒ **chuyá»ƒn Ä‘á»•i xÃ¡c suáº¥t phi tuyáº¿n**, vÃ  **Logistic Regression** ra Ä‘á»i.

> ğŸ§© NÃ³i cÃ¡ch khÃ¡c: hÃ m Sigmoid sinh ra tá»« tá»± nhiÃªn (mÃ´ hÃ¬nh tÄƒng trÆ°á»Ÿng), rá»“i Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng trong há»c mÃ¡y nhÆ° má»™t hÃ m xÃ¡c suáº¥t há»£p lÃ½.

---

## ğŸ§­ 10. Tá»•ng káº¿t

| BÆ°á»›c | Ã tÆ°á»Ÿng chÃ­nh | Káº¿t quáº£ |
|------|----------------|---------|
| 1ï¸âƒ£ | Muá»‘n mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t trong [0,1] | Linear regression khÃ´ng phÃ¹ há»£p |
| 2ï¸âƒ£ | Dá»± Ä‘oÃ¡n tá»· lá»‡ odds thay vÃ¬ xÃ¡c suáº¥t | odds luÃ´n dÆ°Æ¡ng |
| 3ï¸âƒ£ | Láº¥y log cá»§a odds â†’ logit | Má»Ÿ rá»™ng vá» miá»n sá»‘ thá»±c |
| 4ï¸âƒ£ | Giáº£ Ä‘á»‹nh logit = wáµ€x + b | Táº¡o má»‘i quan há»‡ tuyáº¿n tÃ­nh |
| 5ï¸âƒ£ | Biáº¿n Ä‘á»•i ngÆ°á»£c â†’ Sigmoid | Ãnh xáº¡ vá» xÃ¡c suáº¥t |
| 6ï¸âƒ£ | Tá»‘i Æ°u báº±ng MLE / cross-entropy | Gradient Ä‘áº¹p, dá»… huáº¥n luyá»‡n |
| 7ï¸âƒ£ | á»”n Ä‘á»‹nh sá»‘ & triá»ƒn khai thá»±c táº¿ | DÃ¹ng softplus hoáº·c BCEWithLogitsLoss |
| 8ï¸âƒ£ | Logistic vs Probit | KhÃ¡c á»Ÿ phÃ¢n phá»‘i vÃ  á»©ng dá»¥ng |

---

## ğŸ’» 11. VÃ­ dá»¥ code minh há»a

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def grad(w, x, y):
    z = np.dot(w, x)
    return (sigmoid(z) - y) * x

x = np.array([1.0, 2.0])
w = np.array([0.3, -0.5])
y = 1

sig = sigmoid(np.dot(w, x))
g = grad(w, x, y)

print("Sigmoid output:", sig)
print("Gradient:", g)

Sigmoid output: 0.3775406687981454
Gradient: [-0.62245933 -1.24491866]

```

---

## ğŸ’¬ 12. Káº¿t luáº­n

HÃ m **Sigmoid** khÃ´ng pháº£i lÃ  má»™t â€œmÃ¡nhâ€ cá»§a há»c mÃ¡y, mÃ  lÃ  **há»‡ quáº£ tá»± nhiÃªn cá»§a tÆ° duy thá»‘ng kÃª**:

> Khi ta muá»‘n má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh biá»ƒu diá»…n xÃ¡c suáº¥t, viá»‡c chuyá»ƒn qua *log-odds* rá»“i nghá»‹ch Ä‘áº£o láº¡i táº¥t yáº¿u dáº«n Ä‘áº¿n hÃ m Sigmoid.

VÃ¬ tháº¿, **Logistic Regression** vá»«a Ä‘Æ¡n giáº£n vá»«a sÃ¢u sáº¯c, nÃ³ káº¿t há»£p giá»¯a **toÃ¡n há»c**, **thá»‘ng kÃª**, **xÃ¡c suáº¥t** vÃ  **trá»±c giÃ¡c** má»™t cÃ¡ch hoÃ n háº£o. âœ¨

---

## ğŸ“š 13. TÃ i liá»‡u tham kháº£o

- Hosmer & Lemeshow (2000). *Applied Logistic Regression*. Wiley.  
- Andrew Ng (Coursera). *Machine Learning â€” Week 3: Logistic Regression.*  
- [Wikipedia: Logistic function](https://en.wikipedia.org/wiki/Logistic_function)  
- [StatQuest: Logistic Regression Clearly Explained](https://www.youtube.com/watch?v=yIYKR4sgzI8)  
- [PyTorch: BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)