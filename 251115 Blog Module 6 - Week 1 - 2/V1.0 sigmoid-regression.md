---
title: "HÃ m Sigmoid trong Logistic Regression â€” Tá»« log-odds Ä‘áº¿n xÃ¡c suáº¥t"
description: "Giáº£i thÃ­ch vÃ¬ sao hÃ m Sigmoid xuáº¥t hiá»‡n tá»± nhiÃªn trong Logistic Regression: tá»« odds â†’ logit â†’ giáº£ Ä‘á»‹nh tuyáº¿n tÃ­nh â†’ sigmoid. Bao gá»“m lá»‹ch sá»­, tá»‘i Æ°u hÃ³a, vÃ  vÃ­ dá»¥ minh há»a."
date: 2025-11-13
author: Nguyá»…n Quang Linh
tags: [machine learning, logistic regression, sigmoid, xÃ¡c suáº¥t, thá»‘ng kÃª, cross-entropy, maximum likelihood]
---

# HÃ m Sigmoid trong Logistic Regression â€” Tá»« log-odds Ä‘áº¿n xÃ¡c suáº¥t
# Giá»›i thiá»‡u
Khi má»›i lÃ m quen vá»›i cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n, chÃºng ta thÆ°á»ng há»c qua **Linear Regression** â€“ nÆ¡i má»™t hÃ m tuyáº¿n tÃ­nh sáº½ dá»± Ä‘oÃ¡n Ä‘áº§u ra liÃªn tá»¥c, nhÆ° giÃ¡ nhÃ  hoáº·c Ä‘iá»ƒm sá»‘. **Linear Regression** ráº¥t máº¡nh máº½ vá»›i dá»¯ liá»‡u liÃªn tá»¥c, nhÆ°ng láº¡i khÃ´ng phÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n mÃ  Ä‘áº§u ra chá»‰ nháº­n giÃ¡ trá»‹ 0 hoáº·c 1, nhÆ° dá»± Ä‘oÃ¡n má»™t email cÃ³ pháº£i spam hay khÃ´ng, hay khÃ¡ch hÃ ng cÃ³ rá»i bá» dá»‹ch vá»¥ hay khÃ´ng.

Náº¿u Ã¡p dá»¥ng linear regression cho cÃ¡c bÃ i toÃ¡n nÃ y, giÃ¡ trá»‹ dá»± bÃ¡o cÃ³ thá»ƒ vÆ°á»£t ra ngoÃ i khoáº£ng $[0,1]$ vÃ  gÃ¢y khÃ³ khÄƒn trong viá»‡c diá»…n giáº£i lÃ  xÃ¡c suáº¥t. VÃ¬ tháº¿, chÃºng ta cáº§n má»™t mÃ´ hÃ¬nh giá»¯ láº¡i **tÃ­nh tuyáº¿n tÃ­nh (vÃ¬ dá»… giáº£i thÃ­ch, huáº¥n luyá»‡n)** nhÆ°ng láº¡i Ä‘áº£m báº£o Ä‘áº§u ra lÃ  xÃ¡c suáº¥t há»£p lá»‡ â€“ Ä‘Ã³ chÃ­nh lÃ  **Logistic Regression**.

**Logistic Regression** khÃ¡c biá»‡t chá»§ yáº¿u nhá» hÃ m **sigmoid**. HÃ m nÃ y "Ã©p" Ä‘áº§u ra cá»§a biá»ƒu thá»©c tuyáº¿n tÃ­nh vÃ o Ä‘Ãºng khoáº£ng $[0,1]$, Ä‘á»ƒ cÃ³ thá»ƒ diá»…n giáº£i xÃ¡c suáº¥t má»™t cÃ¡ch há»£p lÃ½. 

BÃ i blog nÃ y sáº½ dáº«n báº¡n Ä‘i tá»« ná»n táº£ng **linear regression** sang **logistic regression**, khÃ¡m phÃ¡ lÃ½ do xÃ¡c suáº¥t Ä‘Æ°á»£c káº¿t ná»‘i cháº·t cháº½ vá»›i hÃ m **sigmoid**, cÃ¡c Ã½ tÆ°á»Ÿng tá»« **odds** tá»›i **logit** rá»“i quy vá» **xÃ¡c suáº¥t**, cÃ¹ng vÃ­ dá»¥, tá»‘i Æ°u hÃ³a, cÃ¡c lÆ°u Ã½ thá»±c táº¿ vÃ  cáº­p nháº­t má»›i vá» regularization vÃ  cÃ¢n báº±ng dá»¯ liá»‡u â€“ giÃºp báº¡n hiá»ƒu toÃ n diá»‡n, tá»« trá»±c giÃ¡c Ä‘áº¿n triá»ƒn khai hiá»‡n Ä‘áº¡i.

<div align="center">
<img src="https://i.ibb.co/d41fHgXL/Logo-999.png" alt="Logo CONQ999" width="250">

<sub>BÃ i viáº¿t Ä‘Æ°á»£c biÃªn soáº¡n bá»Ÿi nhÃ³m CONQ999</sub>
</div>

---

## 1. Báº¯t Ä‘áº§u tá»« mong muá»‘n mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t khi Linear Regression khÃ´ng phÃ¹ há»£p

Giáº£ sá»­ ta Ä‘ang giáº£i quyáº¿t bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n:
- $y = 1$: Ä‘á»‘i tÆ°á»£ng thuá»™c lá»›p 1  
- $y = 0$: Ä‘á»‘i tÆ°á»£ng khÃ´ng thuá»™c lá»›p 1  

Ta muá»‘n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n:

$$
P(y=1|x)
$$

vÃ  giÃ¡ trá»‹ nÃ y **pháº£i náº±m trong [0,1]** Ä‘á»ƒ cÃ³ thá»ƒ hiá»ƒu lÃ  xÃ¡c suáº¥t.  
Tuy nhiÃªn, há»“i quy tuyáº¿n tÃ­nh:

$$
\hat{y} = w^T x + b
$$

**Diá»…n giáº£i cÃ´ng thá»©c:**
- $w^T x$ lÃ  tÃ­ch vÃ´ hÆ°á»›ng giá»¯a vector trá»ng sá»‘ $w$ vÃ  vector Ä‘áº·c trÆ°ng $x$, tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i $\sum_{i=1}^{n} w_i x_i$
- $b$ lÃ  há»‡ sá»‘ bias (Ä‘á»™ lá»‡ch), cho phÃ©p Ä‘Æ°á»ng tháº³ng khÃ´ng báº¯t buá»™c pháº£i Ä‘i qua gá»‘c tá»a Ä‘á»™
- Káº¿t quáº£ $\hat{y}$ cÃ³ thá»ƒ lÃ  báº¥t ká»³ sá»‘ thá»±c nÃ o, khÃ´ng bá»‹ giá»›i háº¡n trong $[0,1]$

cho káº¿t quáº£ cÃ³ thá»ƒ Ã¢m, hoáº·c lá»›n hÆ¡n 1 â€” âŒ khÃ´ng há»£p lÃ½ vá» máº·t xÃ¡c suáº¥t.

<div style="text-align:center;">
  <img src="https://i.ibb.co/7tDYQz2k/section1.png" alt="HÃ¬nh 1: VÃ­ dá»¥ Linear Regression dá»± Ä‘oÃ¡n giÃ¡ trá»‹ vÆ°á»£t ngoÃ i [0,1]" width="720"/>
  <br>
  <sub><b>HÃ¬nh 1:</b> VÃ­ dá»¥ Linear Regression dá»± Ä‘oÃ¡n giÃ¡ trá»‹ vÆ°á»£t ngoÃ i khoáº£ng [0,1]</sub>
</div>

Quan sÃ¡t **HÃ¬nh 1**, ta cÃ³ thá»ƒ tháº¥y ráº±ng Ä‘Æ°á»ng tháº³ng cá»§a **Linear Regression** khÃ´ng bá»‹ giá»›i háº¡n trong khoáº£ng $[0,1]$. Khi giÃ¡ trá»‹ Ä‘áº§u vÃ o $x$ tÄƒng lÃªn, Ä‘Æ°á»ng tháº³ng tiáº¿p tá»¥c vÆ°á»£t qua má»©c 1, vÃ  khi $x$ giáº£m xuá»‘ng, nÃ³ cÃ³ thá»ƒ rÆ¡i xuá»‘ng dÆ°á»›i 0. Äiá»u nÃ y cho tháº¥y rÃµ rÃ ng táº¡i sao **Linear Regression** khÃ´ng phÃ¹ há»£p cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n â€” ta cáº§n má»™t hÃ m cÃ³ thá»ƒ "Ã©p" Ä‘áº§u ra vÃ o khoáº£ng há»£p lá»‡ $[0,1]$ Ä‘á»ƒ diá»…n giáº£i nhÆ° xÃ¡c suáº¥t.

---

## 2. Ã tÆ°á»Ÿng: Dá»± Ä‘oÃ¡n **tá»· lá»‡ xáº£y ra vÃ  khÃ´ng xáº£y ra**

Thay vÃ¬ dá»± Ä‘oÃ¡n trá»±c tiáº¿p $P$, cÃ¡c nhÃ  thá»‘ng kÃª tháº¿ ká»· 19 (Ä‘áº·c biá»‡t **Pierre FranÃ§ois Verhulst**, 1838) Ä‘Ã£ nghÄ© khÃ¡c Ä‘i:  
HÃ£y dá»± Ä‘oÃ¡n **tá»· lá»‡ odds**:

$$
\text{odds} = \frac{P(y=1|x)}{1 - P(y=1|x)}
$$

**Diá»…n giáº£i cÃ´ng thá»©c odds:**
- Tá»­ sá»‘ $P(y=1|x)$ lÃ  xÃ¡c suáº¥t sá»± kiá»‡n xáº£y ra
- Máº«u sá»‘ $1 - P(y=1|x)$ lÃ  xÃ¡c suáº¥t sá»± kiá»‡n khÃ´ng xáº£y ra
- Odds cho biáº¿t "sá»± kiá»‡n xáº£y ra gáº¥p bao nhiÃªu láº§n so vá»›i khÃ´ng xáº£y ra"
- VÃ­ dá»¥: odds = 3 nghÄ©a lÃ  sá»± kiá»‡n xáº£y ra gáº¥p 3 láº§n so vá»›i khÃ´ng xáº£y ra

- Náº¿u $P = 0.5$ â†’ odds = 1 (cÆ¡ há»™i ngang nhau)  
- Náº¿u $P = 0.8$ â†’ odds = 4 (xáº£y ra gáº¥p 4 láº§n khÃ´ng xáº£y ra)  
- Náº¿u $P = 0.2$ â†’ odds = 0.25 (xáº£y ra Ã­t hÆ¡n 4 láº§n)

Tuy nhiÃªn, odds váº«n chá»‰ dÆ°Æ¡ng, chÆ°a thá»ƒ mÃ´ hÃ¬nh hÃ³a báº±ng hÃ m tuyáº¿n tÃ­nh (vÃ¬ tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ã¢m).  
VÃ¬ váº­y, ta **láº¥y log cá»§a odds** Ä‘á»ƒ "tráº£i" nÃ³ ra toÃ n trá»¥c thá»±c:

$$
\text{logit}(P) = \log\left(\frac{P}{1 - P}\right)
$$

**Diá»…n giáº£i hÃ m logit:**
- Láº¥y logarit tá»± nhiÃªn (cÆ¡ sá»‘ $e$) cá»§a odds
- Khi $P \to 0$: $\frac{P}{1-P} \to 0$ â†’ $\log(0) = -\infty$ (logit tiáº¿n vá» $-\infty$)
- Khi $P = 0.5$: $\frac{0.5}{0.5} = 1$ â†’ $\log(1) = 0$ (logit = 0)
- Khi $P \to 1$: $\frac{P}{1-P} \to +\infty$ â†’ $\log(+\infty) = +\infty$ (logit tiáº¿n vá» $+\infty$)
- NhÆ° váº­y, logit cÃ³ thá»ƒ nháº­n má»i giÃ¡ trá»‹ thá»±c tá»« $-\infty$ Ä‘áº¿n $+\infty$

â†’ ÄÃ¢y chÃ­nh lÃ  **hÃ m logit**.  
VÃ  nhá» log, ta cÃ³ thá»ƒ cho giÃ¡ trá»‹ nÃ y báº±ng **má»™t biá»ƒu thá»©c tuyáº¿n tÃ­nh**.

<div style="text-align:center;">
  <img src="https://i.ibb.co/QxRj0SV/section2.png" alt="HÃ¬nh 2: So sÃ¡nh Probability â€“ Odds â€“ Logit trÃªn cÃ¹ng trá»¥c" width="720"/>
  <br>
  <sub><b>HÃ¬nh 2:</b> So sÃ¡nh Probability â€“ Odds â€“ Logit trÃªn cÃ¹ng trá»¥c</sub>
</div>

**HÃ¬nh 2** minh há»áº¡ sá»± chuyá»ƒn Ä‘á»•i tá»« **Probability** (giá»›i háº¡n trong $[0,1]$) sang **Odds** (chá»‰ nháº­n giÃ¡ trá»‹ dÆ°Æ¡ng) vÃ  cuá»‘i cÃ¹ng lÃ  **Logit** (cÃ³ thá»ƒ nháº­n má»i giÃ¡ trá»‹ thá»±c). Ta cÃ³ thá»ƒ tháº¥y ráº±ng logit "tráº£i" miá»n giÃ¡ trá»‹ ra toÃ n bá»™ trá»¥c sá»‘, tá»« $-\infty$ Ä‘áº¿n $+\infty$, Ä‘iá»u nÃ y cho phÃ©p ta mÃ´ hÃ¬nh hÃ³a báº±ng má»™t biá»ƒu thá»©c tuyáº¿n tÃ­nh mÃ  khÃ´ng lo vi pháº¡m rÃ ng buá»™c vá» miá»n giÃ¡ trá»‹.

---

## 3. Giáº£ Ä‘á»‹nh quan há»‡ tuyáº¿n tÃ­nh trong miá»n logit

Ta Ä‘áº·t giáº£ Ä‘á»‹nh:

$$
\text{logit}(P) = w^T x + b
$$

hay tÆ°Æ¡ng Ä‘Æ°Æ¡ng:

$$
\log\left(\frac{P}{1 - P}\right) = w^T x + b
$$

**Diá»…n giáº£i giáº£ Ä‘á»‹nh:**
- Ta giáº£ Ä‘á»‹nh ráº±ng logit (log-odds) cÃ³ quan há»‡ **tuyáº¿n tÃ­nh** vá»›i cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o $x$
- $w^T x = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n$ lÃ  tá»•ng cÃ³ trá»ng sá»‘ cá»§a cÃ¡c Ä‘áº·c trÆ°ng
- Má»—i Ä‘áº·c trÆ°ng $x_i$ Ä‘Ã³ng gÃ³p vÃ o logit má»™t lÆ°á»£ng $w_i x_i$
- $b$ lÃ  há»‡ sá»‘ bias, cho phÃ©p Ä‘iá»u chá»‰nh logit khi táº¥t cáº£ Ä‘áº·c trÆ°ng báº±ng 0
- VÃ­ dá»¥: náº¿u $w_1 = 2$ vÃ  $x_1$ tÄƒng 1 Ä‘Æ¡n vá»‹, thÃ¬ logit tÄƒng 2 Ä‘Æ¡n vá»‹

ÄÃ¢y lÃ  bÆ°á»›c "thiáº¿t káº¿" cá»‘t lÃµi â€” **má»™t giáº£ Ä‘á»‹nh thá»‘ng kÃª há»£p lÃ½**:
- VÃ¬ Ä‘áº§u vÃ o $x$ áº£nh hÆ°á»Ÿng tuyáº¿n tÃ­nh lÃªn "Ä‘á»™ tin cáº­y" (log-odds)
- MÃ  logit cho phÃ©p ta lÃ m viá»‡c trÃªn toÃ n miá»n sá»‘ thá»±c

Giá» ta chá»‰ cáº§n biáº¿n Ä‘á»•i ngÆ°á»£c Ä‘á»ƒ tÃ¬m $P$.

<div style="text-align:center;">
  <img src="https://i.ibb.co/k2xkpht7/section3.png" alt="HÃ¬nh 3: Log-odds tÄƒng tuyáº¿n tÃ­nh khi Ä‘áº·c trÆ°ng tÄƒng" width="720"/>
  <br>
  <sub><b>HÃ¬nh 3:</b> Log-odds tÄƒng tuyáº¿n tÃ­nh khi Ä‘áº·c trÆ°ng tÄƒng</sub>
</div>

Trong **HÃ¬nh 3**, ta quan sÃ¡t tháº¥y má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o vÃ  log-odds. ÄÆ°á»ng tháº³ng nÃ y thá»ƒ hiá»‡n giáº£ Ä‘á»‹nh cá»‘t lÃµi cá»§a **Logistic Regression**: má»—i Ä‘Æ¡n vá»‹ tÄƒng cá»§a Ä‘áº·c trÆ°ng sáº½ lÃ m tÄƒng log-odds má»™t lÆ°á»£ng cá»‘ Ä‘á»‹nh, tÆ°Æ¡ng á»©ng vá»›i há»‡ sá»‘ $w$ trong mÃ´ hÃ¬nh. ÄÃ¢y lÃ  lÃ½ do táº¡i sao ta cÃ³ thá»ƒ dÃ¹ng má»™t biá»ƒu thá»©c tuyáº¿n tÃ­nh $w^T x + b$ Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a logit.

---

## 4. Biáº¿n Ä‘á»•i ngÆ°á»£c â†’ HÃ m Sigmoid ra Ä‘á»i

Báº¯t Ä‘áº§u tá»«:

$$
\frac{P}{1 - P} = e^{w^T x + b}
$$

**BÆ°á»›c 1:** Láº¥y mÅ© cáº£ hai váº¿ cá»§a phÆ°Æ¡ng trÃ¬nh logit Ä‘á»ƒ loáº¡i bá» logarit:
- Tá»« $\log\left(\frac{P}{1-P}\right) = w^T x + b$
- Ta cÃ³: $\frac{P}{1-P} = e^{w^T x + b}$ (vÃ¬ $e^{\log(a)} = a$)

**BÆ°á»›c 2:** Giáº£i phÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ tÃ¬m $P$:

$$
\begin{align}
\frac{P}{1 - P} &= e^{w^T x + b} \\
P &= e^{w^T x + b}(1 - P) \\
P &= e^{w^T x + b} - P \cdot e^{w^T x + b} \\
P + P \cdot e^{w^T x + b} &= e^{w^T x + b} \\
P(1 + e^{w^T x + b}) &= e^{w^T x + b} \\
P &= \frac{e^{w^T x + b}}{1 + e^{w^T x + b}}
\end{align}
$$

**BÆ°á»›c 3:** Chia cáº£ tá»­ vÃ  máº«u cho $e^{w^T x + b}$ Ä‘á»ƒ cÃ³ dáº¡ng Ä‘á»‘i xá»©ng:

$$
P = \frac{e^{w^T x + b}}{1 + e^{w^T x + b}} = \frac{1}{1 + e^{-(w^T x + b)}}
$$

â†’ ÄÃ¢y chÃ­nh lÃ  **hÃ m Sigmoid** mÃ  ta dÃ¹ng ngÃ y nay:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

**Diá»…n giáº£i:** Vá»›i $z = w^T x + b$, hÃ m sigmoid chuyá»ƒn Ä‘á»•i logit (cÃ³ thá»ƒ lÃ  báº¥t ká»³ sá»‘ thá»±c nÃ o) thÃ nh xÃ¡c suáº¥t (luÃ´n náº±m trong $[0,1]$).

<div style="text-align:center;">
  <img src="https://i.ibb.co/xt6yxhvh/Figure-1.png" alt="HÃ¬nh 4: ÄÆ°á»ng cong Sigmoid" width="720"/>
  <br>
  <sub><b>HÃ¬nh 4:</b> ÄÆ°á»ng cong Sigmoid - HÃ m chuyá»ƒn Ä‘á»•i log-odds vá» xÃ¡c suáº¥t</sub>
</div>

**HÃ¬nh 4** cho ta tháº¥y hÃ¬nh dáº¡ng Ä‘áº·c trÆ°ng cá»§a hÃ m **Sigmoid** â€” má»™t Ä‘Æ°á»ng cong hÃ¬nh chá»¯ S mÆ°á»£t mÃ . ÄÆ°á»ng cong nÃ y báº¯t Ä‘áº§u tá»« gáº§n 0 khi $z$ ráº¥t Ã¢m, tÄƒng dáº§n á»Ÿ giá»¯a (quanh $z = 0$ nÆ¡i giÃ¡ trá»‹ xÃ¡c suáº¥t lÃ  0.5), vÃ  tiá»‡m cáº­n vá» 1 khi $z$ ráº¥t dÆ°Æ¡ng. Äiá»u quan trá»ng lÃ  hÃ m nÃ y luÃ´n giá»¯ giÃ¡ trá»‹ trong khoáº£ng $[0,1]$, Ä‘áº£m báº£o Ä‘áº§u ra luÃ´n lÃ  xÃ¡c suáº¥t há»£p lá»‡, Ä‘á»“ng thá»i váº«n giá»¯ Ä‘Æ°á»£c má»‘i quan há»‡ tuyáº¿n tÃ­nh á»Ÿ miá»n logit.

---

## 5. HÃ m Sigmoid vÃ  má»‘i quan há»‡ vá»›i Logit

- Sigmoid lÃ  **nghá»‹ch Ä‘áº£o cá»§a hÃ m logit**:  
  $$
  \sigma(z) = \frac{1}{1 + e^{-z}} \iff \text{logit}(P) = z
  $$
- Khi $z = 0$ â†’ $P = 0.5$  
- Khi $z \to +\infty$ â†’ $P \to 1$  
- Khi $z \to -\infty$ â†’ $P \to 0$

HÃ m nÃ y **Ä‘Æ¡n Ä‘iá»‡u tÄƒng**, cÃ³ Ä‘áº¡o hÃ m gá»n Ä‘áº¹p:
$$
\sigma'(z) = \sigma(z)(1 - \sigma(z))
$$

**CÃ¡ch tÃ­nh Ä‘áº¡o hÃ m cá»§a sigmoid:**
- Báº¯t Ä‘áº§u tá»« $\sigma(z) = \frac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}$
- Ãp dá»¥ng quy táº¯c chuá»—i: $\sigma'(z) = -(1 + e^{-z})^{-2} \cdot (-e^{-z}) = \frac{e^{-z}}{(1 + e^{-z})^2}$
- NhÃ¢n vÃ  chia cho $e^z$: $\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{(1 + e^{-z})(1 + e^{z})}$
- Viáº¿t láº¡i: $\sigma'(z) = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} = \sigma(z) \cdot \frac{e^{-z}}{1 + e^{-z}}$
- Nháº­n tháº¥y: $\frac{e^{-z}}{1 + e^{-z}} = 1 - \frac{1}{1 + e^{-z}} = 1 - \sigma(z)$
- Váº­y: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$

**Ã nghÄ©a cá»§a Ä‘áº¡o hÃ m:**
- Äáº¡o hÃ m Ä‘áº¡t giÃ¡ trá»‹ lá»›n nháº¥t (0.25) khi $\sigma(z) = 0.5$ (tá»©c $z = 0$)
- Äáº¡o hÃ m tiáº¿n vá» 0 khi $\sigma(z) \to 0$ hoáº·c $\sigma(z) \to 1$
- Äiá»u nÃ y cÃ³ nghÄ©a: mÃ´ hÃ¬nh há»c nhanh nháº¥t khi cÃ²n chÆ°a cháº¯c cháº¯n (xÃ¡c suáº¥t gáº§n 0.5), vÃ  há»c cháº­m láº¡i khi Ä‘Ã£ cháº¯c cháº¯n

â†’ thuáº­n lá»£i cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng **Gradient Descent**.

<div style="text-align:center;">
  <img src="https://i.ibb.co/r2w9wC30/section5.png" alt="HÃ¬nh 5: Sigmoid + Ä‘áº¡o hÃ m cá»§a sigmoid" width="720"/>
  <br>
  <sub><b>HÃ¬nh 5:</b> HÃ m Sigmoid vÃ  Ä‘áº¡o hÃ m cá»§a nÃ³ - Thuáº­n lá»£i cho Gradient Descent</sub>
</div>

**HÃ¬nh 5** so sÃ¡nh hÃ m **Sigmoid** vÃ  Ä‘áº¡o hÃ m cá»§a nÃ³. Ta cÃ³ thá»ƒ tháº¥y Ä‘áº¡o hÃ m Ä‘áº¡t giÃ¡ trá»‹ lá»›n nháº¥t táº¡i $z = 0$ (nÆ¡i xÃ¡c suáº¥t lÃ  0.5) vÃ  giáº£m dáº§n vá» 0 khi $z$ tiáº¿n vá» hai phÃ­a cá»±c trá»‹. Äáº·c tÃ­nh nÃ y ráº¥t quan trá»ng cho viá»‡c huáº¥n luyá»‡n: khi mÃ´ hÃ¬nh cÃ²n chÆ°a cháº¯c cháº¯n (xÃ¡c suáº¥t gáº§n 0.5), gradient lá»›n giÃºp há»c nhanh; khi Ä‘Ã£ cháº¯c cháº¯n (xÃ¡c suáº¥t gáº§n 0 hoáº·c 1), gradient nhá» giÃºp trÃ¡nh cáº­p nháº­t quÃ¡ má»©c. ÄÃ¢y lÃ  lÃ½ do táº¡i sao **Sigmoid** Ä‘Æ°á»£c Æ°a chuá»™ng trong **Gradient Descent**.

---

## 6. Tá»‘i Æ°u hÃ³a báº±ng Maximum Likelihood vÃ  Cross-Entropy

Sau khi cÃ³ mÃ´ hÃ¬nh $P(y|x; w, b)$, ta cáº§n tÃ¬m tham sá»‘ $w, b$.  **Logistic Regression** há»c tham sá»‘ báº±ng cÃ¡ch **tá»‘i Ä‘a hÃ³a kháº£ nÄƒng dá»¯ liá»‡u xuáº¥t hiá»‡n** â€” Maximum Likelihood Estimation (MLE):

$$
L(w,b) = \prod_i P(y_i|x_i; w,b)
$$

**Diá»…n giáº£i likelihood:**
- Likelihood $L(w,b)$ lÃ  xÃ¡c suáº¥t quan sÃ¡t Ä‘Æ°á»£c toÃ n bá»™ dá»¯ liá»‡u vá»›i tham sá»‘ $(w,b)$
- Vá»›i giáº£ Ä‘á»‹nh cÃ¡c máº«u Ä‘á»™c láº­p, likelihood lÃ  tÃ­ch cá»§a xÃ¡c suáº¥t tá»«ng máº«u
- Ta muá»‘n tÃ¬m $(w,b)$ sao cho likelihood lá»›n nháº¥t (dá»¯ liá»‡u cÃ³ kháº£ nÄƒng xuáº¥t hiá»‡n cao nháº¥t)

Láº¥y log cho dá»… tá»‘i Æ°u (vÃ¬ log lÃ  hÃ m Ä‘Æ¡n Ä‘iá»‡u tÄƒng, tá»‘i Ä‘a hÃ³a log-likelihood tÆ°Æ¡ng Ä‘Æ°Æ¡ng tá»‘i Ä‘a hÃ³a likelihood):

$$
\ell(w,b) = \sum_i \left[y_i \log \sigma(z_i) + (1 - y_i)\log(1 - \sigma(z_i))\right]
$$

**Diá»…n giáº£i log-likelihood:**
- Vá»›i má»—i máº«u $(x_i, y_i)$:
  - Náº¿u $y_i = 1$: ta muá»‘n $\sigma(z_i)$ lá»›n â†’ háº¡ng má»¥c $y_i \log \sigma(z_i)$ lá»›n khi $\sigma(z_i)$ gáº§n 1
  - Náº¿u $y_i = 0$: ta muá»‘n $\sigma(z_i)$ nhá» â†’ háº¡ng má»¥c $(1-y_i)\log(1-\sigma(z_i))$ lá»›n khi $\sigma(z_i)$ gáº§n 0
- Tá»•ng cá»§a táº¥t cáº£ cÃ¡c háº¡ng má»¥c nÃ y lÃ  log-likelihood
- CÃ´ng thá»©c nÃ y Ä‘áº£m báº£o: dá»± Ä‘oÃ¡n Ä‘Ãºng Ä‘Æ°á»£c "thÆ°á»Ÿng" (log-likelihood tÄƒng), dá»± Ä‘oÃ¡n sai bá»‹ "pháº¡t" (log-likelihood giáº£m)

VÃ  ta **tá»‘i Ä‘a hÃ³a log-likelihood**, tÆ°Æ¡ng Ä‘Æ°Æ¡ng **tá»‘i thiá»ƒu hÃ³a hÃ m máº¥t mÃ¡t cross-entropy**:

$$
\mathcal{L} = -\ell(w,b)
$$

**Diá»…n giáº£i cross-entropy loss:**
- Cross-entropy loss lÃ  Ã¢m cá»§a log-likelihood
- Tá»‘i Ä‘a hÃ³a log-likelihood = Tá»‘i thiá»ƒu hÃ³a cross-entropy loss
- Loss cÃ ng nhá» â†’ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cÃ ng chÃ­nh xÃ¡c

Gradient:

$$
\nabla_w \mathcal{L} = (\sigma(z) - y)x
$$

**CÃ¡ch tÃ­nh gradient:**
- Ãp dá»¥ng quy táº¯c chuá»—i: $\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial \mathcal{L}}{\partial \sigma(z)} \cdot \frac{\partial \sigma(z)}{\partial z} \cdot \frac{\partial z}{\partial w}$
- Vá»›i $\mathcal{L} = -[y\log\sigma(z) + (1-y)\log(1-\sigma(z))]$:
  - $\frac{\partial \mathcal{L}}{\partial \sigma(z)} = -\frac{y}{\sigma(z)} + \frac{1-y}{1-\sigma(z)} = \frac{\sigma(z) - y}{\sigma(z)(1-\sigma(z))}$
  - $\frac{\partial \sigma(z)}{\partial z} = \sigma(z)(1-\sigma(z))$ (Ä‘áº¡o hÃ m sigmoid)
  - $\frac{\partial z}{\partial w} = x$ (vÃ¬ $z = w^T x + b$)
- Káº¿t há»£p: $\nabla_w \mathcal{L} = \frac{\sigma(z) - y}{\sigma(z)(1-\sigma(z))} \cdot \sigma(z)(1-\sigma(z)) \cdot x = (\sigma(z) - y)x$

**Ã nghÄ©a gradient:**
- Gradient tá»· lá»‡ vá»›i "lá»—i" $(\sigma(z) - y)$: náº¿u dá»± Ä‘oÃ¡n cao hÆ¡n nhÃ£n thá»±c, gradient dÆ°Æ¡ng (giáº£m $w$); náº¿u tháº¥p hÆ¡n, gradient Ã¢m (tÄƒng $w$)
- Gradient cÅ©ng tá»· lá»‡ vá»›i Ä‘áº·c trÆ°ng $x$: Ä‘áº·c trÆ°ng cÃ³ giÃ¡ trá»‹ lá»›n sáº½ áº£nh hÆ°á»Ÿng nhiá»u hÆ¡n Ä‘áº¿n cáº­p nháº­t

â†’ ÄÃ¢y lÃ  lÃ½ do hÃ m Sigmoid Ä‘Æ°á»£c Æ°a chuá»™ng: Ä‘áº¡o hÃ m cá»§a nÃ³ khiáº¿n cáº­p nháº­t gradient cá»±c ká»³ Ä‘Æ¡n giáº£n vÃ  á»•n Ä‘á»‹nh.

<div style="text-align:center;">
  <img src="https://i.ibb.co/Xx657Ytt/section6.png" alt="HÃ¬nh 6: HÃ¬nh dáº¡ng hÃ m loss cross-entropy cho y=1 vÃ  y=0" width="720"/>
  <br>
  <sub><b>HÃ¬nh 6:</b> HÃ¬nh dáº¡ng hÃ m loss cross-entropy cho y=1 vÃ  y=0</sub>
</div>

Quan sÃ¡t **HÃ¬nh 6**, ta tháº¥y hÃ¬nh dáº¡ng cá»§a hÃ m **cross-entropy loss** cho hai trÆ°á»ng há»£p: khi nhÃ£n thá»±c lÃ  $y = 1$ vÃ  khi $y = 0$. Vá»›i $y = 1$, loss tÄƒng máº¡nh khi dá»± Ä‘oÃ¡n gáº§n 0 (dá»± Ä‘oÃ¡n sai), vÃ  giáº£m vá» 0 khi dá»± Ä‘oÃ¡n gáº§n 1 (dá»± Ä‘oÃ¡n Ä‘Ãºng). NgÆ°á»£c láº¡i, vá»›i $y = 0$, loss tÄƒng khi dá»± Ä‘oÃ¡n gáº§n 1 vÃ  giáº£m khi dá»± Ä‘oÃ¡n gáº§n 0. HÃ¬nh dáº¡ng nÃ y Ä‘áº£m báº£o mÃ´ hÃ¬nh bá»‹ "pháº¡t" nhiá»u hÆ¡n khi dá»± Ä‘oÃ¡n sai, vÃ  Ä‘Æ°á»£c "thÆ°á»Ÿng" khi dá»± Ä‘oÃ¡n Ä‘Ãºng, táº¡o Ä‘á»™ng lá»±c cho quÃ¡ trÃ¬nh há»c.

---

## 7. á»”n Ä‘á»‹nh sá»‘ khi tÃ­nh Sigmoid

Trong thá»±c táº¿, vá»›i $z$ ráº¥t lá»›n hoáº·c ráº¥t nhá», $e^{-z}$ cÃ³ thá»ƒ **trÃ n sá»‘ (overflow/underflow)**.  
Má»™t sá»‘ cÃ¡ch kháº¯c phá»¥c:
- Giá»›i háº¡n $z$ trong khoáº£ng $[-20, 20]$ trÆ°á»›c khi tÃ­nh  
- DÃ¹ng biá»ƒu thá»©c á»•n Ä‘á»‹nh hÆ¡n trong log-loss:  
  $$
  \log(1 + e^{-z}) \approx \text{softplus}(z)
  $$
  
  **Diá»…n giáº£i softplus:**
  - HÃ m softplus: $\text{softplus}(z) = \log(1 + e^z)$
  - Khi $z$ ráº¥t lá»›n: $e^z$ ráº¥t lá»›n â†’ $\log(1 + e^z) \approx \log(e^z) = z$
  - Khi $z$ ráº¥t nhá» (Ã¢m): $e^z \approx 0$ â†’ $\log(1 + e^z) \approx \log(1) = 0$
  - Softplus lÃ  hÃ m mÆ°á»£t mÃ , trÃ¡nh Ä‘Æ°á»£c váº¥n Ä‘á» trÃ n sá»‘ khi tÃ­nh $e^{-z}$ vá»›i $z$ ráº¥t lá»›n
  - Trong tÃ­nh toÃ¡n log-loss, ta cÃ³ thá»ƒ dÃ¹ng: $\log(1 + e^{-z}) = \text{softplus}(-z)$ Ä‘á»ƒ trÃ¡nh overflow

- Trong framework há»c mÃ¡y, ngÆ°á»i ta thÆ°á»ng dÃ¹ng hÃ m `torch.nn.BCEWithLogitsLoss` (PyTorch) hoáº·c `tf.nn.sigmoid_cross_entropy_with_logits` (TensorFlow), vá»‘n Ä‘Ã£ xá»­ lÃ½ á»•n Ä‘á»‹nh sá»‘ bÃªn trong.
  
  **LÆ°u Ã½:** CÃ¡c hÃ m nÃ y nháº­n Ä‘áº§u vÃ o lÃ  **logits** (chÆ°a qua sigmoid) thay vÃ¬ xÃ¡c suáº¥t, vÃ  tá»± Ä‘á»™ng Ã¡p dá»¥ng sigmoid má»™t cÃ¡ch á»•n Ä‘á»‹nh sá»‘ há»c bÃªn trong, trÃ¡nh Ä‘Æ°á»£c váº¥n Ä‘á» trÃ n sá»‘.

---

## 8. So sÃ¡nh Logistic vÃ  Probit

Má»™t mÃ´ hÃ¬nh gáº§n giá»‘ng Logistic Regression lÃ  **Probit Regression**, trong Ä‘Ã³:
$$
P(y=1|x) = \Phi(w^T x + b)
$$
vá»›i $\Phi$ lÃ  hÃ m tÃ­ch lÅ©y cá»§a phÃ¢n phá»‘i chuáº©n.  
Hai mÃ´ hÃ¬nh nÃ y cho káº¿t quáº£ tÆ°Æ¡ng tá»±, chá»‰ khÃ¡c vá» **Ä‘uÃ´i phÃ¢n phá»‘i** vÃ  **diá»…n giáº£i thá»‘ng kÃª** â€” Logistic cÃ³ Ä‘uÃ´i dÃ i hÆ¡n, nÃªn thÆ°á»ng dÃ¹ng phá»• biáº¿n hÆ¡n trong há»c mÃ¡y.

---

## 9. Lá»‹ch sá»­ ra Ä‘á»i cá»§a hÃ m Sigmoid

- NÄƒm **1838**, **Pierre FranÃ§ois Verhulst** (nhÃ  toÃ¡n há»c Bá»‰) giá»›i thiá»‡u hÃ m logistic Ä‘á»ƒ mÃ´ táº£ **tÄƒng trÆ°á»Ÿng dÃ¢n sá»‘**:
  $$
  N(t) = \frac{K}{1 + e^{-r(t - t_0)}}
  $$
  
  **Diá»…n giáº£i mÃ´ hÃ¬nh tÄƒng trÆ°á»Ÿng dÃ¢n sá»‘:**
  - $N(t)$ lÃ  dÃ¢n sá»‘ táº¡i thá»i Ä‘iá»ƒm $t$
  - $K$ lÃ  giá»›i háº¡n dÃ¢n sá»‘ tá»‘i Ä‘a (carrying capacity) â€” dÃ¢n sá»‘ khÃ´ng thá»ƒ vÆ°á»£t quÃ¡ giÃ¡ trá»‹ nÃ y do giá»›i háº¡n tÃ i nguyÃªn
  - $r$ lÃ  tá»‘c Ä‘á»™ tÄƒng trÆ°á»Ÿng â€” há»‡ sá»‘ Ä‘iá»u chá»‰nh Ä‘á»™ dá»‘c cá»§a Ä‘Æ°á»ng cong
  - $t_0$ lÃ  thá»i Ä‘iá»ƒm tÄƒng trÆ°á»Ÿng nhanh nháº¥t â€” khi dÃ¢n sá»‘ Ä‘áº¡t $K/2$
  
  **Má»‘i liÃªn há»‡ vá»›i hÃ m sigmoid:**
  - Náº¿u Ä‘áº·t $z = r(t - t_0)$, ta cÃ³: $N(t) = K \cdot \frac{1}{1 + e^{-z}} = K \cdot \sigma(z)$
  - HÃ m sigmoid $\sigma(z)$ mÃ´ táº£ tá»· lá»‡ dÃ¢n sá»‘ so vá»›i giá»›i háº¡n $K$
  - Khi $t \to -\infty$: $N(t) \to 0$ (dÃ¢n sá»‘ ban Ä‘áº§u ráº¥t nhá»)
  - Khi $t = t_0$: $N(t_0) = K/2$ (dÃ¢n sá»‘ Ä‘áº¡t má»™t ná»­a giá»›i háº¡n, tÄƒng trÆ°á»Ÿng nhanh nháº¥t)
  - Khi $t \to +\infty$: $N(t) \to K$ (dÃ¢n sá»‘ tiá»‡m cáº­n giá»›i háº¡n)
  
  ÄÃ¢y lÃ  má»™t **Ä‘Æ°á»ng cong S (sigmoid)** thá»ƒ hiá»‡n sá»± tÄƒng dáº§n cháº­m dáº§n khi tiáº¿n Ä‘áº¿n giá»›i háº¡n $K$.
  
<div style="text-align:center;">
  <img src="https://i.ibb.co/4wjVY0py/section7.png" alt="HÃ¬nh 7: ÄÆ°á»ng cong dÃ¢n sá»‘ logistic theo thá»i gian" width="720"/>
  <br>
  <sub><b>HÃ¬nh 7:</b> ÄÆ°á»ng cong dÃ¢n sá»‘ logistic theo thá»i gian - Nguá»“n gá»‘c cá»§a hÃ m Sigmoid</sub>
</div>

**HÃ¬nh 7** minh há»áº¡ mÃ´ hÃ¬nh tÄƒng trÆ°á»Ÿng dÃ¢n sá»‘ ban Ä‘áº§u cá»§a Verhulst, nÆ¡i hÃ m **Sigmoid** Ä‘Æ°á»£c sinh ra. Ta cÃ³ thá»ƒ tháº¥y Ä‘Æ°á»ng cong báº¯t Ä‘áº§u tÄƒng cháº­m á»Ÿ giai Ä‘oáº¡n Ä‘áº§u, sau Ä‘Ã³ tÄƒng nhanh nháº¥t á»Ÿ giá»¯a (quanh thá»i Ä‘iá»ƒm $t_0$ khi dÃ¢n sá»‘ Ä‘áº¡t má»™t ná»­a giá»›i háº¡n $K$), vÃ  cuá»‘i cÃ¹ng cháº­m dáº§n khi tiáº¿n Ä‘áº¿n giá»›i háº¡n $K$ (carrying capacity). ÄÆ°á»ng cong nÃ y cÃ³ hÃ¬nh chá»¯ S Ä‘áº·c trÆ°ng, tÆ°Æ¡ng tá»± nhÆ° hÃ m **Sigmoid** ta dÃ¹ng trong **Logistic Regression**. Äiá»u nÃ y cho tháº¥y hÃ m **Sigmoid** khÃ´ng pháº£i lÃ  má»™t cÃ´ng cá»¥ tÃ¹y tiá»‡n, mÃ  xuáº¥t phÃ¡t tá»« cÃ¡c quy luáº­t tá»± nhiÃªn vá» tÄƒng trÆ°á»Ÿng cÃ³ giá»›i háº¡n, sau Ä‘Ã³ Ä‘Æ°á»£c cÃ¡c nhÃ  thá»‘ng kÃª tÃ¡i sá»­ dá»¥ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t.


- Tháº¿ ká»· 20, cÃ¡c nhÃ  thá»‘ng kÃª nháº­n ra cÃ¹ng hÃ m nÃ y cÃ³ thá»ƒ **chuyá»ƒn Ä‘á»•i xÃ¡c suáº¥t phi tuyáº¿n**, vÃ  **Logistic Regression** ra Ä‘á»i.

> ğŸ§© NÃ³i cÃ¡ch khÃ¡c: hÃ m Sigmoid sinh ra tá»« tá»± nhiÃªn (mÃ´ hÃ¬nh tÄƒng trÆ°á»Ÿng), rá»“i Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng trong há»c mÃ¡y nhÆ° má»™t hÃ m xÃ¡c suáº¥t há»£p lÃ½.

---

## 10. Tá»•ng káº¿t

| BÆ°á»›c | Ã tÆ°á»Ÿng chÃ­nh | Káº¿t quáº£ |
|------|----------------|---------|
| 1ï¸âƒ£ | Muá»‘n mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t trong [0,1] | Linear regression khÃ´ng phÃ¹ há»£p |
| 2ï¸âƒ£ | Dá»± Ä‘oÃ¡n tá»· lá»‡ odds thay vÃ¬ xÃ¡c suáº¥t | odds luÃ´n dÆ°Æ¡ng |
| 3ï¸âƒ£ | Láº¥y log cá»§a odds â†’ logit | Má»Ÿ rá»™ng vá» miá»n sá»‘ thá»±c |
| 4ï¸âƒ£ | Giáº£ Ä‘á»‹nh logit = wáµ€x + b | Táº¡o má»‘i quan há»‡ tuyáº¿n tÃ­nh |
| 5ï¸âƒ£ | Biáº¿n Ä‘á»•i ngÆ°á»£c â†’ Sigmoid | Ãnh xáº¡ vá» xÃ¡c suáº¥t |
| 6ï¸âƒ£ | Tá»‘i Æ°u báº±ng MLE / cross-entropy | Gradient Ä‘áº¹p, dá»… huáº¥n luyá»‡n |
| 7ï¸âƒ£ | á»”n Ä‘á»‹nh sá»‘ & triá»ƒn khai thá»±c táº¿ | DÃ¹ng softplus hoáº·c BCEWithLogitsLoss |
| 8ï¸âƒ£ | Logistic vs Probit | KhÃ¡c á»Ÿ phÃ¢n phá»‘i vÃ  á»©ng dá»¥ng |

---

## 11. VÃ­ dá»¥ code minh há»a

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def grad(w, x, y):
    z = np.dot(w, x)
    return (sigmoid(z) - y) * x

x = np.array([1.0, 2.0])
w = np.array([0.3, -0.5])
y = 1

sig = sigmoid(np.dot(w, x))
g = grad(w, x, y)

print("Sigmoid output:", sig)
print("Gradient:", g)

Sigmoid output: 0.3775406687981454
Gradient: [-0.62245933 -1.24491866]

```

**Diá»…n giáº£i chi tiáº¿t Ä‘oáº¡n code:**

1. **HÃ m `sigmoid(z)`:**
   - Thá»±c hiá»‡n cÃ´ng thá»©c $\sigma(z) = \frac{1}{1 + e^{-z}}$ Ä‘Ã£ trÃ¬nh bÃ y á»Ÿ pháº§n 4
   - Nháº­n Ä‘áº§u vÃ o $z$ (cÃ³ thá»ƒ lÃ  má»™t sá»‘ thá»±c báº¥t ká»³) vÃ  tráº£ vá» giÃ¡ trá»‹ trong khoáº£ng $(0, 1)$
   - `np.exp(-z)` tÃ­nh $e^{-z}$, sau Ä‘Ã³ chia 1 cho $(1 + e^{-z})$ Ä‘á»ƒ Ä‘Æ°á»£c xÃ¡c suáº¥t

2. **HÃ m `grad(w, x, y)`:**
   - TÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t cross-entropy theo cÃ´ng thá»©c Ä‘Ã£ trÃ¬nh bÃ y á»Ÿ pháº§n 6: $\nabla_w \mathcal{L} = (\sigma(z) - y)x$
   - BÆ°á»›c 1: TÃ­nh $z = w^T x = \text{np.dot}(w, x)$ â€” Ä‘Ã¢y lÃ  logit (log-odds)
   - BÆ°á»›c 2: TÃ­nh $\sigma(z)$ â€” chuyá»ƒn logit thÃ nh xÃ¡c suáº¥t
   - BÆ°á»›c 3: TÃ­nh $(\sigma(z) - y)$ â€” Ä‘Ã¢y lÃ  "lá»—i" giá»¯a dá»± Ä‘oÃ¡n vÃ  nhÃ£n thá»±c
   - BÆ°á»›c 4: NhÃ¢n vá»›i $x$ Ä‘á»ƒ Ä‘Æ°á»£c gradient â€” gradient tá»· lá»‡ vá»›i Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o

3. **CÃ¡c biáº¿n Ä‘áº§u vÃ o:**
   - `x = [1.0, 2.0]`: Vector Ä‘áº·c trÆ°ng cá»§a má»™t máº«u dá»¯ liá»‡u (vÃ­ dá»¥: tuá»•i=1, thu nháº­p=2)
   - `w = [0.3, -0.5]`: Vector trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh (há»‡ sá»‘ há»“i quy)
   - `y = 1`: NhÃ£n thá»±c cá»§a máº«u (1 = thuá»™c lá»›p dÆ°Æ¡ng tÃ­nh)

4. **TÃ­nh toÃ¡n cá»¥ thá»ƒ:**
   - $z = w^T x = 0.3 \times 1.0 + (-0.5) \times 2.0 = 0.3 - 1.0 = -0.7$
   - $\sigma(z) = \sigma(-0.7) \approx 0.3775$ â€” xÃ¡c suáº¥t dá»± Ä‘oÃ¡n lÃ  khoáº£ng 37.75%
   - VÃ¬ $y = 1$ (nhÃ£n thá»±c lÃ  dÆ°Æ¡ng tÃ­nh) nhÆ°ng dá»± Ä‘oÃ¡n chá»‰ 37.75%, nÃªn mÃ´ hÃ¬nh cáº§n Ä‘iá»u chá»‰nh Ä‘á»ƒ tÄƒng xÃ¡c suáº¥t

5. **Gradient `[-0.6225, -1.2449]`:**
   - Gradient Ã¢m cho cáº£ hai Ä‘áº·c trÆ°ng, nghÄ©a lÃ  Ä‘á»ƒ giáº£m loss, ta cáº§n **tÄƒng** giÃ¡ trá»‹ cá»§a $w$ (vÃ¬ gradient descent sáº½ cáº­p nháº­t $w \leftarrow w - \alpha \nabla_w \mathcal{L}$)
   - Äá»™ lá»›n cá»§a gradient tá»· lá»‡ vá»›i giÃ¡ trá»‹ Ä‘áº·c trÆ°ng: Ä‘áº·c trÆ°ng thá»© hai ($x_2 = 2.0$) cÃ³ gradient lá»›n gáº¥p Ä‘Ã´i Ä‘áº·c trÆ°ng thá»© nháº¥t ($x_1 = 1.0$)
   - Äiá»u nÃ y há»£p lÃ½ vÃ¬ Ä‘áº·c trÆ°ng cÃ³ giÃ¡ trá»‹ lá»›n hÆ¡n sáº½ áº£nh hÆ°á»Ÿng nhiá»u hÆ¡n Ä‘áº¿n dá»± Ä‘oÃ¡n, nÃªn cáº§n Ä‘iá»u chá»‰nh máº¡nh hÆ¡n

6. **Ã nghÄ©a thá»±c táº¿:**
   - Káº¿t quáº£ cho tháº¥y mÃ´ hÃ¬nh Ä‘ang **thiáº¿u tá»± tin** (xÃ¡c suáº¥t 37.75% < 50%) khi nhÃ£n thá»±c lÃ  1
   - Gradient Ã¢m chá»‰ ra ráº±ng ta cáº§n tÄƒng cÃ¡c trá»ng sá»‘ Ä‘á»ƒ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cao hÆ¡n
   - Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, gradient nÃ y sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ cáº­p nháº­t: $w_{\text{new}} = w_{\text{old}} - \alpha \times \text{gradient}$, vá»›i $\alpha$ lÃ  learning rate

---

## 12. Káº¿t luáº­n

HÃ m **Sigmoid** khÃ´ng pháº£i lÃ  má»™t â€œmÃ¡nhâ€ cá»§a há»c mÃ¡y, mÃ  lÃ  **há»‡ quáº£ tá»± nhiÃªn cá»§a tÆ° duy thá»‘ng kÃª**:

> Khi ta muá»‘n má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh biá»ƒu diá»…n xÃ¡c suáº¥t, viá»‡c chuyá»ƒn qua *log-odds* rá»“i nghá»‹ch Ä‘áº£o láº¡i táº¥t yáº¿u dáº«n Ä‘áº¿n hÃ m Sigmoid.

VÃ¬ tháº¿, **Logistic Regression** vá»«a Ä‘Æ¡n giáº£n vá»«a sÃ¢u sáº¯c, nÃ³ káº¿t há»£p giá»¯a **toÃ¡n há»c**, **thá»‘ng kÃª**, **xÃ¡c suáº¥t** vÃ  **trá»±c giÃ¡c** má»™t cÃ¡ch hoÃ n háº£o.

---

## 13. TÃ i liá»‡u tham kháº£o

- [Hosmer & Lemeshow (2000). *Applied Logistic Regression*. Wiley.](https://onlinelibrary.wiley.com/doi/book/10.1002/0471722146) 
- [Andrew Ng (Coursera). *Machine Learning â€” Week 3: Logistic Regression.*](https://www.coursera.org/learn/machine-learning#about) 
- [Wikipedia: Logistic function](https://en.wikipedia.org/wiki/Logistic_function)  
- [StatQuest: Logistic Regression Clearly Explained](https://www.youtube.com/watch?v=yIYKR4sgzI8)  
- [PyTorch: BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)