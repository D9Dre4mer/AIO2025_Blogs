---
title: "HÃ m Sigmoid trong Logistic Regression â€” Tá»« log-odds Ä‘áº¿n xÃ¡c suáº¥t"
description: "Giáº£i thÃ­ch vÃ¬ sao hÃ m Sigmoid xuáº¥t hiá»‡n tá»± nhiÃªn trong Logistic Regression: tá»« odds â†’ logit â†’ giáº£ Ä‘á»‹nh tuyáº¿n tÃ­nh â†’ sigmoid. Bao gá»“m lá»‹ch sá»­, tá»‘i Æ°u hÃ³a, vÃ  vÃ­ dá»¥ minh há»a."
date: 2025-11-13
author: Nguyá»…n Quang Linh
tags: [machine learning, logistic regression, sigmoid, xÃ¡c suáº¥t, thá»‘ng kÃª, cross-entropy, maximum likelihood]
---

# HÃ m Sigmoid trong Logistic Regression â€” Tá»« log-odds Ä‘áº¿n xÃ¡c suáº¥t
# Giá»›i thiá»‡u
Khi má»›i lÃ m quen vá»›i cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n, chÃºng ta thÆ°á»ng há»c qua **Linear Regression** â€“ nÆ¡i má»™t hÃ m tuyáº¿n tÃ­nh sáº½ dá»± Ä‘oÃ¡n Ä‘áº§u ra liÃªn tá»¥c, nhÆ° giÃ¡ nhÃ  hoáº·c Ä‘iá»ƒm sá»‘. **Linear Regression** ráº¥t máº¡nh máº½ vá»›i dá»¯ liá»‡u liÃªn tá»¥c, nhÆ°ng láº¡i khÃ´ng phÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n mÃ  Ä‘áº§u ra chá»‰ nháº­n giÃ¡ trá»‹ 0 hoáº·c 1, nhÆ° dá»± Ä‘oÃ¡n má»™t email cÃ³ pháº£i spam hay khÃ´ng, hay khÃ¡ch hÃ ng cÃ³ rá»i bá» dá»‹ch vá»¥ hay khÃ´ng.

Náº¿u Ã¡p dá»¥ng linear regression cho cÃ¡c bÃ i toÃ¡n nÃ y, giÃ¡ trá»‹ dá»± bÃ¡o cÃ³ thá»ƒ vÆ°á»£t ra ngoÃ i khoáº£ng $[0,1]$ vÃ  gÃ¢y khÃ³ khÄƒn trong viá»‡c diá»…n giáº£i lÃ  xÃ¡c suáº¥t. VÃ¬ tháº¿, chÃºng ta cáº§n má»™t mÃ´ hÃ¬nh giá»¯ láº¡i **tÃ­nh tuyáº¿n tÃ­nh (vÃ¬ dá»… giáº£i thÃ­ch, huáº¥n luyá»‡n)** nhÆ°ng láº¡i Ä‘áº£m báº£o Ä‘áº§u ra lÃ  xÃ¡c suáº¥t há»£p lá»‡ â€“ Ä‘Ã³ chÃ­nh lÃ  **Logistic Regression**.

**Logistic Regression** khÃ¡c biá»‡t chá»§ yáº¿u nhá» hÃ m **sigmoid**. HÃ m nÃ y "Ã©p" Ä‘áº§u ra cá»§a biá»ƒu thá»©c tuyáº¿n tÃ­nh vÃ o Ä‘Ãºng khoáº£ng $[0,1]$, Ä‘á»ƒ cÃ³ thá»ƒ diá»…n giáº£i xÃ¡c suáº¥t má»™t cÃ¡ch há»£p lÃ½. 

BÃ i blog nÃ y sáº½ dáº«n báº¡n Ä‘i tá»« ná»n táº£ng **linear regression** sang **logistic regression**, khÃ¡m phÃ¡ lÃ½ do xÃ¡c suáº¥t Ä‘Æ°á»£c káº¿t ná»‘i cháº·t cháº½ vá»›i hÃ m **sigmoid**, cÃ¡c Ã½ tÆ°á»Ÿng tá»« **odds** tá»›i **logit** rá»“i quy vá» **xÃ¡c suáº¥t**, cÃ¹ng vÃ­ dá»¥, tá»‘i Æ°u hÃ³a, cÃ¡c lÆ°u Ã½ thá»±c táº¿ vÃ  cáº­p nháº­t má»›i vá» regularization vÃ  cÃ¢n báº±ng dá»¯ liá»‡u â€“ giÃºp báº¡n hiá»ƒu toÃ n diá»‡n, tá»« trá»±c giÃ¡c Ä‘áº¿n triá»ƒn khai hiá»‡n Ä‘áº¡i.

<div align="center">
![Logo](https://i.ibb.co/d41fHgXL/Logo-999.png)
</div>

---

## 1. Báº¯t Ä‘áº§u tá»« mong muá»‘n mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t khi Linear Regression khÃ´ng phÃ¹ há»£p

Giáº£ sá»­ ta Ä‘ang giáº£i quyáº¿t bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n:
- $y = 1$: Ä‘á»‘i tÆ°á»£ng thuá»™c lá»›p 1  
- $y = 0$: Ä‘á»‘i tÆ°á»£ng khÃ´ng thuá»™c lá»›p 1  

Ta muá»‘n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n:

$$
P(y=1|x)
$$

vÃ  giÃ¡ trá»‹ nÃ y **pháº£i náº±m trong [0,1]** Ä‘á»ƒ cÃ³ thá»ƒ hiá»ƒu lÃ  xÃ¡c suáº¥t.  
Tuy nhiÃªn, há»“i quy tuyáº¿n tÃ­nh:

$$
\hat{y} = w^T x + b
$$

cho káº¿t quáº£ cÃ³ thá»ƒ Ã¢m, hoáº·c lá»›n hÆ¡n 1 â€” âŒ khÃ´ng há»£p lÃ½ vá» máº·t xÃ¡c suáº¥t.

<div style="text-align:center;">
  <img src="https://i.ibb.co/7tDYQz2k/section1.png" alt="HÃ¬nh 1: VÃ­ dá»¥ Linear Regression dá»± Ä‘oÃ¡n giÃ¡ trá»‹ vÆ°á»£t ngoÃ i [0,1]*" width="720"/>
</div>

---

## 2. Ã tÆ°á»Ÿng: Dá»± Ä‘oÃ¡n **tá»· lá»‡ xáº£y ra vÃ  khÃ´ng xáº£y ra**

Thay vÃ¬ dá»± Ä‘oÃ¡n trá»±c tiáº¿p $P$, cÃ¡c nhÃ  thá»‘ng kÃª tháº¿ ká»· 19 (Ä‘áº·c biá»‡t **Pierre FranÃ§ois Verhulst**, 1838) Ä‘Ã£ nghÄ© khÃ¡c Ä‘i:  
HÃ£y dá»± Ä‘oÃ¡n **tá»· lá»‡ odds**:

$$
\text{odds} = \frac{P(y=1|x)}{1 - P(y=1|x)}
$$

- Náº¿u $P = 0.5$ â†’ odds = 1 (cÆ¡ há»™i ngang nhau)  
- Náº¿u $P = 0.8$ â†’ odds = 4 (xáº£y ra gáº¥p 4 láº§n khÃ´ng xáº£y ra)  
- Náº¿u $P = 0.2$ â†’ odds = 0.25 (xáº£y ra Ã­t hÆ¡n 4 láº§n)

Tuy nhiÃªn, odds váº«n chá»‰ dÆ°Æ¡ng, chÆ°a thá»ƒ mÃ´ hÃ¬nh hÃ³a báº±ng hÃ m tuyáº¿n tÃ­nh (vÃ¬ tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ã¢m).  
VÃ¬ váº­y, ta **láº¥y log cá»§a odds** Ä‘á»ƒ "tráº£i" nÃ³ ra toÃ n trá»¥c thá»±c:

$$
\text{logit}(P) = \log\left(\frac{P}{1 - P}\right)
$$

â†’ ÄÃ¢y chÃ­nh lÃ  **hÃ m logit**.  
VÃ  nhá» log, ta cÃ³ thá»ƒ cho giÃ¡ trá»‹ nÃ y báº±ng **má»™t biá»ƒu thá»©c tuyáº¿n tÃ­nh**.

<div style="text-align:center;">
  <img src="https://i.ibb.co/QxRj0SV/section2.png" alt="HÃ¬nh 2: So sÃ¡nh Probability â€“ Odds â€“ Logit trÃªn cÃ¹ng trá»¥c" width="720"/>
</div>

---

## 3. Giáº£ Ä‘á»‹nh quan há»‡ tuyáº¿n tÃ­nh trong miá»n logit

Ta Ä‘áº·t giáº£ Ä‘á»‹nh:

$$
\text{logit}(P) = w^T x + b
$$

hay tÆ°Æ¡ng Ä‘Æ°Æ¡ng:

$$
\log\left(\frac{P}{1 - P}\right) = w^T x + b
$$

ÄÃ¢y lÃ  bÆ°á»›c "thiáº¿t káº¿" cá»‘t lÃµi â€” **má»™t giáº£ Ä‘á»‹nh thá»‘ng kÃª há»£p lÃ½**:
- VÃ¬ Ä‘áº§u vÃ o $x$ áº£nh hÆ°á»Ÿng tuyáº¿n tÃ­nh lÃªn "Ä‘á»™ tin cáº­y" (log-odds)
- MÃ  logit cho phÃ©p ta lÃ m viá»‡c trÃªn toÃ n miá»n sá»‘ thá»±c

Giá» ta chá»‰ cáº§n biáº¿n Ä‘á»•i ngÆ°á»£c Ä‘á»ƒ tÃ¬m $P$.

<div style="text-align:center;">
  <img src="https://i.ibb.co/k2xkpht7/section3.png" alt="HÃ¬nh 3: Log-odds tÄƒng tuyáº¿n tÃ­nh khi Ä‘áº·c trÆ°ng tÄƒng" width="720"/>
</div>

---

## 4. Biáº¿n Ä‘á»•i ngÆ°á»£c â†’ HÃ m Sigmoid ra Ä‘á»i

Báº¯t Ä‘áº§u tá»«:

$$
\frac{P}{1 - P} = e^{w^T x + b}
$$

Giáº£i theo $P$:

$$
P = \frac{e^{w^T x + b}}{1 + e^{w^T x + b}} = \frac{1}{1 + e^{-(w^T x + b)}}
$$

â†’ ÄÃ¢y chÃ­nh lÃ  **hÃ m Sigmoid** mÃ  ta dÃ¹ng ngÃ y nay:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

<div style="text-align:center;">
  <img src="https://i.ibb.co/xt6yxhvh/Figure-1.png" alt="ÄÆ°á»ng cong Sigmoid" width="720"/>
</div>

---

## 5. HÃ m Sigmoid vÃ  má»‘i quan há»‡ vá»›i Logit

- Sigmoid lÃ  **nghá»‹ch Ä‘áº£o cá»§a hÃ m logit**:  
  $$
  \sigma(z) = \frac{1}{1 + e^{-z}} \iff \text{logit}(P) = z
  $$
- Khi $z = 0$ â†’ $P = 0.5$  
- Khi $z \to +\infty$ â†’ $P \to 1$  
- Khi $z \to -\infty$ â†’ $P \to 0$

HÃ m nÃ y **Ä‘Æ¡n Ä‘iá»‡u tÄƒng**, cÃ³ Ä‘áº¡o hÃ m gá»n Ä‘áº¹p:
$$
\sigma'(z) = \sigma(z)(1 - \sigma(z))
$$
â†’ thuáº­n lá»£i cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng **Gradient Descent**.

<div style="text-align:center;">
  <img src="https://i.ibb.co/r2w9wC30/section5.png" alt="HÃ¬nh 5: Sigmoid + Ä‘áº¡o hÃ m cá»§a sigmoid" width="720"/>
</div>

---

## 6. Tá»‘i Æ°u hÃ³a báº±ng Maximum Likelihood vÃ  Cross-Entropy

Sau khi cÃ³ mÃ´ hÃ¬nh $P(y|x; w, b)$, ta cáº§n tÃ¬m tham sá»‘ $w, b$.  **Logistic Regression** há»c tham sá»‘ báº±ng cÃ¡ch **tá»‘i Ä‘a hÃ³a kháº£ nÄƒng dá»¯ liá»‡u xuáº¥t hiá»‡n** â€” Maximum Likelihood Estimation (MLE):

$$
L(w,b) = \prod_i P(y_i|x_i; w,b)
$$

Láº¥y log cho dá»… tá»‘i Æ°u:

$$
\ell(w,b) = \sum_i \left[y_i \log \sigma(z_i) + (1 - y_i)\log(1 - \sigma(z_i))\right]
$$

VÃ  ta **tá»‘i Ä‘a hÃ³a log-likelihood**, tÆ°Æ¡ng Ä‘Æ°Æ¡ng **tá»‘i thiá»ƒu hÃ³a hÃ m máº¥t mÃ¡t cross-entropy**:

$$
\mathcal{L} = -\ell(w,b)
$$

Gradient:

$$
\nabla_w \mathcal{L} = (\sigma(z) - y)x
$$

â†’ ÄÃ¢y lÃ  lÃ½ do hÃ m Sigmoid Ä‘Æ°á»£c Æ°a chuá»™ng: Ä‘áº¡o hÃ m cá»§a nÃ³ khiáº¿n cáº­p nháº­t gradient cá»±c ká»³ Ä‘Æ¡n giáº£n vÃ  á»•n Ä‘á»‹nh.

<div style="text-align:center;">
  <img src="https://i.ibb.co/Xx657Ytt/section6.png" alt="HÃ¬nh 6: HÃ¬nh dáº¡ng hÃ m loss cross-entropy cho y=1 vÃ  y=0" width="720"/>
</div>

---

## 7. á»”n Ä‘á»‹nh sá»‘ khi tÃ­nh Sigmoid

Trong thá»±c táº¿, vá»›i $z$ ráº¥t lá»›n hoáº·c ráº¥t nhá», $e^{-z}$ cÃ³ thá»ƒ **trÃ n sá»‘ (overflow/underflow)**.  
Má»™t sá»‘ cÃ¡ch kháº¯c phá»¥c:
- Giá»›i háº¡n $z$ trong khoáº£ng $[-20, 20]$ trÆ°á»›c khi tÃ­nh  
- DÃ¹ng biá»ƒu thá»©c á»•n Ä‘á»‹nh hÆ¡n trong log-loss:  
  $$
  \log(1 + e^{-z}) \approx \text{softplus}(z)
  $$
- Trong framework há»c mÃ¡y, ngÆ°á»i ta thÆ°á»ng dÃ¹ng hÃ m `torch.nn.BCEWithLogitsLoss` (PyTorch) hoáº·c `tf.nn.sigmoid_cross_entropy_with_logits` (TensorFlow), vá»‘n Ä‘Ã£ xá»­ lÃ½ á»•n Ä‘á»‹nh sá»‘ bÃªn trong.

---

## 8. So sÃ¡nh Logistic vÃ  Probit

Má»™t mÃ´ hÃ¬nh gáº§n giá»‘ng Logistic Regression lÃ  **Probit Regression**, trong Ä‘Ã³:
$$
P(y=1|x) = \Phi(w^T x + b)
$$
vá»›i $\Phi$ lÃ  hÃ m tÃ­ch lÅ©y cá»§a phÃ¢n phá»‘i chuáº©n.  
Hai mÃ´ hÃ¬nh nÃ y cho káº¿t quáº£ tÆ°Æ¡ng tá»±, chá»‰ khÃ¡c vá» **Ä‘uÃ´i phÃ¢n phá»‘i** vÃ  **diá»…n giáº£i thá»‘ng kÃª** â€” Logistic cÃ³ Ä‘uÃ´i dÃ i hÆ¡n, nÃªn thÆ°á»ng dÃ¹ng phá»• biáº¿n hÆ¡n trong há»c mÃ¡y.

---

## 9. Lá»‹ch sá»­ ra Ä‘á»i cá»§a hÃ m Sigmoid

- NÄƒm **1838**, **Pierre FranÃ§ois Verhulst** (nhÃ  toÃ¡n há»c Bá»‰) giá»›i thiá»‡u hÃ m logistic Ä‘á»ƒ mÃ´ táº£ **tÄƒng trÆ°á»Ÿng dÃ¢n sá»‘**:
  $$
  N(t) = \frac{K}{1 + e^{-r(t - t_0)}}
  $$
  Trong Ä‘Ã³:
  - $K$: giá»›i háº¡n dÃ¢n sá»‘ tá»‘i Ä‘a  
  - $r$: tá»‘c Ä‘á»™ tÄƒng trÆ°á»Ÿng  
  - $t_0$: thá»i Ä‘iá»ƒm tÄƒng máº¡nh nháº¥t  

  ÄÃ¢y lÃ  má»™t **Ä‘Æ°á»ng cong S (sigmoid)** thá»ƒ hiá»‡n sá»± tÄƒng dáº§n cháº­m dáº§n khi tiáº¿n Ä‘áº¿n giá»›i háº¡n $K$.
  
<div style="text-align:center;">
  <img src="https://i.ibb.co/4wjVY0py/section7.png" alt="HÃ¬nh 7: ÄÆ°á»ng cong dÃ¢n sá»‘ logistic theo thá»i gian" width="720"/>
</div>


- Tháº¿ ká»· 20, cÃ¡c nhÃ  thá»‘ng kÃª nháº­n ra cÃ¹ng hÃ m nÃ y cÃ³ thá»ƒ **chuyá»ƒn Ä‘á»•i xÃ¡c suáº¥t phi tuyáº¿n**, vÃ  **Logistic Regression** ra Ä‘á»i.

> ğŸ§© NÃ³i cÃ¡ch khÃ¡c: hÃ m Sigmoid sinh ra tá»« tá»± nhiÃªn (mÃ´ hÃ¬nh tÄƒng trÆ°á»Ÿng), rá»“i Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng trong há»c mÃ¡y nhÆ° má»™t hÃ m xÃ¡c suáº¥t há»£p lÃ½.

---

## 10. Tá»•ng káº¿t

| BÆ°á»›c | Ã tÆ°á»Ÿng chÃ­nh | Káº¿t quáº£ |
|------|----------------|---------|
| 1ï¸âƒ£ | Muá»‘n mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t trong [0,1] | Linear regression khÃ´ng phÃ¹ há»£p |
| 2ï¸âƒ£ | Dá»± Ä‘oÃ¡n tá»· lá»‡ odds thay vÃ¬ xÃ¡c suáº¥t | odds luÃ´n dÆ°Æ¡ng |
| 3ï¸âƒ£ | Láº¥y log cá»§a odds â†’ logit | Má»Ÿ rá»™ng vá» miá»n sá»‘ thá»±c |
| 4ï¸âƒ£ | Giáº£ Ä‘á»‹nh logit = wáµ€x + b | Táº¡o má»‘i quan há»‡ tuyáº¿n tÃ­nh |
| 5ï¸âƒ£ | Biáº¿n Ä‘á»•i ngÆ°á»£c â†’ Sigmoid | Ãnh xáº¡ vá» xÃ¡c suáº¥t |
| 6ï¸âƒ£ | Tá»‘i Æ°u báº±ng MLE / cross-entropy | Gradient Ä‘áº¹p, dá»… huáº¥n luyá»‡n |
| 7ï¸âƒ£ | á»”n Ä‘á»‹nh sá»‘ & triá»ƒn khai thá»±c táº¿ | DÃ¹ng softplus hoáº·c BCEWithLogitsLoss |
| 8ï¸âƒ£ | Logistic vs Probit | KhÃ¡c á»Ÿ phÃ¢n phá»‘i vÃ  á»©ng dá»¥ng |

---

## 11. VÃ­ dá»¥ code minh há»a

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def grad(w, x, y):
    z = np.dot(w, x)
    return (sigmoid(z) - y) * x

x = np.array([1.0, 2.0])
w = np.array([0.3, -0.5])
y = 1

sig = sigmoid(np.dot(w, x))
g = grad(w, x, y)

print("Sigmoid output:", sig)
print("Gradient:", g)

Sigmoid output: 0.3775406687981454
Gradient: [-0.62245933 -1.24491866]

```

---

## 12. Káº¿t luáº­n

HÃ m **Sigmoid** khÃ´ng pháº£i lÃ  má»™t â€œmÃ¡nhâ€ cá»§a há»c mÃ¡y, mÃ  lÃ  **há»‡ quáº£ tá»± nhiÃªn cá»§a tÆ° duy thá»‘ng kÃª**:

> Khi ta muá»‘n má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh biá»ƒu diá»…n xÃ¡c suáº¥t, viá»‡c chuyá»ƒn qua *log-odds* rá»“i nghá»‹ch Ä‘áº£o láº¡i táº¥t yáº¿u dáº«n Ä‘áº¿n hÃ m Sigmoid.

VÃ¬ tháº¿, **Logistic Regression** vá»«a Ä‘Æ¡n giáº£n vá»«a sÃ¢u sáº¯c, nÃ³ káº¿t há»£p giá»¯a **toÃ¡n há»c**, **thá»‘ng kÃª**, **xÃ¡c suáº¥t** vÃ  **trá»±c giÃ¡c** má»™t cÃ¡ch hoÃ n háº£o.

---

## 13. TÃ i liá»‡u tham kháº£o

- [Hosmer & Lemeshow (2000). *Applied Logistic Regression*. Wiley.](https://onlinelibrary.wiley.com/doi/book/10.1002/0471722146) 
- [Andrew Ng (Coursera). *Machine Learning â€” Week 3: Logistic Regression.*](https://www.coursera.org/learn/machine-learning#about) 
- [Wikipedia: Logistic function](https://en.wikipedia.org/wiki/Logistic_function)  
- [StatQuest: Logistic Regression Clearly Explained](https://www.youtube.com/watch?v=yIYKR4sgzI8)  
- [PyTorch: BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)