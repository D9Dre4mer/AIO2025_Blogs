\begin{center}
    \Large\textbf{Random Forest: Basic, Advanced Concepts and Its Application}
\end{center}

\begin{center}
    \Large\textit{Vương Nguyệt Bình}
\end{center}

\section{“Nếu một cái cây có thể đưa ra quyết định, thì một khu rừng sẽ làm được gì?”}
Hãy thử tưởng tượng: bạn đứng trước một quyết định quan trọng -- chẳng hạn, có nên đầu tư vào một cổ phiếu mới hay không. Bạn hỏi ý kiến một người bạn am hiểu tài chính. Người đó suy nghĩ, cân nhắc dữ liệu, rồi đưa ra câu trả lời. Nhưng liệu chỉ một ý kiến có đủ để bạn yên tâm?

Giờ hãy hình dung, thay vì hỏi một người, bạn hỏi \textbf{100 người} với kiến thức và góc nhìn khác nhau. Bạn nhận được nhiều ý kiến, sau đó tổng hợp lại bằng cách ``bỏ phiếu''. Kết quả cuối cùng có khả năng sẽ khách quan và đáng tin cậy hơn nhiều.

Đây chính là ý tưởng đằng sau \textbf{Random Forest} -- một trong những thuật toán mạnh mẽ và phổ biến nhất trong học máy. Random Forest không phải là một cái cây duy nhất (Decision Tree), mà là \textbf{cả một khu rừng của những cây}. Mỗi cây có thể còn ``yếu'', nhưng khi kết hợp, chúng tạo ra một hệ thống dự đoán mạnh mẽ, ổn định và đáng tin cậy.

Trong bài viết này, chúng ta sẽ cùng đi qua:
\begin{itemize}
    \item \textbf{Decision Tree} -- điểm khởi đầu đầy trực quan nhưng còn nhiều hạn chế.
    \item \textbf{Random Forest} -- cách nhiều cây cùng nhau khắc phục điểm yếu.
    \item \textbf{Xử lý dữ liệu bị thiếu} -- một vấn đề thực tiễn thường gặp và cách Random Forest giải quyết sáng tạo.
    \item \textbf{Ứng dụng trong chuỗi thời gian} -- đưa Random Forest đến gần hơn với các bài toán dự báo thực tế.
\end{itemize}

\noindent Đến cuối bài, bạn sẽ thấy rằng Random Forest không chỉ là một công cụ kỹ thuật, mà còn là một \textbf{ẩn dụ thú vị về trí tuệ tập thể}: \textit{cả khu rừng luôn thông minh hơn một cái cây đơn độc.}
\begin{center}
    \includegraphics[width=0.7\textwidth]{image/rd_vs_dt.png}
\end{center}

\section{Khởi nguồn: Decision Tree}
\subsection*{Decision Tree là gì?}
\textbf{Decision Tree} có thể hình dung như một sơ đồ câu hỏi \textit{có/không}, trong đó mỗi nút (node) đại diện cho một câu hỏi về dữ liệu, và mỗi nhánh (branch) dẫn đến một quyết định tiếp theo. Ví dụ: 
\begin{itemize}
    \item \textit{Hôm nay trời mưa?} $\rightarrow$ Nếu có, mang ô; nếu không, đi bộ.
    \item \textit{Có đói không?} $\rightarrow$ Nếu có, chọn quán ăn; nếu không, về nhà nghỉ.
\end{itemize}
Nhờ trực quan và gần gũi, Decision Tree thường là mô hình đầu tiên được giới thiệu trong học máy.

\subsection*{Ưu điểm}
\begin{itemize}
    \item \textbf{Dễ giải thích:} Kết quả dự đoán có thể được minh họa bằng các câu hỏi đơn giản.
    \item \textbf{Phản ánh tư duy con người:} Cách ra quyết định giống quá trình suy luận logic thường ngày.
    \item \textbf{Xử lý biến định tính:} Không cần tạo biến giả (dummy variables) cho dữ liệu dạng phân loại.
\end{itemize}

\subsection*{Hạn chế}
\begin{itemize}
    \item \textbf{Độ chính xác dự đoán:} Thường kém hơn các mô hình hiện đại.
    \item \textbf{Nhạy cảm với dữ liệu:} Chỉ một thay đổi nhỏ trong dữ liệu có thể dẫn đến một cây hoàn toàn khác.
    \item \textbf{Overfitting:} Dễ bị phân nhánh quá mức khi làm việc với biến liên tục, khiến mô hình kém tổng quát.
\end{itemize}

\subsection*{Một điểm kỹ thuật thú vị}
Để kiểm soát độ phức tạp của cây, người ta sử dụng một tham số gọi là \textbf{Tree Complexity Penalty} ($\alpha$).  
\[
\text{Tree Score} = \text{SSR} + \alpha T
\]
Trong đó $T$ là số lá (terminal nodes) và $\alpha$ được xác định qua cross-validation.  
Điều này liên quan trực tiếp đến \textbf{bias-variance trade-off}:  
\begin{itemize}
    \item Nếu $\alpha$ nhỏ $\rightarrow$ cây phát triển sâu $\rightarrow$ \textit{bias thấp nhưng variance cao} $\rightarrow$ overfitting.
    \item Nếu $\alpha$ lớn $\rightarrow$ cây bị cắt ngắn $\rightarrow$ \textit{variance thấp nhưng bias cao} $\rightarrow$ underfitting.
\end{itemize}
Nói cách khác, một cây đơn lẻ luôn phải đối mặt với bài toán ``đánh đổi'': 
hoặc là mô hình đơn giản nhưng kém chính xác, 
hoặc là mô hình phức tạp nhưng dễ bị sai khi gặp dữ liệu mới. 
Đây chính là \textbf{điểm yếu cốt lõi của Decision Tree}.  

\medskip

\noindent
\textbf{Và đây cũng là lý do Random Forest ra đời:} 
thay vì cố gắng tìm một cây ``hoàn hảo'', Random Forest tạo ra một tập hợp nhiều cây quyết định. 
Mỗi cây có thể hơi ``thiên vị'' hoặc hơi ``dao động'', 
nhưng khi gộp lại bằng cách \textit{trung bình} (cho hồi quy) hoặc \textit{bỏ phiếu} (cho phân loại), 
mô hình vừa giữ được \textbf{bias thấp}, vừa giảm đáng kể \textbf{variance}.  
Chính sự kết hợp này giúp Random Forest mạnh mẽ và ổn định hơn nhiều so với một Decision Tree đơn lẻ.

\section{Random Forest -- Sức mạnh từ sự kết hợp}

\subsection{Động lực: Vì sao cần Random Forest?}
Một cây quyết định đơn lẻ giống như việc chỉ tham khảo ý kiến từ \textbf{một người}. Kết quả có thể đúng, nhưng cũng dễ sai nếu người đó thiên vị hoặc dữ liệu chưa đủ.

\noindent
\textbf{Ý tưởng:} Hãy \textbf{hỏi nhiều người và tổng hợp ý kiến}. Random Forest chính là như vậy: nó kết hợp nhiều ``cây yếu'' (weak learners) để tạo nên một ``rừng mạnh'' (strong learner).  

Random Forest thuộc nhóm \textbf{Ensemble Learning}, cụ thể là phương pháp \textbf{Bagging (Bootstrap Aggregation)}. Cơ chế này giúp giảm phương sai, ổn định dự đoán mà không làm bias tăng nhiều.  

\begin{center}
    \includegraphics[width=0.8\textwidth]{image/dt.png}
\end{center}

\noindent
\subsection*{Cách Random Forest được xây dựng}

\begin{enumerate}
    \item \textbf{Bootstrapping dữ liệu (Bagging)}  
    Thay vì huấn luyện tất cả cây trên cùng một tập dữ liệu gốc, Random Forest tạo ra nhiều ``bản sao'' bằng cách \textbf{lấy mẫu ngẫu nhiên có hoàn lại}.  
    Mỗi cây sẽ nhìn thấy một phần khác nhau của dữ liệu, giúp giảm sự tương đồng giữa các cây.

    \item \textbf{Chọn ngẫu nhiên một tập con đặc trưng (Random Subset of Features)}  
    Tại mỗi nút chia, thay vì xét tất cả đặc trưng như phương pháp truyền thống, chỉ chọn một số đặc trưng ngẫu nhiên ((ví dụ: chọn 2 trong số 4 đặc trưng).  
    Điều này ép các cây ``suy nghĩ khác nhau'', giúp rừng trở nên phong phú hơn.
    Đây là bước mấu chốt giúp Random Forest \textbf{giảm overfitting}.  


    \item \textbf{Huấn luyện nhiều cây độc lập}  
    Quá trình được lặp lại hàng trăm hoặc hàng nghìn lần để sinh ra nhiều cây khác nhau.  
    Không cây nào hoàn hảo, nhưng sự kết hợp tạo nên một mô hình mạnh.

    \item \textbf{Cơ chế dự đoán}  
    \begin{itemize}
        \item Với \textbf{classification}: mỗi cây bỏ phiếu cho một nhãn, kết quả cuối cùng là nhãn có nhiều phiếu nhất.
        \item Với \textbf{regression}: lấy trung bình dự đoán của tất cả cây.  
        Ví dụ: Nếu có 9 cây, 7 cây dự đoán ``Có bệnh'', 2 cây dự đoán ``Không bệnh'' $\rightarrow$ kết quả cuối cùng là ``Có bệnh''.
    \end{itemize}
\end{enumerate}
\subsection{Thử nghiệm nhỏ: Decision Tree vs Random Forest}
Ở phần trên, chúng ta đã thấy rằng \textbf{Decision Tree thường bị overfitting} vì quá nhạy cảm với dữ liệu, trong khi \textbf{Random Forest} khắc phục hạn chế này nhờ kết hợp nhiều cây và lựa chọn đặc trưng ngẫu nhiên. Nhưng lý thuyết đôi khi chưa đủ thuyết phục. Hãy cùng làm một thử nghiệm nhỏ.  
\begin{lstlisting}[language=Python, caption={Building and visualizing Random Forest with scikit-learn}]
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Generate synthetic dataset
X, y = make_classification(
    n_samples=2000,
    n_features=20,
    n_informative=10,
    n_redundant=5,
    random_state=42
)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict and evaluate
print("Decision Tree - Train:", accuracy_score(y_train, dt.predict(X_train)))
print("Decision Tree - Test :", accuracy_score(y_test, dt.predict(X_test)))

print("Random Forest - Train:", accuracy_score(y_train, rf.predict(X_train)))
print("Random Forest - Test :", accuracy_score(y_test, rf.predict(X_test)))
\end{lstlisting}
Khi chạy code trên, bạn sẽ thấy kết quả tương tự như sau (số liệu có thể chênh một chút tuỳ lần chạy):  

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Mô hình} & \textbf{Train Accuracy} & \textbf{Test Accuracy} \\
\hline
Decision Tree & 0.99 & 0.82 \\
Random Forest & 0.97 & 0.91 \\
\hline
\end{tabular}
\caption{So sánh độ chính xác giữa Decision Tree và Random Forest}
\end{table}

\subsection*{Biểu đồ so sánh}
\begin{lstlisting}
    
import matplotlib.pyplot as plt

# Giá trị accuracy ví dụ
models = ["Decision Tree", "Random Forest"]
train_scores = [0.99, 0.97]
test_scores = [0.82, 0.91]

x = range(len(models))

plt.bar(x, train_scores, width=0.4, label="Train", align="center")
plt.bar(x, test_scores, width=0.4, label="Test", align="edge")

plt.xticks(x, models)
plt.ylim(0.7, 1.05)
plt.ylabel("Accuracy")
plt.title("So sánh độ chính xác: Decision Tree vs Random Forest")
plt.legend()
plt.show()
\end{lstlisting}

\begin{center}
    \includegraphics[width=0.7\textwidth]{image/randomforest_vs_dt.png}
\end{center}

\subsection*{Giải thích}
\begin{itemize}
    \item \textbf{Decision Tree}: gần như hoàn hảo trên tập huấn luyện (train accuracy $\approx 99\%$) nhưng khi áp dụng vào tập kiểm tra, độ chính xác giảm mạnh ($\approx 82\%$). Đây chính là dấu hiệu của \textbf{overfitting}.
    \item \textbf{Random Forest}: độ chính xác trên tập huấn luyện vẫn cao ($\approx 97\%$) và quan trọng hơn, độ chính xác trên tập kiểm tra cũng cao ($\approx 91\%$). Điều này cho thấy Random Forest \textbf{giảm variance} và \textbf{tổng quát hóa tốt hơn}.
\end{itemize}

\noindent Thử nghiệm nhỏ này củng cố một điều: \textit{một cây có thể sai, nhưng cả rừng thì thường đúng hơn}.

\subsection{Lỗi Out-of-Bag (OOB Error)}
Một ưu điểm của Random Forest là có thể \textbf{đánh giá mô hình mà không cần tập kiểm tra riêng}.  

\begin{itemize}
    \item Trong quá trình bootstrap, trung bình khoảng $1/3$ dữ liệu gốc \textbf{không được chọn} để huấn luyện một cây. Phần này gọi là \textit{Out-of-bag dataset}.
    \item Các mẫu OOB được dùng như tập kiểm tra cho cây tương ứng. Sau đó, kết quả được tổng hợp để đo lường độ chính xác toàn bộ mô hình.
    \item Sai số này gọi là \textbf{OOB Error}, thường cho độ tin cậy gần tương đương với cross-validation.
\end{itemize}

\noindent
$\Rightarrow$ Nhờ vậy, Random Forest tự kèm theo một cơ chế kiểm định mô hình, không cần tốn thêm dữ liệu.

\subsection{Xử lý dữ liệu bị thiếu bằng Random Forest}
Dữ liệu thiếu là một vấn đề phổ biến trong các bộ dữ liệu thực tế và có thể gây ra nhiều thách thức cho các thuật toán học máy. Random Forest cung cấp một cách tiếp cận mạnh mẽ để xử lý dữ liệu thiếu bằng cách coi nó như một bài toán dự đoán. Quy trình này có thể tóm tắt qua ba bước:

\begin{enumerate}
    \item \textbf{Phỏng đoán ban đầu:} Điền vào các giá trị thiếu bằng một phỏng đoán ban đầu, ví dụ như giá trị trung bình hoặc trung vị của đặc trưng đó.
    \item \textbf{Xây dựng mô hình:} Huấn luyện một Random Forest trên dữ liệu đã được điền.
    \item \textbf{Tinh chỉnh phỏng đoán:} Sử dụng mô hình đã xây dựng cùng với một công cụ gọi là \textit{Ma trận Gần (Proximity Matrix)} để tinh chỉnh các giá trị đã phỏng đoán. Quy trình này được lặp lại cho đến khi các giá trị ổn định.
\end{enumerate}

\subsubsection*{Vai trò của Ma trận Gần (Proximity Matrix)}

Ma trận Gần là một ma trận $N \times N$, trong đó $N$ là số lượng mẫu. Giá trị tại ô $(i, j)$ đại diện cho số lần mẫu $i$ và mẫu $j$ cùng rơi vào một nút lá trên cùng một cây. 

Quy trình xây dựng:
\begin{itemize}
    \item Đi qua từng cây trong rừng.
    \item Đếm số lần các cặp mẫu $(i, j)$ cùng xuất hiện trong một nút lá.
    \item Chuẩn hóa ma trận để giá trị cao hơn thể hiện mức độ gần gũi hơn.
\end{itemize}

Khi điền giá trị thiếu cho một mẫu:
\begin{itemize}
    \item Đối với \textbf{biến định tính}: Giá trị thiếu được điền bằng cách \textit{bầu chọn có trọng số (weighted vote)}, trong đó trọng số được xác định bởi sự gần gũi trong ma trận.
    \item Đối với \textbf{biến định lượng}: Giá trị thiếu được điền bằng \textit{giá trị trung bình có trọng số} của các mẫu lân cận.
\end{itemize}

\subsubsection*{Ví dụ minh hoạ: Ma trận Gần}

Bảng dưới đây minh họa một Ma trận Gần được tổng hợp từ nhiều cây:

\begin{table}[H]
\centering
\caption{Ma trận Gần chưa chuẩn hóa}
\begin{tabular}{c|cccc}
\hline
     & 1 & 2 & 3 & 4 \\
\hline
1    & 2 & 1 & 1 & 1 \\
2    & 1 & 2 & 1 & 1 \\
3    & 1 & 1 & 8 & 8 \\
4    & 1 & 1 & 8 & 8 \\
\hline
\end{tabular}
\end{table}

Sau khi chuẩn hóa (giả sử có 10 cây trong rừng), ta thu được:

\begin{table}[H]
\centering
\caption{Ma trận Gần đã chuẩn hóa}
\begin{tabular}{c|cccc}
\hline
     & 1 & 2 & 3 & 4 \\
\hline
1    & 0.2 & 0.1 & 0.1 & 0.1 \\
2    & 0.1 & 0.2 & 0.1 & 0.1 \\
3    & 0.1 & 0.1 & 0.8 & 0.8 \\
4    & 0.1 & 0.1 & 0.8 & 0.8 \\
\hline
\end{tabular}
\end{table}

\section{Ứng dụng của Random Forest trong dự báo chuỗi thời gian (Time Series Forecasting)}

Random Forest không chỉ hoạt động tốt với dữ liệu tĩnh mà còn có thể áp dụng cho \textbf{dự báo chuỗi thời gian}. Tuy nhiên, vì Random Forest là thuật toán học máy giám sát (\textit{supervised learning}), nên dữ liệu chuỗi thời gian cần được chuyển đổi thành dạng bài toán supervised trước khi huấn luyện.

\subsection{Chuyển đổi dữ liệu Time Series thành dạng Supervised}

Giả sử ta có một chuỗi thời gian:
\[
y_1, y_2, y_3, \dots, y_t
\]

Để dự đoán giá trị tại thời điểm $t+1$, ta có thể tạo ra các đặc trưng dựa trên \textbf{các giá trị quá khứ}:
\[
X_t = [y_{t-n}, y_{t-n+1}, \dots, y_{t-1}], \quad y_t = y_t
\]

Trong đó:
\begin{itemize}
    \item $n$ là kích thước cửa sổ (\textbf{Sliding Window}) được sử dụng để lấy các giá trị quá khứ.
    \item $X_t$ là vector đặc trưng đầu vào cho mô hình Random Forest.
    \item $y_t$ là giá trị mục tiêu mà mô hình cần dự đoán.
\end{itemize}

\subsection{Sử dụng Sliding Window để tạo đặc trưng}

\textbf{Sliding Window} là kỹ thuật trượt một cửa sổ có kích thước cố định dọc theo chuỗi thời gian để tạo các mẫu dữ liệu huấn luyện. Ví dụ, với cửa sổ kích thước $n=3$:

\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Đặc trưng (X)} & \textbf{Mục tiêu (y)} \\
\hline
$[y_1, y_2, y_3]$ & $y_4$ \\
$[y_2, y_3, y_4]$ & $y_5$ \\
$[y_3, y_4, y_5]$ & $y_6$ \\
\hline
\end{tabular}
\end{center}

Mỗi hàng tương ứng với một mẫu huấn luyện mà Random Forest có thể sử dụng để học mối quan hệ giữa các giá trị quá khứ và giá trị tương lai.

\subsection{Huấn luyện và dự đoán}

Sau khi tạo các đặc trưng, ta sử dụng mô hình Random Forest để:
\begin{itemize}
    \item \textbf{Huấn luyện:} Học mối quan hệ giữa các đặc trưng quá khứ và giá trị tương lai.
    \item \textbf{Dự đoán:} Dự báo giá trị tại thời điểm tiếp theo dựa trên các giá trị quá khứ hiện tại.
\end{itemize}

\subsection{Đánh giá mô hình với k-Fold Cross Validation}

Để đánh giá hiệu quả dự báo, ta có thể sử dụng \textbf{k-Fold Cross Validation}, nhưng cần cẩn thận với dữ liệu chuỗi thời gian để tránh rò rỉ thông tin (\textit{data leakage}). Một số lưu ý:
\begin{itemize}
    \item Chia dữ liệu theo thời gian thay vì ngẫu nhiên.
    \item Huấn luyện trên các phân đoạn thời gian trước và kiểm tra trên các phân đoạn thời gian sau.
\end{itemize}


\subsection{Ưu điểm khi sử dụng Random Forest cho Time Series}

\begin{itemize}
    \item Không cần giả định tuyến tính giữa các giá trị quá khứ và tương lai.
    \item Xử lý tốt dữ liệu có nhiễu.
    \item Có thể kết hợp nhiều đặc trưng khác ngoài giá trị quá khứ, ví dụ: đặc trưng thời gian (ngày, tháng, mùa).
\end{itemize}


\subsection{Kết luận}

Random Forest có thể áp dụng hiệu quả cho dữ liệu chuỗi thời gian khi dữ liệu được biến đổi thành dạng supervised learning. Mặc dù không mô hình hóa trực tiếp tính tuần tự, nhưng với lựa chọn \textbf{window size hợp lý} và \textbf{tập feature đầy đủ}, mô hình vẫn có khả năng dự đoán chính xác các giá trị tương lai, đặc biệt là với dữ liệu phi tuyến và nhiều biến nhiễu.

% =============================
\section{Tổng kết và Khuyến nghị}

\subsection{Tổng hợp các đặc điểm nổi bật của Random Forest}

Random Forest là một thuật toán học kết hợp mạnh mẽ và hiệu quả, đã được chứng minh giá trị trong nhiều lĩnh vực khác nhau, từ tài chính, y tế đến dự báo và phân tích dữ liệu hình ảnh.  

Khác với \textbf{Cây Quyết định (Decision Tree)} đơn lẻ, Random Forest khắc phục được nhược điểm \textit{phương sai cao} và vấn đề \textit{quá khớp} nhờ kỹ thuật \textit{Bagging} và \textit{ngẫu nhiên hóa đặc trưng (random feature selection)}. Cụ thể:  

\begin{itemize}
    \item \textbf{Độ chính xác cao:} Kết hợp nhiều cây độc lập giúp mô hình dự đoán ổn định hơn và giảm lỗi so với cây đơn lẻ.  
    \item \textbf{Khả năng chống overfitting:} Việc chỉ sử dụng một tập con ngẫu nhiên của các đặc trưng khi chia nút giúp tránh việc mô hình “học thuộc lòng” dữ liệu huấn luyện.  
    \item \textbf{Tính ổn định:} Thay đổi nhỏ trong dữ liệu huấn luyện không làm ảnh hưởng quá nhiều đến kết quả cuối cùng, nhờ trung bình hóa dự đoán của nhiều cây.  
    \item \textbf{Xử lý tốt biến phi tuyến và dữ liệu phức tạp:} Random Forest có thể học các mối quan hệ phi tuyến, kết hợp giữa biến định tính và định lượng mà không cần quá nhiều tiền xử lý.
\end{itemize}

Ngoài ra, Random Forest còn cung cấp các công cụ bổ sung như \textit{Out-of-Bag Error} để đánh giá mô hình ngay trong quá trình huấn luyện, và \textit{Proximity Matrix} để xử lý dữ liệu thiếu, cho thấy tính linh hoạt và ứng dụng rộng rãi của thuật toán.

\subsection{Khuyến nghị và Ứng dụng Thực tiễn}

Nhờ những ưu điểm vượt trội, Random Forest là lựa chọn đáng tin cậy trong nhiều ứng dụng thực tế. Dưới đây là một số ví dụ cụ thể:

\begin{itemize}
    \item \textbf{Tài chính:} Random Forest giúp dự đoán rủi ro tín dụng của khách hàng, xác định khách hàng có khả năng vỡ nợ hoặc gian lận. Mô hình kết hợp nhiều cây giúp nhận diện các mẫu phức tạp trong dữ liệu tài chính, giảm nguy cơ cảnh báo giả.  
    \item \textbf{Y tế:} Thuật toán này được sử dụng để phân tích gen, dự đoán phản ứng của bệnh nhân với thuốc, và hỗ trợ chẩn đoán bệnh. Ensemble của nhiều cây giúp mô hình học được các mối quan hệ phi tuyến phức tạp giữa các đặc trưng sinh học.  
    \item \textbf{Dự báo:} Random Forest được ứng dụng trong dự báo thời tiết, khí hậu, và xu hướng thị trường. Việc kết hợp nhiều cây giúp giảm nhiễu và tăng độ ổn định của dự báo, đặc biệt với dữ liệu biến động cao.  
    \item \textbf{Phân tích hình ảnh:} Mô hình hỗ trợ nhận dạng khuôn mặt, phân tích hình ảnh vệ tinh và xử lý ảnh y tế. Random Forest giúp mô hình học các đặc trưng phức tạp mà không yêu cầu tiền xử lý phức tạp như các mô hình deep learning.
\end{itemize}

\subsubsection*{Lưu ý khi sử dụng Random Forest}

\begin{itemize}
    \item Random Forest thường khó giải thích hơn một cây quyết định đơn lẻ; tuy nhiên, hiệu suất cao thường bù đắp nhược điểm này.  
    \item Việc lựa chọn tham số như số lượng cây (\texttt{n\_estimators}) và số lượng đặc trưng chọn ngẫu nhiên khi chia nút (\texttt{max\_features}) ảnh hưởng trực tiếp đến hiệu suất mô hình.  
    \item Random Forest có thể được kết hợp với các mô hình khác, như Gradient Boosting hoặc Neural Networks, để cải thiện thêm độ chính xác hoặc khả năng dự báo dài hạn.  
\end{itemize}

\subsubsection*{Kết luận}

Nhìn chung, Random Forest là một công cụ mạnh mẽ, đáng tin cậy và linh hoạt trong kho vũ khí của các nhà khoa học dữ liệu. Khi được sử dụng đúng cách, nó cung cấp sự cân bằng tuyệt vời giữa độ chính xác, tính ổn định và khả năng chống overfitting, phù hợp cho cả nghiên cứu học thuật và ứng dụng thực tế.