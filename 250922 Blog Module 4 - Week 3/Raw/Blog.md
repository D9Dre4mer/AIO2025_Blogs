1. Giá»›i thiá»‡u

LightGBM (Light Gradient Boosting Machine) lÃ  má»™t thÆ° viá»‡n máº¡nh máº½ Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Microsoft, giÃºp huáº¥n luyá»‡n mÃ´ hÃ¬nh Gradient Boosting nhanh hÆ¡n vÃ  tiáº¿t kiá»‡m bá»™ nhá»› hÆ¡n.
Trong blog nÃ y, chÃºng ta sáº½:

Ã”n láº¡i cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng (Decision Tree, Random Forest, AdaBoost, Gradient Boosting, XGBoost).

Hiá»ƒu rÃµ cáº£i tiáº¿n cá»‘t lÃµi cá»§a LightGBM: Leaf-wise Growth, Histogram-based Split, GOSS, EFB.

TrÃ¬nh bÃ y qua cÃ¡c vÃ­ dá»¥ chi tiáº¿t (iris dataset, taxi trip dataset).

Minh há»a káº¿t quáº£ báº±ng TikZ vÃ  gá»£i Ã½ thÃªm hÃ¬nh áº£nh thá»±c táº¿.

2. Ã”n táº­p nhanh: CÃ¡c mÃ´ hÃ¬nh trÆ°á»›c LightGBM
MÃ´ hÃ¬nh	HÃ m loss Regression	HÃ m loss Classification	CÃ¡ch phÃ¡t triá»ƒn cÃ¢y	Äiá»ƒm má»›i chÃ­nh
Decision Tree	MSE	Gini/Entropy	Level-wise (top-down)	Chia theo split tá»‘t nháº¥t
Random Forest	MSE	Gini/Entropy	Level-wise + Bagging	Giáº£m variance qua sampling
AdaBoost	MSE	Exponential Loss	Sequential boosting	TÄƒng trá»ng sá»‘ Ä‘iá»ƒm sai
Gradient Boosting	Any diff. loss	Log-loss	Sequential boosting	Boosting theo gradient
XGBoost	Any diff. loss + Reg	Log-loss + Reg	Level-wise + Reg.	Taylor báº­c 2, regularization
LightGBM	Any diff. loss + Reg	Log-loss + Reg	Leaf-wise + Histogram	TÄƒng tá»‘c, giáº£m bá»™ nhá»›, GOSS, EFB

ğŸ‘‰ Ã tÆ°á»Ÿng: LightGBM giá»¯ ná»n táº£ng boosting nhÆ°ng tá»‘i Æ°u tá»‘c Ä‘á»™ vÃ  hiá»‡u quáº£ bá»™ nhá»›.

3. CÃ¡ch LightGBM xÃ¢y dá»±ng cÃ¢y
3.1 Level-wise vs. Leaf-wise

ThÃ´ng thÆ°á»ng cÃ¡c mÃ´ hÃ¬nh (XGBoost, Gradient Boosting) má»Ÿ rá»™ng cÃ¢y theo level-wise (tá»«ng táº§ng).
LightGBM chá»n chiáº¿n lÆ°á»£c leaf-wise: luÃ´n má»Ÿ rá»™ng lÃ¡ cÃ³ tiá»m nÄƒng giáº£m loss nhiá»u nháº¥t.

Minh há»a TikZ:

\begin{tikzpicture}[level distance=1.5cm, every node/.style={circle,draw,minimum size=7mm}]
\node {Root}
  child {node {A} 
    child {node {A1}}
    child {node {A2}}
  }
  child {node {B}
    child {node {B1}
      child {node {B11}} % má»Ÿ rá»™ng lÃ¡ cÃ³ tiá»m nÄƒng nháº¥t
    }
    child {node {B2}}
  };
\end{tikzpicture}


ğŸ‘‰ Trong vÃ­ dá»¥ trÃªn, thay vÃ¬ má»Ÿ rá»™ng toÃ n bá»™ táº§ng nhÆ° Level-wise, LightGBM chá»‰ má»Ÿ rá»™ng B1 vÃ¬ nÃ³ há»©a háº¹n giáº£m loss lá»›n nháº¥t.

3.2 Histogram-based Split

Thay vÃ¬ thá»­ táº¥t cáº£ cÃ¡c ngÆ°á»¡ng chia (threshold), LightGBM gom dá»¯ liá»‡u thÃ nh bins.
VÃ­ dá»¥ vá»›i Petal_Width cÃ³ 6 giÃ¡ trá»‹, thay vÃ¬ xÃ©t 5 Ä‘iá»ƒm chia, ta chá»‰ xÃ©t cÃ¡c bin:

Bin	GiÃ¡ trá»‹	NhÃ³m dá»¯ liá»‡u
1	[0.2â€“0.567]	{0.2, 0.5}
2	(0.567â€“0.933]	{0.6, 0.7, 0.9}
3	(0.933â€“1.3]	{1.3}

ğŸ‘‰ GiÃºp giáº£m sá»‘ lÆ°á»£ng phÃ©p thá»­, tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n.

3.3 Exclusive Feature Bundling (EFB)

Vá»›i dá»¯ liá»‡u categorical (vÃ­ dá»¥ mÃ u sáº¯c: Red, Blue, Green), one-hot encoding táº¡o 3 cá»™t riÃªng biá»‡t.

LightGBM gom chÃºng thÃ nh má»™t cá»™t duy nháº¥t (bundle), vÃ¬ cÃ¡c feature nÃ y khÃ´ng bao giá» Ä‘á»“ng thá»i xuáº¥t hiá»‡n giÃ¡ trá»‹ â‰  0.

VÃ­ dá»¥:

ID	Red	Blue	Green	â†’	Color_bundle
S1	1	0	0	â†’	0
S2	0	1	0	â†’	1
S3	0	0	1	â†’	2

ğŸ‘‰ Tiáº¿t kiá»‡m bá»™ nhá»› vÃ  tÄƒng tá»‘c tÃ­nh toÃ¡n.

3.4 Gradient-based One-Side Sampling (GOSS)

Vá»›i dataset ráº¥t lá»›n, khÃ´ng cáº§n dÃ¹ng toÃ n bá»™ máº«u.

LightGBM chá»n:

Táº¥t cáº£ máº«u cÃ³ gradient lá»›n (khÃ³ há»c, quan trá»ng).

Má»™t pháº§n ngáº«u nhiÃªn cÃ¡c máº«u dá»… (gradient nhá»).

ğŸ‘‰ Váº«n giá»¯ Ä‘Æ°á»£c thÃ´ng tin, nhÆ°ng giáº£m sá»‘ lÆ°á»£ng máº«u.

4. Case Study
4.1 Taxi Trip Duration (NYC)

Dataset: 1.4 triá»‡u chuyáº¿n taxi.

Nhiá»‡m vá»¥: Dá»± Ä‘oÃ¡n thá»i gian chuyáº¿n Ä‘i.

Káº¿t quáº£:

Model	RMSE â†“	Training time â†“
GradientBoosting	0.3466	452s
XGBoost	0.3497	3.39s
LightGBM	0.3482	2.65s

ğŸ‘‰ LightGBM Ä‘áº¡t tá»‘c Ä‘á»™ nhanh nháº¥t, RMSE gáº§n tá»‘t nháº¥t.

4.2 HEPMASS (High Energy Physics)

Dataset: 3.5 triá»‡u sá»± kiá»‡n va cháº¡m háº¡t.

Nhiá»‡m vá»¥: PhÃ¢n loáº¡i tÃ­n hiá»‡u (signal) vs. nhiá»…u (background).

Káº¿t quáº£:

Model	Accuracy â†‘	AUC-ROC â†‘	Training time â†“
GradientBoosting	0.9181	0.9711	4100s
XGBoost	0.9178	0.9712	20s
LightGBM	0.9179	0.9711	21s

ğŸ‘‰ Vá»›i dá»¯ liá»‡u sá»‘ liá»‡u dÃ y Ä‘áº·c (dense numeric), XGBoost nhá»‰nh hÆ¡n má»™t chÃºt. NhÆ°ng LightGBM váº«n ráº¥t cáº¡nh tranh.

5. Giáº£i thÃ­ch mÃ´ hÃ¬nh báº±ng SHAP

LightGBM há»— trá»£ tá»‘t SHAP (SHapley Additive exPlanations), giÃºp:

Hiá»ƒu rÃµ feature nÃ o quan trá»ng nháº¥t.

So sÃ¡nh LightGBM vÃ  XGBoost á»Ÿ cÃ¹ng thang Ä‘o.

VÃ­ dá»¥: Vá»›i dataset Taxi Trip, cáº£ hai mÃ´ hÃ¬nh Ä‘á»u chá»‰ ra Top-3 feature giá»‘ng nhau, do Ä‘Ã³ káº¿t quáº£ Ä‘Ã¡ng tin cáº­y.

6. Káº¿t luáº­n

LightGBM vÆ°á»£t trá»™i trong cÃ¡c dataset lá»›n, nhiá»u feature categorical.

Tá»‘c Ä‘á»™ huáº¥n luyá»‡n nhanh, bá»™ nhá»› hiá»‡u quáº£, Ä‘á»™ chÃ­nh xÃ¡c cao.

Tuy nhiÃªn, vá»›i dataset nhá» hoáº·c chá»‰ gá»“m numeric dense, XGBoost Ä‘Ã´i khi nhá»‰nh hÆ¡n.

Vá»›i nhu cáº§u giáº£i thÃ­ch mÃ´ hÃ¬nh, SHAP káº¿t há»£p cÃ¹ng LightGBM lÃ  lá»±a chá»n máº¡nh máº½.

7. Gá»£i Ã½ thÃªm hÃ¬nh áº£nh minh há»a ngoÃ i TikZ

Infographic: so sÃ¡nh tá»‘c Ä‘á»™ train (cá»™t GradientBoosting vs. XGBoost vs. LightGBM).

Diagram: minh há»a quy trÃ¬nh GOSS (máº«u lá»›n gradient Ä‘Æ°á»£c giá»¯ láº¡i, máº«u nhá» gradient Ä‘Æ°á»£c láº¥y ngáº«u nhiÃªn).

SHAP bar chart: biá»ƒu diá»…n má»©c Ä‘Ã³ng gÃ³p cá»§a feature.